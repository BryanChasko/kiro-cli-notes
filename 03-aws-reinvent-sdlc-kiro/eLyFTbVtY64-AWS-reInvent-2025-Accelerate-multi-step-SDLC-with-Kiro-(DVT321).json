{"text": " So, hi everyone, welcome to Accelerate Multi-Step SDLC with Kiro. Glad to have you here. My name is Derek, and with me is Kieran. And we both work on the developer Agents and Experiences team at AWS. We both went on the team for the last year, so it's really exciting time to be in the space. We work with our Agents' Software Development Products, including Kiro. And then customers all around the world who are using these products to develop software. Our team is former software developers that are now engaged with other software developers around the world that are adopting these products. I'm Kiro's show of hands, who in the room is a software developer by profession? Okay, yeah, at least maybe half, okay. Who's vibe-coded, whoever's vibe-coded before? Okay, some of the same hands, some other hands. vibe-coding is obviously a hot topic using a hydrogen-rate code. These tools are getting better and better all the time. And I think vibe-coding is a great start, but the conversation we're having a lot with customers is beyond using AI to do the build step. How do I use AI at every step in the software development lifecycle and not just the build step? So that's the purpose of the talk today. And this is a coding session, so I'm going to do a little bit of talking up front, trying to do too much, but just want to set the stage for a few things here. So we'll talk about the software development workflow that we're going to use with Agentic tooling. And we're going to talk a little bit about how we compose Agents, what they're made up of, how the Agentic loop works, and how we can configure them in Kiro's CLI. Very important step before we go into actually using AI for software development workflow. So we'll do that up front, and then it's quickly as I can hand over to Kiro and who's going to do a bunch of live coding today. We're not just going to write code. We're going to start at the beginning of the SDLC. We're going to do requirements. We're going to do system design. We're going to make some decisions. And eventually we'll write some code, and hopefully we'll have a working game to play by the end of the session here. Live coding, live AI codeings, always going to be fun time. So just a level set. I think any of us could draw a picture of the software development lifecycle, and there's lots of different ways we could express it. If you ask different folks, love different opinions about which steps there should be, and which order they should be in, and how fast you iterate on the different steps. But broadly speaking, we need to have a requirements. We need to do a system design. We need to plan. We need to build. We need to test. We need to package. And then we need to deploy and operate. And when we talk about vibe coding, in my opinion, we're really just talking about skipping to the right code step. Which can be great for prototyping, or for trying something out, or maybe just scratching in it, in a language that you don't know or something. But when it's time to actually do professional software development, we don't want to skip steps. And so the question becomes, how can we use AI for all of these steps? There are a few approaches for this. We've got two Kiro products now. We've got the Kiro IDE, which just went into GA in the last month or so. And then we also have the Kiro CLI, which we'll be using today. The approach that we're going to use for the live coding session. You could use in Kiro IDE or Kiro CLI. This is really a preference thing in terms of the user experience that you like. Kiro and I are both a bit of console geeks. And so we're going to invite you all to be console geeks with us for Kiro CLI today. So the framework we're going to use is called AI DLC. And we want to show one way that you can approach this. This is not the only way. Kiro IDE has spectrum of development built into it. And so that's another way that you can approach using AI powered requirements and system design and planning. AI DLC is another way. It's a little bit more granular than what you would see in the Kiro IDE. And this is something that we've actually opened sourced and released on GitHub. You'll last a week or two. So we won't go too much into the details of AI DLC. It's going to be in the background sort of guiding us along as we go through this steps. But we have a slide at the end that has a QR code and a link. So if you like what you see and you'd like to try out AI DLC for yourself and some of these prompts, you're welcome to do so and we'd love to hear your feedback. So AI DLC roughly speaking has this concept of an inception phase where we're going through the requirements and the design and the planning. Construction phase where we're doing the building and the packaging and then an operation phase. We're going to go through the inception and the construction phase today and get hopefully to a working game by the end of the hour here. So I want to spend a few minutes on agent composition before we jump into the coding. So this I think is a commonly accepted definition of an agent. This is Simon Wilson's definition. An LLM agent runs tools in a loop to achieve a goal. Very simple. The simplicity is what we like for today's purposes. I'm going to modify that a little bit. And I'm going to say an LLM agent runs tools in a loop while building context to achieve a goal. And I'll elaborate on that a minute here, but we're finding is managing the context and the tools are critically important to getting the results that we want. Out of this agentic software development process. If we skip this step or if we don't get it right, we're not going to get the results that we want. So this becomes an important step in the workflow to making sure that our agent is configured correctly. So here's a diagram. Again, an agent. Agentic loop is nice and simple. The user, in this case, we're using the Kira CLI. We're going to ask it to do something as input. It's going to take that into its context. We're going to pass it to the LLM to decide what to do about our goal. The LLM is going to come back and say, I want to take some action using a tool. We're going to go call that tool, whatever it is. The tool is going to return the result. That goes back into the context and we go back to the agent again and we say, okay, we ran the tool. What would you like to do next? We'll keep iterating until the agent decides until the LLM decides we're done and we're ready to give a response back to the user. So let's talk about each of these parts briefly. Starting with the LLM. So for today's purposes, we're going to care about applied AI and not the gory details about how LLM works. So for our purposes today, I'm not going to try and talk about the semantic relationships that are in the model weights and and dimensional vector space and all that stuff. I'm sure some of the folks in the room are experts at this stuff. What we're going to say is an LLM gives us reasoning. We could debate how it gives us reasoning or the details. But if we ask it to reason about something, it will do that and it will come back and come up with an approach for a problem. We're also not going to talk about auto regression and multi-head attention and all that stuff. What we're going to say is the model has knowledge embedded into the model weights. What we would call latent knowledge that's been trained into the model during the pre-training step. No stuff. How it does the recall will leave for a different session. But our LLM is going to give us the ability to reason and it's going to have latent knowledge. Facts that it knows about the world. So that's great, but next we need working memory for the model to be able to understand what it's doing and keep track of where we're going. And that's the context window. So here's a very simplified sort of mental model of the context window. It's everything that's in the model's working memory. So starting with system prompts that are behind the scenes, guiding the agent's behavior, any context files that we've loaded. We're going to see example of that here in a minute. The history of our chat so far. So we've opened up our chat window, we're going back and forth, working on a problem with the agent. That's all in the context window. Any source code that it's pulled up and looked at if we're working on an existing project. The output of all the tools that we've called so far. And then hopefully some free space so that we can keep going. It's a managing this context window is extremely important for us to get the results that we want. If I'm asking the agent to work with some custom library that I've developed and it doesn't know anything about that custom library, we're going to get a very different result than if I were to load up the documentation for that library into the context window. So we want to make sure that we get this right up front. And then finally for this section, tool selection and tool execution. MCP has taken over in the last six months or so as the de facto standard for tool use with agents. For those of you who aren't familiar, I think MCP is the USB protocol for agents. I can take a plug, it'll plug into any agent and it'll let me plug skills into the agent so that it learns how to do something new. It'll let me give it access to reading resources from another location that it needs. And possibly give it the ability to take action. So almost like an API where it can do a mutative or make a change on another system. And so getting the MCP tools right again is another critical piece of getting ready for us to go do our agents at software development. If we need access to JIRA, if we need access to our internal wiki, if we need access to go search the internet, we need to make sure that any of these tools that we need are plugged in. So we talked about the agentic loop and how having the right LLM model to do reasoning, the right context window content in order for it to have that in its working memory what it needs to know. And we talked about having the right tools plugged in. These are all the ingredients to having the configuration that we need. So today we're going to be using Kiro CLI to do our live coding session. And in Kiro CLI we have this concept of a custom agent. And the custom agent is really a bundle of all of those things that we can store. We can share with our teammates. We can publish internally if we want other teams to be able to use it. We could just save it for ourselves. But it gives us the ability to combine our model selection. What context we want in the context window system prompt static resource files hooks which give us the ability to do some dynamic context. And then tools which MCP servers do I want to be using. Within each MCP server that could be exposing multiple tools which ones do I want to allow the agent to use. Maybe not all of them. I want to give them different names. So all of this goes into one file which Kiro will show us here in a minute. That lets us build our custom agent. So we're going to do that first. Kiro is going to walk us through building our custom agent. And then we'll come back briefly and talk about what we're going to go build. So with that. Kiro. Overdo. Thanks everyone. Let's work yet. The joys of alive demo. Yep. Good fun. I saw a flicker. Yeah. I've worked two seconds before we started as well. I've worked two seconds before we started. Yeah. There we go. Reboot. Okay. All right. So the first thing. Hi everyone. So the first thing we're going to do today is configure the custom agent. And so you can see exactly what Derek was talking about on those slides. So diving right in. I'm just going to go straight into Kiro. There we go into the Kiro CLI. And as Derek mentioned, we have an IDE that you can use as well. But for the purposes of this, we're going to use the Kiro CLI. So the first thing we're going to do. I was going to configure our custom agent before we actually go ahead and build a game. So in order to do that, I'm going to go. Agent. Let's start the agent start. We've got. So when you first start off, you have a Kiro default agent. You can see they're in green. And then I've got a flappy Kiro agent here that I've built specifically for this demo. But if you want to build your own, you just go. Agent. Oops. So it's agent. And then it's create. Give it a name. And we're going to call this demo. Agent. One. Now this is your agent configuration file. This one's empty. I'll show you the populated one in just a second. But this is where you configure all those things. Derek told you about your MCP servers. The tools that your MCP servers have access to. And we should have called out the acronym on first use. I don't think we did. And model context protocol. The tool aliases, as Derek mentioned. In case you want to give those different engineering name. The tools that your agents are allowed to use and the different resources. So what we'll do is we'll quit out of here and show you. What that looks like. Oops. I forgot the prompt sat here. I thought I had it. Derek. Oh, one. There we go. Sorry about that. So for this particular agent, we've got a couple of MCP servers configured. So I've got a mermaid MCP server. So that's going to help us build diagrams for the requirements. And the business requirements that we're going to do. As well as the implementation plan. I have a fetch MCP server to go and fetch things off the internet for us. And that's because when we build the plepic hero game. I wanted to be able to go and fetch AWS services from the internet and use that instead of normal. Fluffy bird game. And make that a little bit more interesting. Maybe use a Halloween and a Vegas type theme. So I've got some tools that I've got loaded here as well. Some resources and read and write tools as well. And the reason for not putting old tools that the MCP server has access to is as Derek mentioned. You've only got a certain amount of space in your context window. So you don't want to overwhelm the agent with tools that it doesn't need. So I've locked it down just for specific tools for to do that with. And then I've got under the resources the AIDLC workflow. And as we mentioned, we'll link that in a QR code towards the presentation. But it'll give you a list of all the different components of the AIDLC. And all the different questions and answers and how it all works throughout there. So I'll tell the agent to be able to access that as well for the purposes of building this out. And then hooks, as we mentioned earlier, that's something for you to for it to the agent to take action on. So really good for sort of CICD pipeline type things that you might be doing. We're not going to do anything for that in here. We're just going to focus on using the MCP servers and the AIDLC and the resources can be good with that. So with that, I'm going to switch to my agent. And we're going to select that one. And that's going to give us the agent that we're going to use to actually build out the game. So yeah. Thanks, Karen. And you know, just to emphasize, this is a relatively simple thing to configure. But by pulling in those resource files, we're going to dramatically alter the behavior of the agent. As you'll see in a minute here from what we would get by default. And so there's a lot of power under the hood that we can have here. We're developing custom workflows, specific ways that we want the agent to work. And that's something that teams can work together on and refine over time. And we're seeing libraries of these emerge within customers as the different teams, kind of maybe build their own repo internally, just to refine these files, get these better and better over time. So there's a lot that you can do here by just loading in a few context files and a few tools. So we're going to spend the rest of the time here building. And the goal here is to build a game called Flapicuro by the end of the session. We'll see how it goes. It's live. So we have some backup plans in case things. We have some kind of problem, but I'll try not to jinx this here. Flapicuro is a game where the go, I think his name is ghosty actually. Okay. So ghosty is going to be able to scroll through a side scrolling game. And when we push spacebar, he'll flap up in the air. When we let go the spacebar, he'll drop down and he's got to avoid obstacles and get a score. The point here is less about the implementation details that we pick or what language or hopefully the game actually works. And whether it's good or not, what we really want to do is focus on the workflow. And focus on how we can use AI from the very beginning to help us get our requirements and our design and our thinking about our plan and our testing and everything that we need to do in order to build production grade software. So while we're not building production grade software today, we're going to sort of speed through some of the steps. That you walk away today, maybe inspire a little bit to try some of these techniques yourselves and try doing an end to end, you know, task or story from your backlog using this process and using AI from the very beginning of your project. Anything else I should do to tee a plappy caro here, I think it's a pretty straightforward game and we'll get into some of the details. Yeah, we're going to try and get it to work on here as well as a mobile phone, so we'll switch to the development tools and see if it works on a mobile, but try and get to work first and then we'll go from there. All right, let's see how we get on. Okay, see if this works. Oh great. Okay, so to build this, we're going to just use one prompt, so this is the prompt I'm going to use, we're going to walk spend a little bit of time going through it. So the goal is not to do any prompt engineering or have any secrets or spirit baked into this, we're going to have one clear prompt and then we're going to go through the AI DLC process, which has been shortened to fit into the session today. So the first thing we're going to do is we're going to use Derek mentioned in the presentation, a lot of the LLMs trained, you know, a latent knowledge that it's got when it was trained. So a lot of this, it's aware of and it can actually make a lot of assumptions based on this, but we still want to guide it with proper requirements, business requirements, you know, functional requirements and functional requirements and then implementation plans. So the first thing is, I'm going to say, I want to build a browser based game called Flapicuro for a live demo at AWS reinvent. We're going to follow the AI DLC methodology and workflow and we're going to ask the LLM to ask us clarifying questions, so that it can then provide the answers as inputs for each phase. I want to show you some mermaid diagrams, so when it actually builds out the far structure and the implementation plan, you can see what that looks like and if you look in the IDE, it actually gives you a really good visual of that. Because we're doing this in the CLI, I'm just going to use a mermaid webpage that we've got and we'll show you all the architecture diagrams that it does. So we've streamlined the process, obviously to fit into the session today, when we run this with our customers, it's usually a one day workshop sometimes too, but we're going to get this to work in 45 minutes. And then we're going to maintain interactive questions and answer throughout the process and we want to save the files in the working directory so it doesn't put it everywhere. So I'm being quite specific with it, but we still have to, we're still going to do a quieter guidance and give it the themes and everything like that. The question was, are we using questions as you go? Take questions that we go. I think while the agent is thinking, I think we'll have some time. Yeah. Especially through the bill phase, it takes a good five, seven minutes. Yeah. It can be two then. Okay. So we're going to go ahead and execute this and English cost. So the first stage is going to be the analysis phase or we're going to gather all the requirements. And in the real world, this will be your business requirements that you might have within your organization. I give this agent permission to do that. So it'll be things like, you know, what sort of third party systems do you need to use, what sort of logging or anything like that that you might have if you have any security postures that you need to follow, things like that, that will be part of your business requirements. So here we go, it's going to create it a, and I will make this bigger. If anyone has any suggestions, we can take them as well as be go through this, although it'll just be me filling these out as we go along. And as Derek mentioned each time we run this, we get a slightly different result, so it's not always the same. The questions do come back a bit different, but when you use the full AI DLC, you get very specific and then you tend to get the correct response. Just to elaborate if I can, Karen. Yeah. I think what we find is that the frontier models, the LLMs, they're eager to please, right? And so if we're vibe coding, often the model will sort of take what you've asked it to do and start running with it and start building stuff, rather than sort of stepping back and thinking critically, like maybe a senior engineer might step back and say, hey, before we build this, I've got like 15 questions about, hey, what did you think about security? Did you think about this, you know, what about this edge case, right? And that's kind of the conversation that we want to be having. And so really, what we've got loaded into the context here is a lot of specific instructions saying, hey, don't skip any of these steps. If we don't know the answer yet, let's get the answer before we keep moving. And so the first step here where we've told Kiro, hey, I want to build this thing is, okay, I've got a bunch of questions. Please answer those for me before we continue. So we'll fill those out here, but it should keep asking us until it's clear. Yeah, exactly. Yeah. And these questions are predefined ones. This isn't like a final list. If you have something when you're doing this for an enterprise app, you can go and actually put those in there. So there's a question you feel that your hasn't, you know, met your organizations needs. That's something that you can just insert in there too. It doesn't have to be, it won't freak out. And I'll show you an example of that. So the way I answer these questions, I'm supposed to fill them in down here. I'm just going to say yes, no. And then for this one, I'll fill them all in down here for question two. And we'll go through it like that and see all the agent deals with that. So the first question, we wanted to continuously fly. And it says all only when clicking, when pressing the, with space bar. So I'll say, yeah, press. I'll just say a button. Press button to fly. And then for the easy difficult, easy medium or hard. I'm going to say easy has large larger, yeah, it's a large larger. Optical gaps, getting harder as the game progresses. There we go. And should the game speed increase over time? I'll just get that a flat yes. This takes quite a bit of time when you're building out an enterprise app, but I hope it will get through this quite quickly. So then we get to the visual size of this too. So what I want to do is I'll change this a little bit. We're going to use maybe some of the AWS colours. I'll go with orange and black. I'll go with a Halloween theme. I think it's recently Halloween. And say Las Vegas theme as well. Let's see how it deals with that. One at a time was there. Do you want the Kira character to be a simple shapes, bright, or custom design? So Kira is a ghost. So we're going to use a purple, purple, ghost character. Should the obstacles be classy? You know, the classic, obviously it's got that late in knowledge. It knows exactly what that game is. So let's go with AWS themed. And it's got that from the prompt that we did with saying that we're doing an AWS reinvent presentation. And then we want to say use say AI and coding services from AWS. There we go. And then scoring in leaderboard. We're kind of scoring features which you like. I'll take suggestions if you want to shout them out too. I'll go with all these. So simple score. Or do you want to use a leaderboard. So we'll just go with simple for now. And then here we're just going to use local storage because we don't have anything like that configured. And in the real world, obviously it's looking for, you know, if you're using an API gateway, you need something like cloud front. If you want this game to be globally available, close to your customers in different regions. That's something you'd specify over here. For the purposes of this, we're going to do this. We'll say yes to that. And then how long do you want it to be? I'll just go one to two minutes. And I'll say yes to that. And on a mobile phone. Test that towards the end. I like the ambition level here. Yeah. You're having a lot of features. HTML and Java. Let's use those two for this purpose of this. Single file. I think we'll use multiple files because I want to have proper structure for this. So in case we need to go back and change something, or we want to document any of these decisions and come back and ask why we did that. It's all nicely documented. So go with multiple files for that. Yeah. Good catch. It's pretty good at picking up all my bad typos. So I'll see how it deals with that one. I'll leave it alone. Any AWS services want to showcase? Anything to do there? I don't think it knows about curiosity. It's got that MCP server to go and fetch things like that. I'm a proud workmail user, but it's probably not. Workmail. So I'm not appropriate. Yeah. Sorry. Amplify. Do that. Let's see what it does there. And I'll put maybe bedrock since it's going to be running on that. Okay. Let's try those. And then start string instructions. Say yes. We're just suggesting for Amplify and bedrock. Maybe just make a note that we need a way to test locally. Okay. Since we're not going to have to find time to deploy to an AWS environment. Yeah. Cool. All right. Restock button. I'll just say yes. And then we'll move on to the next one. Mobile support. Support for mobile browser. So I think. And then maybe want to use maybe Chrome and Firefox. So I think there are two common use ones. Chrome and Firefox. Okay. Great. And that's it. So we'll go ahead and save that. Close that up. And then we can say. Done. And again, we're still using that single prompt. And it should now go ahead read those answers and come back and start building out the design phase. And the whole process of this is we're bringing the agent and the LLM into the decision making process. So yeah. So about that, saving a lot of time. And just to note that we were just doing a lot of. What I would call classic product manager work unless software development. But it's a very important obviously. And one thing that we're finding is that. Getting together cross functional teams that maybe not might not work together next to each other day by day. But getting the product managers and the stakeholders along with the software developers. Maybe QA ops. Right. Getting that cross functional team that's going to be responsible for this thing. Sometimes physically together, or at least on the same call. And working together on answering these questions can help teams get this right up front. And skip a lot of back and forth that we might traditionally do where we need to email or cut a ticket to someone to find out. And so if we're able to do this collaboratively up front, we can dramatically cut down on the time needed for us to get all of this right. And the agent in this case is really our helper to make sure that we're documenting it properly. Yeah. Thanks. So now we've moved moving on from what to build. And now we're going on to how we're actually going to go ahead and build that. So it's taken some of the suggestions of SageMaker, amplify us so that they're in bedrock. And now it's going to come through and ask us how are we going to do these. And if you think about these and now you're more technical requirements, we've defined the business requirements in the first phase. We're moving on to the more of the technical stuff right now. So early on in here, it's given us using that agent that we can figure. We told that to have the Mermaid MSP server. It's gone in and actually built out some Mermaid diagrams for us. So I'll show you what that looks like. Hopefully that looks good on the screen. There we go. So it's actually already documented and drawn out what it thinks this is going to look like. And do you remember that one prompt? Do you want everything in a single file or do you want to split up? And it's going to hit and split that up for us as well. It's specific files for different things. So we have JavaScript files and HTML files. So you can go in and make changes later. So this is really useful for things like that too. Sorry. I'll get back there. Okay. And then if we look over here. So let's have a look at this one here too. It shows you the different components. So we've got hope it here. Sorry, I'll lift my muscle home. What a bit easier with that. There we go. So it's showing you the different layers. It's going to build out and how that's going to look. Look at on the screen. Go a bit bigger. So we find this really, really useful. Customers have told us that this saves a lot of time with the documentation. And how that's going to look. And they can use that for all the different change controls. And when they submit tickets to have this all built out. So we're getting all of that. You can remember I said local storage early. We're not going to deploy that to anything else. So that's kind of there. We already talked about how we're going to do the obstacles. It's going to fill all that in the scoring. I said do that as we pass the obstacles. The difficulty controller from the easy medium and hard. So all of that's all in there already. Okay. So let's go back in and fill out that. Fill out the rest of that for the technical requirements. Okay. So the first question is which AI coding services did appear as obstacles. So I'm just going to say use all and use them randomly. How should the difficulty levels differ? Now I found building this our testing this is it usually the gravity is way too strong. So the board just falls like a stone no matter how quickly I press this base bar. And the gaps are way too short. So I can never get past the first couple of hurdles. So I might go in here from my previous knowledge and tell it to be to be a little bit better with that. So I'll say start slower logic gaps and less gravity. And then I want to use 60 frames per second. And then now we've got the visuals. So shall I generate a simple CSS canvas or do we have any image assets? I start with a normal simple CSS canvas. But then you can go back in download specific assets or any audio images that you've got for the game. And you can put that in. But it does a pretty good job of filling just coming up with stuff itself. So we'll go with CSS. And then ghost animation. Do you want to static bobbing, flipping wings. Let's animate the ghost. What it's applying. Let's use official AWS icons. I don't think it's going to be able to do that. And this will give it to them. But it's going to try and draw them up. Typically using mode uses something like that too. Sound effects. I'll just say use. Oh, suggested effects. Discup interest of time. Here we go. And then this question. Yeah. The bedroke integration. How should Amazon bedrope be integrated dynamically. AI power difficulty adjustments. General rate. What do you think? This is going to be interesting. I think those are all future future enhancements. Yeah. Yeah. Probably doesn't matter for this demo. Yeah. And then here's the file structure that it's going to go ahead and create for you. So instead of creating that one big file, he saw that diagram. It's going to go ahead and break these down for you in the different files that you're going to use. And that means you can come back in and put in your own images and your own sounds when you want to do so. I'll just say. Okay. And I hope it's set by. So again, we're almost kind of like speed running through the steps and almost five coding in a way right by not engaging more here and doing a much richer discussion in the interest of time. But, you know, again, I think you can imagine. This is where, in my opinion, the interesting work is going to happen is around these decisions that we're making around technology selection. And what should we do first and what can, you know, what's a P0 versus a P1 feature and what's our architecture going to look like, you know, this is where we as humans should be spending our time. And if we get this right up front, the actual implementation phase is almost the easy part, hopefully, because we've already done all of this hard work up front. And so if you're not already, I would encourage you. If you try to add you'll see, you know, spend a good amount of time up front in these phases. Get this as refined as you can. And then see how the implementation step works out for you. Yeah. So we'll link at the end to the GitHub repo, but our teams have put a good amount of work over the last few months. Again, working with lots of different customers to refine this. We're using conditional context loading because we have a finite amount of context window space. There's an initial context file that we loaded. That kind of kicks off things with the agent and tells it, hey, you're using this workflow. You're going to use these various specific steps. And then depending on what you're doing, if you're doing green field or if you're extending an existing application, if you want to do microservices, there's a bunch of decision trees that it'll make and say, oh, we're using it. We're going to extend a legacy application. It'll actually load in additional instruction sets to make sure that we're doing things in an orderly way. So there's a lot behind the scenes here that's getting loaded in as needed. Yep. Yep. There's just the one marked down file is the entry point. And it has instructions that says, conditionally load these other files depending on what you're doing. We found that approach works well to keep the context window, not too busy. Yeah. And then to your point, the other thing you can do is you can have different agents for different phases of your development as well. So if you're doing front end development back end development, you can have different agents with different MCP servers, different resources. And that way you can just switch between those agents as easily as I showed you there. Yeah. And that way, that's a good way of getting a project done without loading everything up. So yeah. Okay. So now this is the part where we move to the actual implementation phase. It's 11 written any code yet, but now we're going to, you know, this thing you can start having it, you know, georeticates for you and you can have your junior developers go ahead and run this based on the architecture diagrams that your senior developers or Kero has built for you. So again, we're just going to go through these real quick and see what we can get from all of it. So a proof to implement. Yes, we're just going to go through the the reservation size, link all the modules together. And then here, if you have any specific styling that your organization might use, this is a, you know, a good place where you can do that. And if you're building an enterprise app, so that's something you can do there as well. And if there's anything, you know, you'd be sitting with different teams across your business going through all of this and figuring out, is this the right thing to do, but it's not captured in here. You can obviously add it in there and change some of those files as well. Then we go through that as well. Yeah. So we are going to use JavaScript and HTML. I think we decided that was going to be the best thing. So we're going to go ahead with that. It's just asking for that final verification before we do anything. Yep. There is a GMCP and so I was just thinking the same thing. Once you get to this step in the real world, each of these might be more complicated and take more time. So you can imagine each of these becoming a story that goes on to the gerabord and gets pulled down for sure. That could be as simple as us taking this file and saying, go, go create gerastories for each of these. Yeah. I'll be honest, this is different from the other ones that I've done before. Normally it's another bunch of questions, but here it's just basically saying you've provided me enough. And just give me the verification for that. So just answering yes. Well, I'll have a look at this diagram that it's given us. This should be the implementation diagram. I didn't grab everything that I have for some reason. I thought I had it all. Yeah, I did. Didn't like that. I'll try it once more. I suggest here in the interest of time that we, yeah, we are. Okay. And you can see when it gets to this part, the agents are really thinking about how it's going to generate the testing. So it's going to have functional testing and any sort of unit testing and stuff you might want to do for your code. It'll generate that in the next phase too. But it's already thinking ahead how am I going to test this into end. You know, this is quite a, you know, obviously the different I've said to it use five box and chrome. Because it's going to go on mobile phones. It's figured that some people are going to have iPhones and it's put Safari in there as well, which is great. Chrome mobile chrome desktop. So yeah. So we'll go ahead and save that. Okay, now it's going to go ahead and create all the different files for us to actually build a project. And there we go. It's going to create the assets file, the CSS files, the JavaScript files, and hopefully we'll get it working again. So this is where it takes a few minutes. Five or seven minutes. It works. And we spent a lot of time on requirements, design, technology selection, not coding, right? We've got 16 minutes left in the hour. We'll see how we get on. But again, it's just sort of the almost the opposite of, of vibe coding and starting with build. And this was really the, the thing that we wanted to impart is it's, it's time well spent doing this up front. A ideal C is one framework. Again, there are other approaches. You could build something yourself. You might consider forking what we've got. You might consider, in fact, send us a merger request. If you find a way to do to improve on the process. But the idea is to have a plan going into the, into the process. Yeah. To summarize the question, is this process. It's been linear, right, where we do all these steps in order. And in reality, in real life, we might start implementing phase one and say actually I changed my mind. I want to change tweeter requirements. I think you certainly can go back to one of the previous stages. Edit the requirements and then ask the tool to update the design for example. I don't think I have any, any real shortcuts for that. I think we've got to pay the price at some point. If we, if we want to change the requirements, we're going to have to go through those steps again. I think that the, by getting the right people in the room up front. We can hopefully minimize that, that back and forth, in terms of what the requirements should be. In terms of iterating on the design and the implementation. One of the benefits is we can move pretty quickly here. And so the iteration step, whereas in previous lives, that might be like a whole sprint that we just used to build something. We've got to start over again. We could potentially make that cycle time quite a bit shorter. But I don't have a way around, you know, needing to go back and, and. I know it's trying to go back. So I thought that like, if you really think of this game, document what you did, that I'll look at the change in our mind. And then, like, we're looking at the, you know, specific, self-harmonic changes, specific changes. So it's like, like a cycle process. Yeah. So just to repeat the question, the question is about. Being able to save the context and go back and edit it later and be able to iterate based on what we've learned so far. So we're writing all of the design decisions into, into Markdown files. All of the Q&A and all of the descriptions of the requirements and design and such. There's also an audit log that automatically gets written of all the questions and decisions that were made. One thing that we've seen is just taking all of these files and attaching them back into the jurisdictions. And so anybody that wants to go back and understand why do we make this decision or, you know, what was used to, as input here, we can go back and reproduce it. We've also seen, you know, the context window can get full. And so in between each of these steps, we've actually cleared the context and then started fresh again. And that kind of proves to ourselves that everything that's needed to reproduce what we've got is stored in these Markdown files. Yep. And that's in our custom agent. Yep. And so in theory, we should be able to go back to any step in any time. Use one of these as artifacts. Make a tweak and move forward from there. How does this take place for us using the power of our IDV? Because I use it and it follows very similar phases where we have the design requirements and tasks. And here you have the analysis design. Do you think one is more powerful than the other? Or depending like if we use it, you would dig your own best intelligence. And in that case, we would first be more powerful than the spec. The question was, which is sort of the better approach. Care ID has spectrum and development. We're doing this ADLC workflow in the CLI. They have similar bits of functionality. We're moving fast in all of this. The IDE, I would say, spectrum and development is less customizable at this point. But you're going to get what you get. And for a lot of customers, they really like it and they're using it. This is much more configurable. The ADLC approach or an approach like this gives you the ability to tailor this process with your developer. Development teams over time and make it as granular and specific. Always do it this way, don't do it that way as you want. In return, the process is more complicated. It's going to take more time. We're skipping a bunch of steps. If we were going through the full thing and we hadn't told it, we were doing a demo. It would be documenting tons of user stories and asking what personas should we be doing. So it'll go a lot deeper than this. And so I would suggest that maybe as a starting point or for features that are a little bit more. I don't want to say straight forward, but I would say a clear IDE is a good starting point. But if you're looking for more ability to customize or the ability to get a few levels deeper into the requirements, I would suggest that this is a good approach. But I also think that you'll see you over time. We'll continue to build out that spectrum and workflow in the IDE. And that'll get more and more features as well. How are we doing here? I built quite quickly. There was no contention on Bidrock. I think we're good there. I might have to see if it works. Yeah, well that's. So it did build us a implementation plan. And then let me close that down and bring up the other ones. You know, all the coding was complete. You get a full report of everything that was done. How much code was written. All the features that were implemented. The stuff that we walked through in the beginning. And yeah, tells you how to test it. And it also gives you a whole testing plan as well. This is quite a basic one, obviously. But you can obviously imagine what it will produce when you do a proper document. So yeah, with that, let's see what happens. I think it's just this file here. We can go. So there we go. We got the start screen. Look, they're not so far. This is the worst part. Easy. There we go. I think it wasn't too bad. Okay, nice. The bedrock thing that wasn't as impressive as I thought. I think that's a pack man goes there. Yeah, it doesn't actually look. But it's picking up scores. I don't know if there's any sound on that. I think we said no sound. But they should have been, I can unplug my mic in a sec. Oh, no. I mean, when we set up the stage. Yeah. That was our mistake. That's actually better than I thought. Yeah. A lot of them just didn't work. It just sunk like a stone. Cool. Nice. We've got eight minutes. Happy to take more questions and discussion about the process. Also, I'll be to take a modification request to the game if we want to try that. Yeah. Yes. I will put that up right at the end here. I'll do some crowd work here. So our team recently did a test where we didn't full-care and just had it kind of go crazy and make a whole web app. My issue with that was that a lot of it was like an accessible. And like had like there was like window to open things and things like that. And I'm just wondering what recommendations you have. Like, is it like adding a front end MCP to the front of it? Like, what would you recommend to help make sure it kind of followed. We'll connect guidelines. Excuse me about accessibility like A11. Yeah. Okay. How specific were you at the beginning about your accessibility requirements and what was must have nice to have? I'm not sure. I have delegated this. I got it. So I think that would be my answer. Right? The more time we spend up front. If this is a web app talking to the UX folks, the stakeholders, the product managers, getting really into the weeds on what are the non-negotiables for accessibility. And if we can get all of that, you know, decide it up front and document it into one of these markdown files. I would expect that we would get, you know, better results there. Is there a way to like, there's like a black tag guidelines? Like, is there a way to like more easily integrate that even? Just as a base. There's this like actual specs and whatnot. Yep. So, you know, one thing as you're developing your, you know, version of this for your teams. Again, you could imagine having a repo of these context files. And a custom agent that you publish for the teams and say, use this when you're building a web app. And in that custom agent, in the context, you could say, the following or kind of company standards for accessibility is like, how should I make sure that you hit these versions of these WCAG requirements. The model is going to have a pretty good understanding of those requirements, baked into its latent knowledge. But if you wanted to really make sure that you nailed it, you could say, hey, I'm guessing I would be willing to wager that there is a WCAG, MCP server where you can go, where you can go and grab the actual spec. So, that would come at the cost of more context that you'd have to load. But you could imagine saying, hey, as part of our verification, I want you to, I want you to make sure that you've built all of this in and actually go test it. So, you think you'd get a sophisticated as you wanted to. And, you know, I would suggest starting by adding that up front into the context and having it build that in. Thanks. That one over here. Yeah. I work as a manager of developer experience team, our company and we have like 300 engineers as a whole. And one of the difficulties that we have is like, how are we standardized the AI knowledge, because people like to use heroes. Some of those are like to use cursor or cloud code or whatever. And when I look at these, it's kind of like a bit more considerate. On Kiro, like, how is the difference with like, age it's MD, for example, that's kind of like a standardization independent of the tools. Like, that's, that's a little bit of my question here is like, how can we make that reusable, instead of the tool that developers choosing, so probably making easier to adopt. Yeah. So we're seeing a lot of innovation in all of the tools in this space. So things are moving fast. Agents that MD seems to be a de facto standard that's emerging. That's good. The different tools have different ways for specifying, you know, this concept of custom agents. This workflow, the AI DLC is just a bunch of markdown files. So it would work fine in cloud code or cursor or what have you. So this isn't meant to be specifically tied to Kiro. We think that Kiro is a great platform for doing this, but we're not trying to sort of lock it down to one particular tool. But ideally, you're able to bundle together the AI DLC or the context files that you have that you want to steer the behavior of the model, as well as your list of NCP servers together, and have a way to publish that out to other folks in your organization. So with Kiro CLI, that's just a JSON file. And so that all that could be is an internal repository where you say, hey, if you're building web apps, use this custom agent. If you're extending this old legacy app, go use this agent. And that combined kind of collective wisdom of how to operate around those different domains is baked into those agents. Yeah. Guess so one more. Oh, come over to you next. First of all, thank you for that great presentation. When you have these different stages in the AI DLC, and in the agent definition, there was this line model that was null. So you leave the selection of the model to use to the automatic, or would you suggest using different models for different tasks and stages in the AI DLC? Yep. Thanks for the question. So by default, we now have an auto mode in Kiro, and that's going to allow us to select the model and give you a discount in terms of credits. And our goal with the engineer and team, and there's been a lot of energy on this, is making that auto mode work great, and you as a user would never have a reason to change the model, because you're very happy with the outcome. So that would be our recommendation. Start with the auto mode, hopefully you get great. If you're not sure you're getting great and you want to say, no, I definitely want Sonic 4-5 or whatever the latest model of the day is, you could switch to that and see if you get different results. But our hope in our expectation is that you can start with just not defining the model, use the default auto mode. Yep. I want to take one more here, and I know we're just wrapping up on time. So first of all, thank you so much for the presentation at demo, and I just tried to Kiro app and also the CLI as well. They both work really fast. I really like it. Great. I like the broader. But the things are, you know, I know a lot of independent developers. They try to show me like AI coding agent, cursor, cloud code. So actually, I want to get more insight that, how to differentiate from Kiro and those not a coding agent. For example, are we going to have more integration on AWS or, you know, to some more integration work on enterprise integration? Yep. Yep. Thank you. We're right at time. But I'll say, in terms of differentiation, we want care to be a great product regardless of where you're developing, even if it's not a native US. And so our intention is to just focus on a great developer experience. Of course, we want it to be really good at AWS as well. So that's the thinking. You know, differentiators. There's a lot of smart folks that are building other cool tools in this space. I think it's a good time for end users because that competition is, is having folks kind of leapfrog each other in capabilities. I expect that's going to continue to happen. I'd say one nice thing about Kiro is when you subscribe, you have access to both the CLI and the IDE for one subscription. And we see a lot of folks that like to switch back and forth between those modes, or maybe even use both at once. So I think that's a nice thing. I would say the spectrum and development in the IDE, which we didn't look at today, but we've heard a lot of great feedback that that's a good way for developers to sort of structure their approach. So I would encourage you if you haven't to try both. Try the spectrum and workflow in Kiro IDE. Try the AI DLC, which is a bit more in depth, and see which one suits you the best. So very brief answer to your question, but I know we're right at time. All right? Thank you all. Thanks, Kiro.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " So, hi everyone, welcome to Accelerate Multi-Step SDLC with Kiro.", "tokens": [50364, 407, 11, 4879, 1518, 11, 2928, 281, 5725, 6185, 473, 29238, 12, 23624, 14638, 14766, 365, 591, 5182, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3148285638718378, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.03516487032175064}, {"id": 1, "seek": 0, "start": 7.0, "end": 9.0, "text": " Glad to have you here.", "tokens": [50714, 28301, 281, 362, 291, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3148285638718378, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.03516487032175064}, {"id": 2, "seek": 0, "start": 9.0, "end": 12.0, "text": " My name is Derek, and with me is Kieran.", "tokens": [50814, 1222, 1315, 307, 22887, 11, 293, 365, 385, 307, 591, 38516, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3148285638718378, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.03516487032175064}, {"id": 3, "seek": 0, "start": 12.0, "end": 17.0, "text": " And we both work on the developer Agents and Experiences team at AWS.", "tokens": [50964, 400, 321, 1293, 589, 322, 264, 10754, 2725, 791, 293, 12522, 14004, 1469, 412, 17650, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3148285638718378, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.03516487032175064}, {"id": 4, "seek": 0, "start": 17.0, "end": 24.0, "text": " We both went on the team for the last year, so it's really exciting time to be in the space.", "tokens": [51214, 492, 1293, 1437, 322, 264, 1469, 337, 264, 1036, 1064, 11, 370, 309, 311, 534, 4670, 565, 281, 312, 294, 264, 1901, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3148285638718378, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.03516487032175064}, {"id": 5, "seek": 0, "start": 24.0, "end": 29.0, "text": " We work with our Agents' Software Development Products, including Kiro.", "tokens": [51564, 492, 589, 365, 527, 2725, 791, 6, 27428, 15041, 47699, 11, 3009, 591, 5182, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3148285638718378, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.03516487032175064}, {"id": 6, "seek": 2900, "start": 29.0, "end": 34.0, "text": " And then customers all around the world who are using these products to develop software.", "tokens": [50364, 400, 550, 4581, 439, 926, 264, 1002, 567, 366, 1228, 613, 3383, 281, 1499, 4722, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17043723273523076, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.03409383445978165}, {"id": 7, "seek": 2900, "start": 34.0, "end": 43.0, "text": " Our team is former software developers that are now engaged with other software developers around the world that are adopting these products.", "tokens": [50614, 2621, 1469, 307, 5819, 4722, 8849, 300, 366, 586, 8237, 365, 661, 4722, 8849, 926, 264, 1002, 300, 366, 32328, 613, 3383, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17043723273523076, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.03409383445978165}, {"id": 8, "seek": 2900, "start": 43.0, "end": 49.0, "text": " I'm Kiro's show of hands, who in the room is a software developer by profession?", "tokens": [51064, 286, 478, 591, 5182, 311, 855, 295, 2377, 11, 567, 294, 264, 1808, 307, 257, 4722, 10754, 538, 7032, 30, 51364], "temperature": 0.0, "avg_logprob": -0.17043723273523076, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.03409383445978165}, {"id": 9, "seek": 2900, "start": 49.0, "end": 53.0, "text": " Okay, yeah, at least maybe half, okay.", "tokens": [51364, 1033, 11, 1338, 11, 412, 1935, 1310, 1922, 11, 1392, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17043723273523076, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.03409383445978165}, {"id": 10, "seek": 2900, "start": 53.0, "end": 58.0, "text": " Who's vibe-coded, whoever's vibe-coded before?", "tokens": [51564, 2102, 311, 14606, 12, 66, 12340, 11, 11387, 311, 14606, 12, 66, 12340, 949, 30, 51814], "temperature": 0.0, "avg_logprob": -0.17043723273523076, "compression_ratio": 1.8341013824884793, "no_speech_prob": 0.03409383445978165}, {"id": 11, "seek": 5800, "start": 58.0, "end": 63.0, "text": " Okay, some of the same hands, some other hands.", "tokens": [50364, 1033, 11, 512, 295, 264, 912, 2377, 11, 512, 661, 2377, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10898291948929574, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.009980394504964352}, {"id": 12, "seek": 5800, "start": 63.0, "end": 67.0, "text": " vibe-coding is obviously a hot topic using a hydrogen-rate code.", "tokens": [50614, 14606, 12, 66, 8616, 307, 2745, 257, 2368, 4829, 1228, 257, 12697, 12, 4404, 3089, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10898291948929574, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.009980394504964352}, {"id": 13, "seek": 5800, "start": 67.0, "end": 70.0, "text": " These tools are getting better and better all the time.", "tokens": [50814, 1981, 3873, 366, 1242, 1101, 293, 1101, 439, 264, 565, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10898291948929574, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.009980394504964352}, {"id": 14, "seek": 5800, "start": 70.0, "end": 79.0, "text": " And I think vibe-coding is a great start, but the conversation we're having a lot with customers is beyond using AI to do the build step.", "tokens": [50964, 400, 286, 519, 14606, 12, 66, 8616, 307, 257, 869, 722, 11, 457, 264, 3761, 321, 434, 1419, 257, 688, 365, 4581, 307, 4399, 1228, 7318, 281, 360, 264, 1322, 1823, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10898291948929574, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.009980394504964352}, {"id": 15, "seek": 5800, "start": 79.0, "end": 85.0, "text": " How do I use AI at every step in the software development lifecycle and not just the build step?", "tokens": [51414, 1012, 360, 286, 764, 7318, 412, 633, 1823, 294, 264, 4722, 3250, 45722, 293, 406, 445, 264, 1322, 1823, 30, 51714], "temperature": 0.0, "avg_logprob": -0.10898291948929574, "compression_ratio": 1.6584362139917694, "no_speech_prob": 0.009980394504964352}, {"id": 16, "seek": 8500, "start": 85.0, "end": 88.0, "text": " So that's the purpose of the talk today.", "tokens": [50364, 407, 300, 311, 264, 4334, 295, 264, 751, 965, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09651723758194798, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.03634853661060333}, {"id": 17, "seek": 8500, "start": 88.0, "end": 93.0, "text": " And this is a coding session, so I'm going to do a little bit of talking up front, trying to do too much,", "tokens": [50514, 400, 341, 307, 257, 17720, 5481, 11, 370, 286, 478, 516, 281, 360, 257, 707, 857, 295, 1417, 493, 1868, 11, 1382, 281, 360, 886, 709, 11, 50764], "temperature": 0.0, "avg_logprob": -0.09651723758194798, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.03634853661060333}, {"id": 18, "seek": 8500, "start": 93.0, "end": 97.0, "text": " but just want to set the stage for a few things here.", "tokens": [50764, 457, 445, 528, 281, 992, 264, 3233, 337, 257, 1326, 721, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09651723758194798, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.03634853661060333}, {"id": 19, "seek": 8500, "start": 97.0, "end": 104.0, "text": " So we'll talk about the software development workflow that we're going to use with Agentic tooling.", "tokens": [50964, 407, 321, 603, 751, 466, 264, 4722, 3250, 20993, 300, 321, 434, 516, 281, 764, 365, 2725, 317, 299, 46593, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09651723758194798, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.03634853661060333}, {"id": 20, "seek": 8500, "start": 104.0, "end": 110.0, "text": " And we're going to talk a little bit about how we compose Agents, what they're made up of, how the Agentic loop works,", "tokens": [51314, 400, 321, 434, 516, 281, 751, 257, 707, 857, 466, 577, 321, 35925, 2725, 791, 11, 437, 436, 434, 1027, 493, 295, 11, 577, 264, 2725, 317, 299, 6367, 1985, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09651723758194798, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.03634853661060333}, {"id": 21, "seek": 8500, "start": 110.0, "end": 114.0, "text": " and how we can configure them in Kiro's CLI.", "tokens": [51614, 293, 577, 321, 393, 22162, 552, 294, 591, 5182, 311, 12855, 40, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09651723758194798, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.03634853661060333}, {"id": 22, "seek": 11400, "start": 114.0, "end": 120.0, "text": " Very important step before we go into actually using AI for software development workflow.", "tokens": [50364, 4372, 1021, 1823, 949, 321, 352, 666, 767, 1228, 7318, 337, 4722, 3250, 20993, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10283106651859007, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.008190829306840897}, {"id": 23, "seek": 11400, "start": 120.0, "end": 129.0, "text": " So we'll do that up front, and then it's quickly as I can hand over to Kiro and who's going to do a bunch of live coding today.", "tokens": [50664, 407, 321, 603, 360, 300, 493, 1868, 11, 293, 550, 309, 311, 2661, 382, 286, 393, 1011, 670, 281, 591, 5182, 293, 567, 311, 516, 281, 360, 257, 3840, 295, 1621, 17720, 965, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10283106651859007, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.008190829306840897}, {"id": 24, "seek": 11400, "start": 129.0, "end": 130.0, "text": " We're not just going to write code.", "tokens": [51114, 492, 434, 406, 445, 516, 281, 2464, 3089, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10283106651859007, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.008190829306840897}, {"id": 25, "seek": 11400, "start": 130.0, "end": 132.0, "text": " We're going to start at the beginning of the SDLC.", "tokens": [51164, 492, 434, 516, 281, 722, 412, 264, 2863, 295, 264, 14638, 14766, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10283106651859007, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.008190829306840897}, {"id": 26, "seek": 11400, "start": 132.0, "end": 133.0, "text": " We're going to do requirements.", "tokens": [51264, 492, 434, 516, 281, 360, 7728, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10283106651859007, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.008190829306840897}, {"id": 27, "seek": 11400, "start": 133.0, "end": 134.0, "text": " We're going to do system design.", "tokens": [51314, 492, 434, 516, 281, 360, 1185, 1715, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10283106651859007, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.008190829306840897}, {"id": 28, "seek": 11400, "start": 134.0, "end": 136.0, "text": " We're going to make some decisions.", "tokens": [51364, 492, 434, 516, 281, 652, 512, 5327, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10283106651859007, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.008190829306840897}, {"id": 29, "seek": 11400, "start": 136.0, "end": 142.0, "text": " And eventually we'll write some code, and hopefully we'll have a working game to play by the end of the session here.", "tokens": [51464, 400, 4728, 321, 603, 2464, 512, 3089, 11, 293, 4696, 321, 603, 362, 257, 1364, 1216, 281, 862, 538, 264, 917, 295, 264, 5481, 510, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10283106651859007, "compression_ratio": 1.776271186440678, "no_speech_prob": 0.008190829306840897}, {"id": 30, "seek": 14200, "start": 142.0, "end": 147.0, "text": " Live coding, live AI codeings, always going to be fun time.", "tokens": [50364, 10385, 17720, 11, 1621, 7318, 3089, 1109, 11, 1009, 516, 281, 312, 1019, 565, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13741244440493378, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.007622959092259407}, {"id": 31, "seek": 14200, "start": 151.0, "end": 153.0, "text": " So just a level set.", "tokens": [50814, 407, 445, 257, 1496, 992, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13741244440493378, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.007622959092259407}, {"id": 32, "seek": 14200, "start": 153.0, "end": 160.0, "text": " I think any of us could draw a picture of the software development lifecycle, and there's lots of different ways we could express it.", "tokens": [50914, 286, 519, 604, 295, 505, 727, 2642, 257, 3036, 295, 264, 4722, 3250, 45722, 11, 293, 456, 311, 3195, 295, 819, 2098, 321, 727, 5109, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13741244440493378, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.007622959092259407}, {"id": 33, "seek": 14200, "start": 160.0, "end": 168.0, "text": " If you ask different folks, love different opinions about which steps there should be, and which order they should be in, and how fast you iterate on the different steps.", "tokens": [51264, 759, 291, 1029, 819, 4024, 11, 959, 819, 11819, 466, 597, 4439, 456, 820, 312, 11, 293, 597, 1668, 436, 820, 312, 294, 11, 293, 577, 2370, 291, 44497, 322, 264, 819, 4439, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13741244440493378, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.007622959092259407}, {"id": 34, "seek": 16800, "start": 168.0, "end": 171.0, "text": " But broadly speaking, we need to have a requirements.", "tokens": [50364, 583, 19511, 4124, 11, 321, 643, 281, 362, 257, 7728, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 35, "seek": 16800, "start": 171.0, "end": 173.0, "text": " We need to do a system design.", "tokens": [50514, 492, 643, 281, 360, 257, 1185, 1715, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 36, "seek": 16800, "start": 173.0, "end": 174.0, "text": " We need to plan.", "tokens": [50614, 492, 643, 281, 1393, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 37, "seek": 16800, "start": 174.0, "end": 175.0, "text": " We need to build.", "tokens": [50664, 492, 643, 281, 1322, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 38, "seek": 16800, "start": 175.0, "end": 176.0, "text": " We need to test.", "tokens": [50714, 492, 643, 281, 1500, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 39, "seek": 16800, "start": 176.0, "end": 177.0, "text": " We need to package.", "tokens": [50764, 492, 643, 281, 7372, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 40, "seek": 16800, "start": 177.0, "end": 180.0, "text": " And then we need to deploy and operate.", "tokens": [50814, 400, 550, 321, 643, 281, 7274, 293, 9651, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 41, "seek": 16800, "start": 180.0, "end": 186.0, "text": " And when we talk about vibe coding, in my opinion, we're really just talking about skipping to the right code step.", "tokens": [50964, 400, 562, 321, 751, 466, 14606, 17720, 11, 294, 452, 4800, 11, 321, 434, 534, 445, 1417, 466, 31533, 281, 264, 558, 3089, 1823, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 42, "seek": 16800, "start": 186.0, "end": 194.0, "text": " Which can be great for prototyping, or for trying something out, or maybe just scratching in it, in a language that you don't know or something.", "tokens": [51264, 3013, 393, 312, 869, 337, 46219, 3381, 11, 420, 337, 1382, 746, 484, 11, 420, 1310, 445, 29699, 294, 309, 11, 294, 257, 2856, 300, 291, 500, 380, 458, 420, 746, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10306657409667969, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.026931097730994225}, {"id": 43, "seek": 19400, "start": 194.0, "end": 199.0, "text": " But when it's time to actually do professional software development, we don't want to skip steps.", "tokens": [50364, 583, 562, 309, 311, 565, 281, 767, 360, 4843, 4722, 3250, 11, 321, 500, 380, 528, 281, 10023, 4439, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10852542470713131, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.04477563500404358}, {"id": 44, "seek": 19400, "start": 199.0, "end": 204.0, "text": " And so the question becomes, how can we use AI for all of these steps?", "tokens": [50614, 400, 370, 264, 1168, 3643, 11, 577, 393, 321, 764, 7318, 337, 439, 295, 613, 4439, 30, 50864], "temperature": 0.0, "avg_logprob": -0.10852542470713131, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.04477563500404358}, {"id": 45, "seek": 19400, "start": 204.0, "end": 207.0, "text": " There are a few approaches for this.", "tokens": [50864, 821, 366, 257, 1326, 11587, 337, 341, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10852542470713131, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.04477563500404358}, {"id": 46, "seek": 19400, "start": 207.0, "end": 209.0, "text": " We've got two Kiro products now.", "tokens": [51014, 492, 600, 658, 732, 591, 5182, 3383, 586, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10852542470713131, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.04477563500404358}, {"id": 47, "seek": 19400, "start": 209.0, "end": 214.0, "text": " We've got the Kiro IDE, which just went into GA in the last month or so.", "tokens": [51114, 492, 600, 658, 264, 591, 5182, 40930, 11, 597, 445, 1437, 666, 22841, 294, 264, 1036, 1618, 420, 370, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10852542470713131, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.04477563500404358}, {"id": 48, "seek": 19400, "start": 214.0, "end": 218.0, "text": " And then we also have the Kiro CLI, which we'll be using today.", "tokens": [51364, 400, 550, 321, 611, 362, 264, 591, 5182, 12855, 40, 11, 597, 321, 603, 312, 1228, 965, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10852542470713131, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.04477563500404358}, {"id": 49, "seek": 19400, "start": 218.0, "end": 222.0, "text": " The approach that we're going to use for the live coding session.", "tokens": [51564, 440, 3109, 300, 321, 434, 516, 281, 764, 337, 264, 1621, 17720, 5481, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10852542470713131, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.04477563500404358}, {"id": 50, "seek": 22200, "start": 222.0, "end": 225.0, "text": " You could use in Kiro IDE or Kiro CLI.", "tokens": [50364, 509, 727, 764, 294, 591, 5182, 40930, 420, 591, 5182, 12855, 40, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09196716442442777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.08572982251644135}, {"id": 51, "seek": 22200, "start": 225.0, "end": 230.0, "text": " This is really a preference thing in terms of the user experience that you like.", "tokens": [50514, 639, 307, 534, 257, 17502, 551, 294, 2115, 295, 264, 4195, 1752, 300, 291, 411, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09196716442442777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.08572982251644135}, {"id": 52, "seek": 22200, "start": 230.0, "end": 232.0, "text": " Kiro and I are both a bit of console geeks.", "tokens": [50764, 591, 5182, 293, 286, 366, 1293, 257, 857, 295, 11076, 1519, 24785, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09196716442442777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.08572982251644135}, {"id": 53, "seek": 22200, "start": 232.0, "end": 240.0, "text": " And so we're going to invite you all to be console geeks with us for Kiro CLI today.", "tokens": [50864, 400, 370, 321, 434, 516, 281, 7980, 291, 439, 281, 312, 11076, 1519, 24785, 365, 505, 337, 591, 5182, 12855, 40, 965, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09196716442442777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.08572982251644135}, {"id": 54, "seek": 22200, "start": 240.0, "end": 244.0, "text": " So the framework we're going to use is called AI DLC.", "tokens": [51264, 407, 264, 8388, 321, 434, 516, 281, 764, 307, 1219, 7318, 30272, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09196716442442777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.08572982251644135}, {"id": 55, "seek": 22200, "start": 244.0, "end": 247.0, "text": " And we want to show one way that you can approach this.", "tokens": [51464, 400, 321, 528, 281, 855, 472, 636, 300, 291, 393, 3109, 341, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09196716442442777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.08572982251644135}, {"id": 56, "seek": 22200, "start": 247.0, "end": 248.0, "text": " This is not the only way.", "tokens": [51614, 639, 307, 406, 264, 787, 636, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09196716442442777, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.08572982251644135}, {"id": 57, "seek": 24800, "start": 248.0, "end": 251.0, "text": " Kiro IDE has spectrum of development built into it.", "tokens": [50364, 591, 5182, 40930, 575, 11143, 295, 3250, 3094, 666, 309, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10723460617885795, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.025659844279289246}, {"id": 58, "seek": 24800, "start": 251.0, "end": 258.0, "text": " And so that's another way that you can approach using AI powered requirements and system design and planning.", "tokens": [50514, 400, 370, 300, 311, 1071, 636, 300, 291, 393, 3109, 1228, 7318, 17786, 7728, 293, 1185, 1715, 293, 5038, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10723460617885795, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.025659844279289246}, {"id": 59, "seek": 24800, "start": 258.0, "end": 260.0, "text": " AI DLC is another way.", "tokens": [50864, 7318, 30272, 307, 1071, 636, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10723460617885795, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.025659844279289246}, {"id": 60, "seek": 24800, "start": 260.0, "end": 266.0, "text": " It's a little bit more granular than what you would see in the Kiro IDE.", "tokens": [50964, 467, 311, 257, 707, 857, 544, 39962, 813, 437, 291, 576, 536, 294, 264, 591, 5182, 40930, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10723460617885795, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.025659844279289246}, {"id": 61, "seek": 24800, "start": 266.0, "end": 271.0, "text": " And this is something that we've actually opened sourced and released on GitHub.", "tokens": [51264, 400, 341, 307, 746, 300, 321, 600, 767, 5625, 11006, 1232, 293, 4736, 322, 23331, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10723460617885795, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.025659844279289246}, {"id": 62, "seek": 24800, "start": 271.0, "end": 273.0, "text": " You'll last a week or two.", "tokens": [51514, 509, 603, 1036, 257, 1243, 420, 732, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10723460617885795, "compression_ratio": 1.5271966527196652, "no_speech_prob": 0.025659844279289246}, {"id": 63, "seek": 27300, "start": 273.0, "end": 277.0, "text": " So we won't go too much into the details of AI DLC.", "tokens": [50364, 407, 321, 1582, 380, 352, 886, 709, 666, 264, 4365, 295, 7318, 30272, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09128266848050631, "compression_ratio": 1.7328767123287672, "no_speech_prob": 0.044721368700265884}, {"id": 64, "seek": 27300, "start": 277.0, "end": 281.0, "text": " It's going to be in the background sort of guiding us along as we go through this steps.", "tokens": [50564, 467, 311, 516, 281, 312, 294, 264, 3678, 1333, 295, 25061, 505, 2051, 382, 321, 352, 807, 341, 4439, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09128266848050631, "compression_ratio": 1.7328767123287672, "no_speech_prob": 0.044721368700265884}, {"id": 65, "seek": 27300, "start": 281.0, "end": 285.0, "text": " But we have a slide at the end that has a QR code and a link.", "tokens": [50764, 583, 321, 362, 257, 4137, 412, 264, 917, 300, 575, 257, 32784, 3089, 293, 257, 2113, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09128266848050631, "compression_ratio": 1.7328767123287672, "no_speech_prob": 0.044721368700265884}, {"id": 66, "seek": 27300, "start": 285.0, "end": 290.0, "text": " So if you like what you see and you'd like to try out AI DLC for yourself and some of these prompts,", "tokens": [50964, 407, 498, 291, 411, 437, 291, 536, 293, 291, 1116, 411, 281, 853, 484, 7318, 30272, 337, 1803, 293, 512, 295, 613, 41095, 11, 51214], "temperature": 0.0, "avg_logprob": -0.09128266848050631, "compression_ratio": 1.7328767123287672, "no_speech_prob": 0.044721368700265884}, {"id": 67, "seek": 27300, "start": 290.0, "end": 294.0, "text": " you're welcome to do so and we'd love to hear your feedback.", "tokens": [51214, 291, 434, 2928, 281, 360, 370, 293, 321, 1116, 959, 281, 1568, 428, 5824, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09128266848050631, "compression_ratio": 1.7328767123287672, "no_speech_prob": 0.044721368700265884}, {"id": 68, "seek": 27300, "start": 294.0, "end": 302.0, "text": " So AI DLC roughly speaking has this concept of an inception phase where we're going through the requirements and the design and the planning.", "tokens": [51414, 407, 7318, 30272, 9810, 4124, 575, 341, 3410, 295, 364, 49834, 5574, 689, 321, 434, 516, 807, 264, 7728, 293, 264, 1715, 293, 264, 5038, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09128266848050631, "compression_ratio": 1.7328767123287672, "no_speech_prob": 0.044721368700265884}, {"id": 69, "seek": 30200, "start": 302.0, "end": 307.0, "text": " Construction phase where we're doing the building and the packaging and then an operation phase.", "tokens": [50364, 40017, 5574, 689, 321, 434, 884, 264, 2390, 293, 264, 16836, 293, 550, 364, 6916, 5574, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08008708416576117, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0025953580625355244}, {"id": 70, "seek": 30200, "start": 307.0, "end": 321.0, "text": " We're going to go through the inception and the construction phase today and get hopefully to a working game by the end of the hour here.", "tokens": [50614, 492, 434, 516, 281, 352, 807, 264, 49834, 293, 264, 6435, 5574, 965, 293, 483, 4696, 281, 257, 1364, 1216, 538, 264, 917, 295, 264, 1773, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08008708416576117, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0025953580625355244}, {"id": 71, "seek": 30200, "start": 321.0, "end": 329.0, "text": " So I want to spend a few minutes on agent composition before we jump into the coding.", "tokens": [51314, 407, 286, 528, 281, 3496, 257, 1326, 2077, 322, 9461, 12686, 949, 321, 3012, 666, 264, 17720, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08008708416576117, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0025953580625355244}, {"id": 72, "seek": 32900, "start": 329.0, "end": 333.0, "text": " So this I think is a commonly accepted definition of an agent.", "tokens": [50364, 407, 341, 286, 519, 307, 257, 12719, 9035, 7123, 295, 364, 9461, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09443774367823746, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.017727527767419815}, {"id": 73, "seek": 32900, "start": 333.0, "end": 335.0, "text": " This is Simon Wilson's definition.", "tokens": [50564, 639, 307, 13193, 15388, 311, 7123, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09443774367823746, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.017727527767419815}, {"id": 74, "seek": 32900, "start": 335.0, "end": 339.0, "text": " An LLM agent runs tools in a loop to achieve a goal.", "tokens": [50664, 1107, 441, 43, 44, 9461, 6676, 3873, 294, 257, 6367, 281, 4584, 257, 3387, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09443774367823746, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.017727527767419815}, {"id": 75, "seek": 32900, "start": 339.0, "end": 341.0, "text": " Very simple.", "tokens": [50864, 4372, 2199, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09443774367823746, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.017727527767419815}, {"id": 76, "seek": 32900, "start": 341.0, "end": 345.0, "text": " The simplicity is what we like for today's purposes.", "tokens": [50964, 440, 25632, 307, 437, 321, 411, 337, 965, 311, 9932, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09443774367823746, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.017727527767419815}, {"id": 77, "seek": 32900, "start": 345.0, "end": 347.0, "text": " I'm going to modify that a little bit.", "tokens": [51164, 286, 478, 516, 281, 16927, 300, 257, 707, 857, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09443774367823746, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.017727527767419815}, {"id": 78, "seek": 32900, "start": 347.0, "end": 354.0, "text": " And I'm going to say an LLM agent runs tools in a loop while building context to achieve a goal.", "tokens": [51264, 400, 286, 478, 516, 281, 584, 364, 441, 43, 44, 9461, 6676, 3873, 294, 257, 6367, 1339, 2390, 4319, 281, 4584, 257, 3387, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09443774367823746, "compression_ratio": 1.7004830917874396, "no_speech_prob": 0.017727527767419815}, {"id": 79, "seek": 35400, "start": 354.0, "end": 364.0, "text": " And I'll elaborate on that a minute here, but we're finding is managing the context and the tools are critically important to getting the results that we want.", "tokens": [50364, 400, 286, 603, 20945, 322, 300, 257, 3456, 510, 11, 457, 321, 434, 5006, 307, 11642, 264, 4319, 293, 264, 3873, 366, 22797, 1021, 281, 1242, 264, 3542, 300, 321, 528, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09428882598876953, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.019717145711183548}, {"id": 80, "seek": 35400, "start": 364.0, "end": 368.0, "text": " Out of this agentic software development process.", "tokens": [50864, 5925, 295, 341, 623, 317, 299, 4722, 3250, 1399, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09428882598876953, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.019717145711183548}, {"id": 81, "seek": 35400, "start": 368.0, "end": 373.0, "text": " If we skip this step or if we don't get it right, we're not going to get the results that we want.", "tokens": [51064, 759, 321, 10023, 341, 1823, 420, 498, 321, 500, 380, 483, 309, 558, 11, 321, 434, 406, 516, 281, 483, 264, 3542, 300, 321, 528, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09428882598876953, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.019717145711183548}, {"id": 82, "seek": 35400, "start": 373.0, "end": 382.0, "text": " So this becomes an important step in the workflow to making sure that our agent is configured correctly.", "tokens": [51314, 407, 341, 3643, 364, 1021, 1823, 294, 264, 20993, 281, 1455, 988, 300, 527, 9461, 307, 30538, 8944, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09428882598876953, "compression_ratio": 1.7136929460580912, "no_speech_prob": 0.019717145711183548}, {"id": 83, "seek": 38200, "start": 382.0, "end": 385.0, "text": " So here's a diagram. Again, an agent.", "tokens": [50364, 407, 510, 311, 257, 10686, 13, 3764, 11, 364, 9461, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11945353209517384, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.006864645052701235}, {"id": 84, "seek": 38200, "start": 385.0, "end": 388.0, "text": " Agentic loop is nice and simple.", "tokens": [50514, 2725, 317, 299, 6367, 307, 1481, 293, 2199, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11945353209517384, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.006864645052701235}, {"id": 85, "seek": 38200, "start": 388.0, "end": 391.0, "text": " The user, in this case, we're using the Kira CLI.", "tokens": [50664, 440, 4195, 11, 294, 341, 1389, 11, 321, 434, 1228, 264, 591, 4271, 12855, 40, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11945353209517384, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.006864645052701235}, {"id": 86, "seek": 38200, "start": 391.0, "end": 394.0, "text": " We're going to ask it to do something as input.", "tokens": [50814, 492, 434, 516, 281, 1029, 309, 281, 360, 746, 382, 4846, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11945353209517384, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.006864645052701235}, {"id": 87, "seek": 38200, "start": 394.0, "end": 396.0, "text": " It's going to take that into its context.", "tokens": [50964, 467, 311, 516, 281, 747, 300, 666, 1080, 4319, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11945353209517384, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.006864645052701235}, {"id": 88, "seek": 38200, "start": 396.0, "end": 400.0, "text": " We're going to pass it to the LLM to decide what to do about our goal.", "tokens": [51064, 492, 434, 516, 281, 1320, 309, 281, 264, 441, 43, 44, 281, 4536, 437, 281, 360, 466, 527, 3387, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11945353209517384, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.006864645052701235}, {"id": 89, "seek": 38200, "start": 400.0, "end": 404.0, "text": " The LLM is going to come back and say, I want to take some action using a tool.", "tokens": [51264, 440, 441, 43, 44, 307, 516, 281, 808, 646, 293, 584, 11, 286, 528, 281, 747, 512, 3069, 1228, 257, 2290, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11945353209517384, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.006864645052701235}, {"id": 90, "seek": 38200, "start": 404.0, "end": 407.0, "text": " We're going to go call that tool, whatever it is.", "tokens": [51464, 492, 434, 516, 281, 352, 818, 300, 2290, 11, 2035, 309, 307, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11945353209517384, "compression_ratio": 1.7341772151898733, "no_speech_prob": 0.006864645052701235}, {"id": 91, "seek": 40700, "start": 407.0, "end": 409.0, "text": " The tool is going to return the result.", "tokens": [50364, 440, 2290, 307, 516, 281, 2736, 264, 1874, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11081095195951916, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.015779772773385048}, {"id": 92, "seek": 40700, "start": 409.0, "end": 414.0, "text": " That goes back into the context and we go back to the agent again and we say, okay, we ran the tool.", "tokens": [50464, 663, 1709, 646, 666, 264, 4319, 293, 321, 352, 646, 281, 264, 9461, 797, 293, 321, 584, 11, 1392, 11, 321, 5872, 264, 2290, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11081095195951916, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.015779772773385048}, {"id": 93, "seek": 40700, "start": 414.0, "end": 416.0, "text": " What would you like to do next?", "tokens": [50714, 708, 576, 291, 411, 281, 360, 958, 30, 50814], "temperature": 0.0, "avg_logprob": -0.11081095195951916, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.015779772773385048}, {"id": 94, "seek": 40700, "start": 416.0, "end": 424.0, "text": " We'll keep iterating until the agent decides until the LLM decides we're done and we're ready to give a response back to the user.", "tokens": [50814, 492, 603, 1066, 17138, 990, 1826, 264, 9461, 14898, 1826, 264, 441, 43, 44, 14898, 321, 434, 1096, 293, 321, 434, 1919, 281, 976, 257, 4134, 646, 281, 264, 4195, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11081095195951916, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.015779772773385048}, {"id": 95, "seek": 40700, "start": 424.0, "end": 428.0, "text": " So let's talk about each of these parts briefly.", "tokens": [51214, 407, 718, 311, 751, 466, 1184, 295, 613, 3166, 10515, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11081095195951916, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.015779772773385048}, {"id": 96, "seek": 40700, "start": 428.0, "end": 431.0, "text": " Starting with the LLM.", "tokens": [51414, 16217, 365, 264, 441, 43, 44, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11081095195951916, "compression_ratio": 1.6816143497757847, "no_speech_prob": 0.015779772773385048}, {"id": 97, "seek": 43100, "start": 431.0, "end": 441.0, "text": " So for today's purposes, we're going to care about applied AI and not the gory details about how LLM works.", "tokens": [50364, 407, 337, 965, 311, 9932, 11, 321, 434, 516, 281, 1127, 466, 6456, 7318, 293, 406, 264, 290, 827, 4365, 466, 577, 441, 43, 44, 1985, 13, 50864], "temperature": 0.0, "avg_logprob": -0.103645920753479, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.05535070598125458}, {"id": 98, "seek": 43100, "start": 441.0, "end": 450.0, "text": " So for our purposes today, I'm not going to try and talk about the semantic relationships that are in the model weights and", "tokens": [50864, 407, 337, 527, 9932, 965, 11, 286, 478, 406, 516, 281, 853, 293, 751, 466, 264, 47982, 6159, 300, 366, 294, 264, 2316, 17443, 293, 51314], "temperature": 0.0, "avg_logprob": -0.103645920753479, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.05535070598125458}, {"id": 99, "seek": 43100, "start": 450.0, "end": 452.0, "text": " and dimensional vector space and all that stuff.", "tokens": [51314, 293, 18795, 8062, 1901, 293, 439, 300, 1507, 13, 51414], "temperature": 0.0, "avg_logprob": -0.103645920753479, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.05535070598125458}, {"id": 100, "seek": 43100, "start": 452.0, "end": 456.0, "text": " I'm sure some of the folks in the room are experts at this stuff.", "tokens": [51414, 286, 478, 988, 512, 295, 264, 4024, 294, 264, 1808, 366, 8572, 412, 341, 1507, 13, 51614], "temperature": 0.0, "avg_logprob": -0.103645920753479, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.05535070598125458}, {"id": 101, "seek": 43100, "start": 456.0, "end": 460.0, "text": " What we're going to say is an LLM gives us reasoning.", "tokens": [51614, 708, 321, 434, 516, 281, 584, 307, 364, 441, 43, 44, 2709, 505, 21577, 13, 51814], "temperature": 0.0, "avg_logprob": -0.103645920753479, "compression_ratio": 1.680672268907563, "no_speech_prob": 0.05535070598125458}, {"id": 102, "seek": 46000, "start": 460.0, "end": 464.0, "text": " We could debate how it gives us reasoning or the details.", "tokens": [50364, 492, 727, 7958, 577, 309, 2709, 505, 21577, 420, 264, 4365, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1080007639017191, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.008238101378083229}, {"id": 103, "seek": 46000, "start": 464.0, "end": 471.0, "text": " But if we ask it to reason about something, it will do that and it will come back and come up with an approach for a problem.", "tokens": [50564, 583, 498, 321, 1029, 309, 281, 1778, 466, 746, 11, 309, 486, 360, 300, 293, 309, 486, 808, 646, 293, 808, 493, 365, 364, 3109, 337, 257, 1154, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1080007639017191, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.008238101378083229}, {"id": 104, "seek": 46000, "start": 471.0, "end": 476.0, "text": " We're also not going to talk about auto regression and multi-head attention and all that stuff.", "tokens": [50914, 492, 434, 611, 406, 516, 281, 751, 466, 8399, 24590, 293, 4825, 12, 1934, 3202, 293, 439, 300, 1507, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1080007639017191, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.008238101378083229}, {"id": 105, "seek": 46000, "start": 476.0, "end": 481.0, "text": " What we're going to say is the model has knowledge embedded into the model weights.", "tokens": [51164, 708, 321, 434, 516, 281, 584, 307, 264, 2316, 575, 3601, 16741, 666, 264, 2316, 17443, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1080007639017191, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.008238101378083229}, {"id": 106, "seek": 46000, "start": 481.0, "end": 486.0, "text": " What we would call latent knowledge that's been trained into the model during the pre-training step.", "tokens": [51414, 708, 321, 576, 818, 48994, 3601, 300, 311, 668, 8895, 666, 264, 2316, 1830, 264, 659, 12, 17227, 1760, 1823, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1080007639017191, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.008238101378083229}, {"id": 107, "seek": 48600, "start": 486.0, "end": 489.0, "text": " No stuff.", "tokens": [50364, 883, 1507, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12035435676574707, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.014406482689082623}, {"id": 108, "seek": 48600, "start": 489.0, "end": 492.0, "text": " How it does the recall will leave for a different session.", "tokens": [50514, 1012, 309, 775, 264, 9901, 486, 1856, 337, 257, 819, 5481, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12035435676574707, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.014406482689082623}, {"id": 109, "seek": 48600, "start": 492.0, "end": 497.0, "text": " But our LLM is going to give us the ability to reason and it's going to have latent knowledge.", "tokens": [50664, 583, 527, 441, 43, 44, 307, 516, 281, 976, 505, 264, 3485, 281, 1778, 293, 309, 311, 516, 281, 362, 48994, 3601, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12035435676574707, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.014406482689082623}, {"id": 110, "seek": 48600, "start": 497.0, "end": 502.0, "text": " Facts that it knows about the world.", "tokens": [50914, 479, 15295, 300, 309, 3255, 466, 264, 1002, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12035435676574707, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.014406482689082623}, {"id": 111, "seek": 48600, "start": 502.0, "end": 509.0, "text": " So that's great, but next we need working memory for the model to be able to understand what it's doing and keep track of where we're going.", "tokens": [51164, 407, 300, 311, 869, 11, 457, 958, 321, 643, 1364, 4675, 337, 264, 2316, 281, 312, 1075, 281, 1223, 437, 309, 311, 884, 293, 1066, 2837, 295, 689, 321, 434, 516, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12035435676574707, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.014406482689082623}, {"id": 112, "seek": 48600, "start": 509.0, "end": 513.0, "text": " And that's the context window.", "tokens": [51514, 400, 300, 311, 264, 4319, 4910, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12035435676574707, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.014406482689082623}, {"id": 113, "seek": 51300, "start": 513.0, "end": 517.0, "text": " So here's a very simplified sort of mental model of the context window.", "tokens": [50364, 407, 510, 311, 257, 588, 26335, 1333, 295, 4973, 2316, 295, 264, 4319, 4910, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10699842034316645, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.036332953721284866}, {"id": 114, "seek": 51300, "start": 517.0, "end": 520.0, "text": " It's everything that's in the model's working memory.", "tokens": [50564, 467, 311, 1203, 300, 311, 294, 264, 2316, 311, 1364, 4675, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10699842034316645, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.036332953721284866}, {"id": 115, "seek": 51300, "start": 520.0, "end": 524.0, "text": " So starting with system prompts that are behind the scenes, guiding the agent's behavior,", "tokens": [50714, 407, 2891, 365, 1185, 41095, 300, 366, 2261, 264, 8026, 11, 25061, 264, 9461, 311, 5223, 11, 50914], "temperature": 0.0, "avg_logprob": -0.10699842034316645, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.036332953721284866}, {"id": 116, "seek": 51300, "start": 524.0, "end": 527.0, "text": " any context files that we've loaded.", "tokens": [50914, 604, 4319, 7098, 300, 321, 600, 13210, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10699842034316645, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.036332953721284866}, {"id": 117, "seek": 51300, "start": 527.0, "end": 529.0, "text": " We're going to see example of that here in a minute.", "tokens": [51064, 492, 434, 516, 281, 536, 1365, 295, 300, 510, 294, 257, 3456, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10699842034316645, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.036332953721284866}, {"id": 118, "seek": 51300, "start": 529.0, "end": 531.0, "text": " The history of our chat so far.", "tokens": [51164, 440, 2503, 295, 527, 5081, 370, 1400, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10699842034316645, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.036332953721284866}, {"id": 119, "seek": 51300, "start": 531.0, "end": 536.0, "text": " So we've opened up our chat window, we're going back and forth, working on a problem with the agent.", "tokens": [51264, 407, 321, 600, 5625, 493, 527, 5081, 4910, 11, 321, 434, 516, 646, 293, 5220, 11, 1364, 322, 257, 1154, 365, 264, 9461, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10699842034316645, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.036332953721284866}, {"id": 120, "seek": 51300, "start": 536.0, "end": 538.0, "text": " That's all in the context window.", "tokens": [51514, 663, 311, 439, 294, 264, 4319, 4910, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10699842034316645, "compression_ratio": 1.7677902621722847, "no_speech_prob": 0.036332953721284866}, {"id": 121, "seek": 53800, "start": 538.0, "end": 543.0, "text": " Any source code that it's pulled up and looked at if we're working on an existing project.", "tokens": [50364, 2639, 4009, 3089, 300, 309, 311, 7373, 493, 293, 2956, 412, 498, 321, 434, 1364, 322, 364, 6741, 1716, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0799191429501488, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.026725027710199356}, {"id": 122, "seek": 53800, "start": 543.0, "end": 546.0, "text": " The output of all the tools that we've called so far.", "tokens": [50614, 440, 5598, 295, 439, 264, 3873, 300, 321, 600, 1219, 370, 1400, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0799191429501488, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.026725027710199356}, {"id": 123, "seek": 53800, "start": 546.0, "end": 550.0, "text": " And then hopefully some free space so that we can keep going.", "tokens": [50764, 400, 550, 4696, 512, 1737, 1901, 370, 300, 321, 393, 1066, 516, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0799191429501488, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.026725027710199356}, {"id": 124, "seek": 53800, "start": 550.0, "end": 555.0, "text": " It's a managing this context window is extremely important for us to get the results that we want.", "tokens": [50964, 467, 311, 257, 11642, 341, 4319, 4910, 307, 4664, 1021, 337, 505, 281, 483, 264, 3542, 300, 321, 528, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0799191429501488, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.026725027710199356}, {"id": 125, "seek": 53800, "start": 555.0, "end": 562.0, "text": " If I'm asking the agent to work with some custom library that I've developed and it doesn't know anything about that custom library,", "tokens": [51214, 759, 286, 478, 3365, 264, 9461, 281, 589, 365, 512, 2375, 6405, 300, 286, 600, 4743, 293, 309, 1177, 380, 458, 1340, 466, 300, 2375, 6405, 11, 51564], "temperature": 0.0, "avg_logprob": -0.0799191429501488, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.026725027710199356}, {"id": 126, "seek": 56200, "start": 562.0, "end": 568.0, "text": " we're going to get a very different result than if I were to load up the documentation for that library into the context window.", "tokens": [50364, 321, 434, 516, 281, 483, 257, 588, 819, 1874, 813, 498, 286, 645, 281, 3677, 493, 264, 14333, 337, 300, 6405, 666, 264, 4319, 4910, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11182587645774664, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.009662085212767124}, {"id": 127, "seek": 56200, "start": 568.0, "end": 574.0, "text": " So we want to make sure that we get this right up front.", "tokens": [50664, 407, 321, 528, 281, 652, 988, 300, 321, 483, 341, 558, 493, 1868, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11182587645774664, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.009662085212767124}, {"id": 128, "seek": 56200, "start": 574.0, "end": 580.0, "text": " And then finally for this section, tool selection and tool execution.", "tokens": [50964, 400, 550, 2721, 337, 341, 3541, 11, 2290, 9450, 293, 2290, 15058, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11182587645774664, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.009662085212767124}, {"id": 129, "seek": 56200, "start": 580.0, "end": 587.0, "text": " MCP has taken over in the last six months or so as the de facto standard for tool use with agents.", "tokens": [51264, 8797, 47, 575, 2726, 670, 294, 264, 1036, 2309, 2493, 420, 370, 382, 264, 368, 42225, 3832, 337, 2290, 764, 365, 12554, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11182587645774664, "compression_ratio": 1.5663716814159292, "no_speech_prob": 0.009662085212767124}, {"id": 130, "seek": 58700, "start": 588.0, "end": 593.0, "text": " For those of you who aren't familiar, I think MCP is the USB protocol for agents.", "tokens": [50414, 1171, 729, 295, 291, 567, 3212, 380, 4963, 11, 286, 519, 8797, 47, 307, 264, 10109, 10336, 337, 12554, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09689326758857246, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.09448713809251785}, {"id": 131, "seek": 58700, "start": 593.0, "end": 601.0, "text": " I can take a plug, it'll plug into any agent and it'll let me plug skills into the agent so that it learns how to do something new.", "tokens": [50664, 286, 393, 747, 257, 5452, 11, 309, 603, 5452, 666, 604, 9461, 293, 309, 603, 718, 385, 5452, 3942, 666, 264, 9461, 370, 300, 309, 27152, 577, 281, 360, 746, 777, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09689326758857246, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.09448713809251785}, {"id": 132, "seek": 58700, "start": 601.0, "end": 607.0, "text": " It'll let me give it access to reading resources from another location that it needs.", "tokens": [51064, 467, 603, 718, 385, 976, 309, 2105, 281, 3760, 3593, 490, 1071, 4914, 300, 309, 2203, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09689326758857246, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.09448713809251785}, {"id": 133, "seek": 58700, "start": 607.0, "end": 610.0, "text": " And possibly give it the ability to take action.", "tokens": [51364, 400, 6264, 976, 309, 264, 3485, 281, 747, 3069, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09689326758857246, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.09448713809251785}, {"id": 134, "seek": 58700, "start": 610.0, "end": 616.0, "text": " So almost like an API where it can do a mutative or make a change on another system.", "tokens": [51514, 407, 1920, 411, 364, 9362, 689, 309, 393, 360, 257, 5839, 1166, 420, 652, 257, 1319, 322, 1071, 1185, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09689326758857246, "compression_ratio": 1.6463878326996197, "no_speech_prob": 0.09448713809251785}, {"id": 135, "seek": 61600, "start": 616.0, "end": 624.0, "text": " And so getting the MCP tools right again is another critical piece of getting ready for us to go do our agents at software development.", "tokens": [50364, 400, 370, 1242, 264, 8797, 47, 3873, 558, 797, 307, 1071, 4924, 2522, 295, 1242, 1919, 337, 505, 281, 352, 360, 527, 12554, 412, 4722, 3250, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11783696443606646, "compression_ratio": 1.671875, "no_speech_prob": 0.008063524030148983}, {"id": 136, "seek": 61600, "start": 624.0, "end": 640.0, "text": " If we need access to JIRA, if we need access to our internal wiki, if we need access to go search the internet, we need to make sure that any of these tools that we need are plugged in.", "tokens": [50764, 759, 321, 643, 2105, 281, 50172, 3750, 11, 498, 321, 643, 2105, 281, 527, 6920, 261, 9850, 11, 498, 321, 643, 2105, 281, 352, 3164, 264, 4705, 11, 321, 643, 281, 652, 988, 300, 604, 295, 613, 3873, 300, 321, 643, 366, 25679, 294, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11783696443606646, "compression_ratio": 1.671875, "no_speech_prob": 0.008063524030148983}, {"id": 137, "seek": 64000, "start": 640.0, "end": 655.0, "text": " So we talked about the agentic loop and how having the right LLM model to do reasoning, the right context window content in order for it to have that in its working memory what it needs to know.", "tokens": [50364, 407, 321, 2825, 466, 264, 9461, 299, 6367, 293, 577, 1419, 264, 558, 441, 43, 44, 2316, 281, 360, 21577, 11, 264, 558, 4319, 4910, 2701, 294, 1668, 337, 309, 281, 362, 300, 294, 1080, 1364, 4675, 437, 309, 2203, 281, 458, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1364429473876953, "compression_ratio": 1.6546391752577319, "no_speech_prob": 0.07680949568748474}, {"id": 138, "seek": 64000, "start": 655.0, "end": 658.0, "text": " And we talked about having the right tools plugged in.", "tokens": [51114, 400, 321, 2825, 466, 1419, 264, 558, 3873, 25679, 294, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1364429473876953, "compression_ratio": 1.6546391752577319, "no_speech_prob": 0.07680949568748474}, {"id": 139, "seek": 64000, "start": 658.0, "end": 662.0, "text": " These are all the ingredients to having the configuration that we need.", "tokens": [51264, 1981, 366, 439, 264, 6952, 281, 1419, 264, 11694, 300, 321, 643, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1364429473876953, "compression_ratio": 1.6546391752577319, "no_speech_prob": 0.07680949568748474}, {"id": 140, "seek": 66200, "start": 662.0, "end": 668.0, "text": " So today we're going to be using Kiro CLI to do our live coding session.", "tokens": [50364, 407, 965, 321, 434, 516, 281, 312, 1228, 591, 5182, 12855, 40, 281, 360, 527, 1621, 17720, 5481, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1233024854917784, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.12152128666639328}, {"id": 141, "seek": 66200, "start": 668.0, "end": 671.0, "text": " And in Kiro CLI we have this concept of a custom agent.", "tokens": [50664, 400, 294, 591, 5182, 12855, 40, 321, 362, 341, 3410, 295, 257, 2375, 9461, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1233024854917784, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.12152128666639328}, {"id": 142, "seek": 66200, "start": 671.0, "end": 676.0, "text": " And the custom agent is really a bundle of all of those things that we can store.", "tokens": [50814, 400, 264, 2375, 9461, 307, 534, 257, 24438, 295, 439, 295, 729, 721, 300, 321, 393, 3531, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1233024854917784, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.12152128666639328}, {"id": 143, "seek": 66200, "start": 676.0, "end": 678.0, "text": " We can share with our teammates.", "tokens": [51064, 492, 393, 2073, 365, 527, 20461, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1233024854917784, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.12152128666639328}, {"id": 144, "seek": 66200, "start": 678.0, "end": 682.0, "text": " We can publish internally if we want other teams to be able to use it.", "tokens": [51164, 492, 393, 11374, 19501, 498, 321, 528, 661, 5491, 281, 312, 1075, 281, 764, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1233024854917784, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.12152128666639328}, {"id": 145, "seek": 66200, "start": 682.0, "end": 684.0, "text": " We could just save it for ourselves.", "tokens": [51364, 492, 727, 445, 3155, 309, 337, 4175, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1233024854917784, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.12152128666639328}, {"id": 146, "seek": 66200, "start": 684.0, "end": 688.0, "text": " But it gives us the ability to combine our model selection.", "tokens": [51464, 583, 309, 2709, 505, 264, 3485, 281, 10432, 527, 2316, 9450, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1233024854917784, "compression_ratio": 1.6844262295081966, "no_speech_prob": 0.12152128666639328}, {"id": 147, "seek": 68800, "start": 689.0, "end": 700.0, "text": " What context we want in the context window system prompt static resource files hooks which give us the ability to do some dynamic context.", "tokens": [50414, 708, 4319, 321, 528, 294, 264, 4319, 4910, 1185, 12391, 13437, 7684, 7098, 26485, 597, 976, 505, 264, 3485, 281, 360, 512, 8546, 4319, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14225171162531927, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.08313734084367752}, {"id": 148, "seek": 68800, "start": 700.0, "end": 705.0, "text": " And then tools which MCP servers do I want to be using.", "tokens": [50964, 400, 550, 3873, 597, 8797, 47, 15909, 360, 286, 528, 281, 312, 1228, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14225171162531927, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.08313734084367752}, {"id": 149, "seek": 68800, "start": 705.0, "end": 711.0, "text": " Within each MCP server that could be exposing multiple tools which ones do I want to allow the agent to use.", "tokens": [51214, 15996, 1184, 8797, 47, 7154, 300, 727, 312, 33178, 3866, 3873, 597, 2306, 360, 286, 528, 281, 2089, 264, 9461, 281, 764, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14225171162531927, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.08313734084367752}, {"id": 150, "seek": 68800, "start": 711.0, "end": 713.0, "text": " Maybe not all of them.", "tokens": [51514, 2704, 406, 439, 295, 552, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14225171162531927, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.08313734084367752}, {"id": 151, "seek": 71300, "start": 713.0, "end": 716.0, "text": " I want to give them different names.", "tokens": [50364, 286, 528, 281, 976, 552, 819, 5288, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 152, "seek": 71300, "start": 716.0, "end": 720.0, "text": " So all of this goes into one file which Kiro will show us here in a minute.", "tokens": [50514, 407, 439, 295, 341, 1709, 666, 472, 3991, 597, 591, 5182, 486, 855, 505, 510, 294, 257, 3456, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 153, "seek": 71300, "start": 720.0, "end": 722.0, "text": " That lets us build our custom agent.", "tokens": [50714, 663, 6653, 505, 1322, 527, 2375, 9461, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 154, "seek": 71300, "start": 722.0, "end": 724.0, "text": " So we're going to do that first.", "tokens": [50814, 407, 321, 434, 516, 281, 360, 300, 700, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 155, "seek": 71300, "start": 724.0, "end": 727.0, "text": " Kiro is going to walk us through building our custom agent.", "tokens": [50914, 591, 5182, 307, 516, 281, 1792, 505, 807, 2390, 527, 2375, 9461, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 156, "seek": 71300, "start": 727.0, "end": 732.0, "text": " And then we'll come back briefly and talk about what we're going to go build.", "tokens": [51064, 400, 550, 321, 603, 808, 646, 10515, 293, 751, 466, 437, 321, 434, 516, 281, 352, 1322, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 157, "seek": 71300, "start": 732.0, "end": 735.0, "text": " So with that.", "tokens": [51314, 407, 365, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 158, "seek": 71300, "start": 735.0, "end": 736.0, "text": " Kiro.", "tokens": [51464, 591, 5182, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 159, "seek": 71300, "start": 736.0, "end": 737.0, "text": " Overdo.", "tokens": [51514, 4886, 2595, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 160, "seek": 71300, "start": 737.0, "end": 739.0, "text": " Thanks everyone.", "tokens": [51564, 2561, 1518, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1965515830300071, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09303469955921173}, {"id": 161, "seek": 73900, "start": 739.0, "end": 754.0, "text": " Let's work yet.", "tokens": [50364, 961, 311, 589, 1939, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3808613190284142, "compression_ratio": 1.1743119266055047, "no_speech_prob": 0.12111933529376984}, {"id": 162, "seek": 73900, "start": 754.0, "end": 756.0, "text": " The joys of alive demo.", "tokens": [51114, 440, 1488, 749, 295, 5465, 10723, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3808613190284142, "compression_ratio": 1.1743119266055047, "no_speech_prob": 0.12111933529376984}, {"id": 163, "seek": 73900, "start": 756.0, "end": 757.0, "text": " Yep.", "tokens": [51214, 7010, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3808613190284142, "compression_ratio": 1.1743119266055047, "no_speech_prob": 0.12111933529376984}, {"id": 164, "seek": 73900, "start": 757.0, "end": 760.0, "text": " Good fun.", "tokens": [51264, 2205, 1019, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3808613190284142, "compression_ratio": 1.1743119266055047, "no_speech_prob": 0.12111933529376984}, {"id": 165, "seek": 73900, "start": 760.0, "end": 762.0, "text": " I saw a flicker.", "tokens": [51414, 286, 1866, 257, 22774, 260, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3808613190284142, "compression_ratio": 1.1743119266055047, "no_speech_prob": 0.12111933529376984}, {"id": 166, "seek": 73900, "start": 762.0, "end": 763.0, "text": " Yeah.", "tokens": [51514, 865, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3808613190284142, "compression_ratio": 1.1743119266055047, "no_speech_prob": 0.12111933529376984}, {"id": 167, "seek": 73900, "start": 763.0, "end": 767.0, "text": " I've worked two seconds before we started as well.", "tokens": [51564, 286, 600, 2732, 732, 3949, 949, 321, 1409, 382, 731, 13, 51764], "temperature": 0.0, "avg_logprob": -0.3808613190284142, "compression_ratio": 1.1743119266055047, "no_speech_prob": 0.12111933529376984}, {"id": 168, "seek": 76700, "start": 767.0, "end": 770.0, "text": " I've worked two seconds before we started.", "tokens": [50364, 286, 600, 2732, 732, 3949, 949, 321, 1409, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 169, "seek": 76700, "start": 770.0, "end": 773.0, "text": " Yeah.", "tokens": [50514, 865, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 170, "seek": 76700, "start": 773.0, "end": 775.0, "text": " There we go.", "tokens": [50664, 821, 321, 352, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 171, "seek": 76700, "start": 775.0, "end": 777.0, "text": " Reboot.", "tokens": [50764, 1300, 1763, 310, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 172, "seek": 76700, "start": 777.0, "end": 779.0, "text": " Okay.", "tokens": [50864, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 173, "seek": 76700, "start": 779.0, "end": 780.0, "text": " All right.", "tokens": [50964, 1057, 558, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 174, "seek": 76700, "start": 780.0, "end": 781.0, "text": " So the first thing.", "tokens": [51014, 407, 264, 700, 551, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 175, "seek": 76700, "start": 781.0, "end": 782.0, "text": " Hi everyone.", "tokens": [51064, 2421, 1518, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 176, "seek": 76700, "start": 782.0, "end": 785.0, "text": " So the first thing we're going to do today is configure the custom agent.", "tokens": [51114, 407, 264, 700, 551, 321, 434, 516, 281, 360, 965, 307, 22162, 264, 2375, 9461, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 177, "seek": 76700, "start": 785.0, "end": 789.0, "text": " And so you can see exactly what Derek was talking about on those slides.", "tokens": [51264, 400, 370, 291, 393, 536, 2293, 437, 22887, 390, 1417, 466, 322, 729, 9788, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 178, "seek": 76700, "start": 789.0, "end": 790.0, "text": " So diving right in.", "tokens": [51464, 407, 20241, 558, 294, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 179, "seek": 76700, "start": 790.0, "end": 792.0, "text": " I'm just going to go straight into Kiro.", "tokens": [51514, 286, 478, 445, 516, 281, 352, 2997, 666, 591, 5182, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 180, "seek": 76700, "start": 792.0, "end": 795.0, "text": " There we go into the Kiro CLI.", "tokens": [51614, 821, 321, 352, 666, 264, 591, 5182, 12855, 40, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2990572024614383, "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.06865813583135605}, {"id": 181, "seek": 79500, "start": 795.0, "end": 799.0, "text": " And as Derek mentioned, we have an IDE that you can use as well.", "tokens": [50364, 400, 382, 22887, 2835, 11, 321, 362, 364, 40930, 300, 291, 393, 764, 382, 731, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 182, "seek": 79500, "start": 799.0, "end": 803.0, "text": " But for the purposes of this, we're going to use the Kiro CLI.", "tokens": [50564, 583, 337, 264, 9932, 295, 341, 11, 321, 434, 516, 281, 764, 264, 591, 5182, 12855, 40, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 183, "seek": 79500, "start": 803.0, "end": 805.0, "text": " So the first thing we're going to do.", "tokens": [50764, 407, 264, 700, 551, 321, 434, 516, 281, 360, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 184, "seek": 79500, "start": 805.0, "end": 809.0, "text": " I was going to configure our custom agent before we actually go ahead and build a game.", "tokens": [50864, 286, 390, 516, 281, 22162, 527, 2375, 9461, 949, 321, 767, 352, 2286, 293, 1322, 257, 1216, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 185, "seek": 79500, "start": 809.0, "end": 812.0, "text": " So in order to do that, I'm going to go.", "tokens": [51064, 407, 294, 1668, 281, 360, 300, 11, 286, 478, 516, 281, 352, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 186, "seek": 79500, "start": 812.0, "end": 814.0, "text": " Agent.", "tokens": [51214, 27174, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 187, "seek": 79500, "start": 814.0, "end": 816.0, "text": " Let's start the agent start.", "tokens": [51314, 961, 311, 722, 264, 9461, 722, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 188, "seek": 79500, "start": 816.0, "end": 817.0, "text": " We've got.", "tokens": [51414, 492, 600, 658, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 189, "seek": 79500, "start": 817.0, "end": 820.0, "text": " So when you first start off, you have a Kiro default agent.", "tokens": [51464, 407, 562, 291, 700, 722, 766, 11, 291, 362, 257, 591, 5182, 7576, 9461, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12388227819427242, "compression_ratio": 1.6708333333333334, "no_speech_prob": 0.07409181445837021}, {"id": 190, "seek": 82000, "start": 820.0, "end": 822.0, "text": " You can see they're in green.", "tokens": [50364, 509, 393, 536, 436, 434, 294, 3092, 13, 50464], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 191, "seek": 82000, "start": 822.0, "end": 827.0, "text": " And then I've got a flappy Kiro agent here that I've built specifically for this demo.", "tokens": [50464, 400, 550, 286, 600, 658, 257, 932, 1746, 88, 591, 5182, 9461, 510, 300, 286, 600, 3094, 4682, 337, 341, 10723, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 192, "seek": 82000, "start": 827.0, "end": 830.0, "text": " But if you want to build your own, you just go.", "tokens": [50714, 583, 498, 291, 528, 281, 1322, 428, 1065, 11, 291, 445, 352, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 193, "seek": 82000, "start": 830.0, "end": 832.0, "text": " Agent.", "tokens": [50864, 27174, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 194, "seek": 82000, "start": 832.0, "end": 833.0, "text": " Oops.", "tokens": [50964, 21726, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 195, "seek": 82000, "start": 833.0, "end": 835.0, "text": " So it's agent.", "tokens": [51014, 407, 309, 311, 9461, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 196, "seek": 82000, "start": 835.0, "end": 837.0, "text": " And then it's create.", "tokens": [51114, 400, 550, 309, 311, 1884, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 197, "seek": 82000, "start": 837.0, "end": 838.0, "text": " Give it a name.", "tokens": [51214, 5303, 309, 257, 1315, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 198, "seek": 82000, "start": 838.0, "end": 841.0, "text": " And we're going to call this demo.", "tokens": [51264, 400, 321, 434, 516, 281, 818, 341, 10723, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 199, "seek": 82000, "start": 841.0, "end": 842.0, "text": " Agent.", "tokens": [51414, 27174, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 200, "seek": 82000, "start": 842.0, "end": 844.0, "text": " One.", "tokens": [51464, 1485, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 201, "seek": 82000, "start": 844.0, "end": 847.0, "text": " Now this is your agent configuration file.", "tokens": [51564, 823, 341, 307, 428, 9461, 11694, 3991, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 202, "seek": 82000, "start": 847.0, "end": 848.0, "text": " This one's empty.", "tokens": [51714, 639, 472, 311, 6707, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16475695422571948, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.04336443170905113}, {"id": 203, "seek": 84800, "start": 848.0, "end": 851.0, "text": " I'll show you the populated one in just a second.", "tokens": [50364, 286, 603, 855, 291, 264, 32998, 472, 294, 445, 257, 1150, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 204, "seek": 84800, "start": 851.0, "end": 853.0, "text": " But this is where you configure all those things.", "tokens": [50514, 583, 341, 307, 689, 291, 22162, 439, 729, 721, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 205, "seek": 84800, "start": 853.0, "end": 856.0, "text": " Derek told you about your MCP servers.", "tokens": [50614, 22887, 1907, 291, 466, 428, 8797, 47, 15909, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 206, "seek": 84800, "start": 856.0, "end": 859.0, "text": " The tools that your MCP servers have access to.", "tokens": [50764, 440, 3873, 300, 428, 8797, 47, 15909, 362, 2105, 281, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 207, "seek": 84800, "start": 859.0, "end": 861.0, "text": " And we should have called out the acronym on first use.", "tokens": [50914, 400, 321, 820, 362, 1219, 484, 264, 39195, 322, 700, 764, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 208, "seek": 84800, "start": 861.0, "end": 862.0, "text": " I don't think we did.", "tokens": [51014, 286, 500, 380, 519, 321, 630, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 209, "seek": 84800, "start": 862.0, "end": 864.0, "text": " And model context protocol.", "tokens": [51064, 400, 2316, 4319, 10336, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 210, "seek": 84800, "start": 864.0, "end": 867.0, "text": " The tool aliases, as Derek mentioned.", "tokens": [51164, 440, 2290, 10198, 1957, 11, 382, 22887, 2835, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 211, "seek": 84800, "start": 867.0, "end": 870.0, "text": " In case you want to give those different engineering name.", "tokens": [51314, 682, 1389, 291, 528, 281, 976, 729, 819, 7043, 1315, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 212, "seek": 84800, "start": 870.0, "end": 874.0, "text": " The tools that your agents are allowed to use and the different resources.", "tokens": [51464, 440, 3873, 300, 428, 12554, 366, 4350, 281, 764, 293, 264, 819, 3593, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 213, "seek": 84800, "start": 874.0, "end": 877.0, "text": " So what we'll do is we'll quit out of here and show you.", "tokens": [51664, 407, 437, 321, 603, 360, 307, 321, 603, 10366, 484, 295, 510, 293, 855, 291, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1904615674700056, "compression_ratio": 1.7483221476510067, "no_speech_prob": 0.039323531091213226}, {"id": 214, "seek": 87700, "start": 877.0, "end": 880.0, "text": " What that looks like.", "tokens": [50364, 708, 300, 1542, 411, 13, 50514], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 215, "seek": 87700, "start": 880.0, "end": 883.0, "text": " Oops.", "tokens": [50514, 21726, 13, 50664], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 216, "seek": 87700, "start": 883.0, "end": 887.0, "text": " I forgot the prompt sat here.", "tokens": [50664, 286, 5298, 264, 12391, 3227, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 217, "seek": 87700, "start": 887.0, "end": 890.0, "text": " I thought I had it.", "tokens": [50864, 286, 1194, 286, 632, 309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 218, "seek": 87700, "start": 890.0, "end": 892.0, "text": " Derek.", "tokens": [51014, 22887, 13, 51114], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 219, "seek": 87700, "start": 892.0, "end": 895.0, "text": " Oh, one.", "tokens": [51114, 876, 11, 472, 13, 51264], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 220, "seek": 87700, "start": 895.0, "end": 902.0, "text": " There we go.", "tokens": [51264, 821, 321, 352, 13, 51614], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 221, "seek": 87700, "start": 902.0, "end": 903.0, "text": " Sorry about that.", "tokens": [51614, 4919, 466, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 222, "seek": 87700, "start": 903.0, "end": 906.0, "text": " So for this particular agent,", "tokens": [51664, 407, 337, 341, 1729, 9461, 11, 51814], "temperature": 0.0, "avg_logprob": -0.4347081184387207, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.016698624938726425}, {"id": 223, "seek": 90600, "start": 906.0, "end": 909.0, "text": " we've got a couple of MCP servers configured.", "tokens": [50364, 321, 600, 658, 257, 1916, 295, 8797, 47, 15909, 30538, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16716706948202165, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.08981950581073761}, {"id": 224, "seek": 90600, "start": 909.0, "end": 911.0, "text": " So I've got a mermaid MCP server.", "tokens": [50514, 407, 286, 600, 658, 257, 43146, 8797, 47, 7154, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16716706948202165, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.08981950581073761}, {"id": 225, "seek": 90600, "start": 911.0, "end": 914.0, "text": " So that's going to help us build diagrams for the requirements.", "tokens": [50614, 407, 300, 311, 516, 281, 854, 505, 1322, 36709, 337, 264, 7728, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16716706948202165, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.08981950581073761}, {"id": 226, "seek": 90600, "start": 914.0, "end": 916.0, "text": " And the business requirements that we're going to do.", "tokens": [50764, 400, 264, 1606, 7728, 300, 321, 434, 516, 281, 360, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16716706948202165, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.08981950581073761}, {"id": 227, "seek": 90600, "start": 916.0, "end": 918.0, "text": " As well as the implementation plan.", "tokens": [50864, 1018, 731, 382, 264, 11420, 1393, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16716706948202165, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.08981950581073761}, {"id": 228, "seek": 90600, "start": 918.0, "end": 922.0, "text": " I have a fetch MCP server to go and fetch things off the internet for us.", "tokens": [50964, 286, 362, 257, 23673, 8797, 47, 7154, 281, 352, 293, 23673, 721, 766, 264, 4705, 337, 505, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16716706948202165, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.08981950581073761}, {"id": 229, "seek": 90600, "start": 922.0, "end": 925.0, "text": " And that's because when we build the plepic hero game.", "tokens": [51164, 400, 300, 311, 570, 562, 321, 1322, 264, 3362, 37509, 5316, 1216, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16716706948202165, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.08981950581073761}, {"id": 230, "seek": 90600, "start": 925.0, "end": 931.0, "text": " I wanted to be able to go and fetch AWS services from the internet and use that instead of normal.", "tokens": [51314, 286, 1415, 281, 312, 1075, 281, 352, 293, 23673, 17650, 3328, 490, 264, 4705, 293, 764, 300, 2602, 295, 2710, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16716706948202165, "compression_ratio": 1.7868217054263567, "no_speech_prob": 0.08981950581073761}, {"id": 231, "seek": 93100, "start": 931.0, "end": 932.0, "text": " Fluffy bird game.", "tokens": [50364, 3235, 14297, 5255, 1216, 13, 50414], "temperature": 0.0, "avg_logprob": -0.19644223848978679, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.1300952136516571}, {"id": 232, "seek": 93100, "start": 932.0, "end": 934.0, "text": " And make that a little bit more interesting.", "tokens": [50414, 400, 652, 300, 257, 707, 857, 544, 1880, 13, 50514], "temperature": 0.0, "avg_logprob": -0.19644223848978679, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.1300952136516571}, {"id": 233, "seek": 93100, "start": 934.0, "end": 937.0, "text": " Maybe use a Halloween and a Vegas type theme.", "tokens": [50514, 2704, 764, 257, 13860, 293, 257, 15841, 2010, 6314, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19644223848978679, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.1300952136516571}, {"id": 234, "seek": 93100, "start": 937.0, "end": 941.0, "text": " So I've got some tools that I've got loaded here as well.", "tokens": [50664, 407, 286, 600, 658, 512, 3873, 300, 286, 600, 658, 13210, 510, 382, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19644223848978679, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.1300952136516571}, {"id": 235, "seek": 93100, "start": 941.0, "end": 944.0, "text": " Some resources and read and write tools as well.", "tokens": [50864, 2188, 3593, 293, 1401, 293, 2464, 3873, 382, 731, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19644223848978679, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.1300952136516571}, {"id": 236, "seek": 93100, "start": 944.0, "end": 950.0, "text": " And the reason for not putting old tools that the MCP server has access to is as Derek mentioned.", "tokens": [51014, 400, 264, 1778, 337, 406, 3372, 1331, 3873, 300, 264, 8797, 47, 7154, 575, 2105, 281, 307, 382, 22887, 2835, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19644223848978679, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.1300952136516571}, {"id": 237, "seek": 93100, "start": 950.0, "end": 954.0, "text": " You've only got a certain amount of space in your context window.", "tokens": [51314, 509, 600, 787, 658, 257, 1629, 2372, 295, 1901, 294, 428, 4319, 4910, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19644223848978679, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.1300952136516571}, {"id": 238, "seek": 93100, "start": 954.0, "end": 958.0, "text": " So you don't want to overwhelm the agent with tools that it doesn't need.", "tokens": [51514, 407, 291, 500, 380, 528, 281, 9103, 76, 264, 9461, 365, 3873, 300, 309, 1177, 380, 643, 13, 51714], "temperature": 0.0, "avg_logprob": -0.19644223848978679, "compression_ratio": 1.641304347826087, "no_speech_prob": 0.1300952136516571}, {"id": 239, "seek": 95800, "start": 958.0, "end": 962.0, "text": " So I've locked it down just for specific tools for to do that with.", "tokens": [50364, 407, 286, 600, 9376, 309, 760, 445, 337, 2685, 3873, 337, 281, 360, 300, 365, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09830022680348363, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.16397371888160706}, {"id": 240, "seek": 95800, "start": 962.0, "end": 966.0, "text": " And then I've got under the resources the AIDLC workflow.", "tokens": [50564, 400, 550, 286, 600, 658, 833, 264, 3593, 264, 316, 2777, 14766, 20993, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09830022680348363, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.16397371888160706}, {"id": 241, "seek": 95800, "start": 966.0, "end": 971.0, "text": " And as we mentioned, we'll link that in a QR code towards the presentation.", "tokens": [50764, 400, 382, 321, 2835, 11, 321, 603, 2113, 300, 294, 257, 32784, 3089, 3030, 264, 5860, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09830022680348363, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.16397371888160706}, {"id": 242, "seek": 95800, "start": 971.0, "end": 976.0, "text": " But it'll give you a list of all the different components of the AIDLC.", "tokens": [51014, 583, 309, 603, 976, 291, 257, 1329, 295, 439, 264, 819, 6677, 295, 264, 316, 2777, 14766, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09830022680348363, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.16397371888160706}, {"id": 243, "seek": 95800, "start": 976.0, "end": 980.0, "text": " And all the different questions and answers and how it all works throughout there.", "tokens": [51264, 400, 439, 264, 819, 1651, 293, 6338, 293, 577, 309, 439, 1985, 3710, 456, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09830022680348363, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.16397371888160706}, {"id": 244, "seek": 95800, "start": 980.0, "end": 986.0, "text": " So I'll tell the agent to be able to access that as well for the purposes of building this out.", "tokens": [51464, 407, 286, 603, 980, 264, 9461, 281, 312, 1075, 281, 2105, 300, 382, 731, 337, 264, 9932, 295, 2390, 341, 484, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09830022680348363, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.16397371888160706}, {"id": 245, "seek": 98600, "start": 987.0, "end": 992.0, "text": " And then hooks, as we mentioned earlier, that's something for you to for it to the agent to take action on.", "tokens": [50414, 400, 550, 26485, 11, 382, 321, 2835, 3071, 11, 300, 311, 746, 337, 291, 281, 337, 309, 281, 264, 9461, 281, 747, 3069, 322, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13101309650349166, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.04603337123990059}, {"id": 246, "seek": 98600, "start": 992.0, "end": 996.0, "text": " So really good for sort of CICD pipeline type things that you might be doing.", "tokens": [50664, 407, 534, 665, 337, 1333, 295, 383, 2532, 35, 15517, 2010, 721, 300, 291, 1062, 312, 884, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13101309650349166, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.04603337123990059}, {"id": 247, "seek": 98600, "start": 996.0, "end": 998.0, "text": " We're not going to do anything for that in here.", "tokens": [50864, 492, 434, 406, 516, 281, 360, 1340, 337, 300, 294, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13101309650349166, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.04603337123990059}, {"id": 248, "seek": 98600, "start": 998.0, "end": 1005.0, "text": " We're just going to focus on using the MCP servers and the AIDLC and the resources can be good with that.", "tokens": [50964, 492, 434, 445, 516, 281, 1879, 322, 1228, 264, 8797, 47, 15909, 293, 264, 316, 2777, 14766, 293, 264, 3593, 393, 312, 665, 365, 300, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13101309650349166, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.04603337123990059}, {"id": 249, "seek": 98600, "start": 1005.0, "end": 1012.0, "text": " So with that, I'm going to switch to my agent.", "tokens": [51314, 407, 365, 300, 11, 286, 478, 516, 281, 3679, 281, 452, 9461, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13101309650349166, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.04603337123990059}, {"id": 250, "seek": 101200, "start": 1012.0, "end": 1015.0, "text": " And we're going to select that one.", "tokens": [50364, 400, 321, 434, 516, 281, 3048, 300, 472, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1239309235224648, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.017635781317949295}, {"id": 251, "seek": 101200, "start": 1015.0, "end": 1020.0, "text": " And that's going to give us the agent that we're going to use to actually build out the game.", "tokens": [50514, 400, 300, 311, 516, 281, 976, 505, 264, 9461, 300, 321, 434, 516, 281, 764, 281, 767, 1322, 484, 264, 1216, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1239309235224648, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.017635781317949295}, {"id": 252, "seek": 101200, "start": 1020.0, "end": 1022.0, "text": " So yeah.", "tokens": [50764, 407, 1338, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1239309235224648, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.017635781317949295}, {"id": 253, "seek": 101200, "start": 1022.0, "end": 1023.0, "text": " Thanks, Karen.", "tokens": [50864, 2561, 11, 14834, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1239309235224648, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.017635781317949295}, {"id": 254, "seek": 101200, "start": 1023.0, "end": 1028.0, "text": " And you know, just to emphasize, this is a relatively simple thing to configure.", "tokens": [50914, 400, 291, 458, 11, 445, 281, 16078, 11, 341, 307, 257, 7226, 2199, 551, 281, 22162, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1239309235224648, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.017635781317949295}, {"id": 255, "seek": 101200, "start": 1028.0, "end": 1033.0, "text": " But by pulling in those resource files, we're going to dramatically alter the behavior of the agent.", "tokens": [51164, 583, 538, 8407, 294, 729, 7684, 7098, 11, 321, 434, 516, 281, 17548, 11337, 264, 5223, 295, 264, 9461, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1239309235224648, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.017635781317949295}, {"id": 256, "seek": 101200, "start": 1033.0, "end": 1036.0, "text": " As you'll see in a minute here from what we would get by default.", "tokens": [51414, 1018, 291, 603, 536, 294, 257, 3456, 510, 490, 437, 321, 576, 483, 538, 7576, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1239309235224648, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.017635781317949295}, {"id": 257, "seek": 101200, "start": 1036.0, "end": 1039.0, "text": " And so there's a lot of power under the hood that we can have here.", "tokens": [51564, 400, 370, 456, 311, 257, 688, 295, 1347, 833, 264, 13376, 300, 321, 393, 362, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1239309235224648, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.017635781317949295}, {"id": 258, "seek": 103900, "start": 1039.0, "end": 1044.0, "text": " We're developing custom workflows, specific ways that we want the agent to work.", "tokens": [50364, 492, 434, 6416, 2375, 43461, 11, 2685, 2098, 300, 321, 528, 264, 9461, 281, 589, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09618724067256136, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.01168232411146164}, {"id": 259, "seek": 103900, "start": 1044.0, "end": 1048.0, "text": " And that's something that teams can work together on and refine over time.", "tokens": [50614, 400, 300, 311, 746, 300, 5491, 393, 589, 1214, 322, 293, 33906, 670, 565, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09618724067256136, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.01168232411146164}, {"id": 260, "seek": 103900, "start": 1048.0, "end": 1052.0, "text": " And we're seeing libraries of these emerge within customers as the different teams,", "tokens": [50814, 400, 321, 434, 2577, 15148, 295, 613, 21511, 1951, 4581, 382, 264, 819, 5491, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09618724067256136, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.01168232411146164}, {"id": 261, "seek": 103900, "start": 1052.0, "end": 1057.0, "text": " kind of maybe build their own repo internally, just to refine these files,", "tokens": [51014, 733, 295, 1310, 1322, 641, 1065, 49040, 19501, 11, 445, 281, 33906, 613, 7098, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09618724067256136, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.01168232411146164}, {"id": 262, "seek": 103900, "start": 1057.0, "end": 1059.0, "text": " get these better and better over time.", "tokens": [51264, 483, 613, 1101, 293, 1101, 670, 565, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09618724067256136, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.01168232411146164}, {"id": 263, "seek": 103900, "start": 1059.0, "end": 1064.0, "text": " So there's a lot that you can do here by just loading in a few context files and a few tools.", "tokens": [51364, 407, 456, 311, 257, 688, 300, 291, 393, 360, 510, 538, 445, 15114, 294, 257, 1326, 4319, 7098, 293, 257, 1326, 3873, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09618724067256136, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.01168232411146164}, {"id": 264, "seek": 106400, "start": 1064.0, "end": 1074.0, "text": " So we're going to spend the rest of the time here building.", "tokens": [50364, 407, 321, 434, 516, 281, 3496, 264, 1472, 295, 264, 565, 510, 2390, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11987543106079102, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.026662552729249}, {"id": 265, "seek": 106400, "start": 1074.0, "end": 1079.0, "text": " And the goal here is to build a game called Flapicuro by the end of the session.", "tokens": [50864, 400, 264, 3387, 510, 307, 281, 1322, 257, 1216, 1219, 3235, 569, 299, 7052, 538, 264, 917, 295, 264, 5481, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11987543106079102, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.026662552729249}, {"id": 266, "seek": 106400, "start": 1079.0, "end": 1080.0, "text": " We'll see how it goes.", "tokens": [51114, 492, 603, 536, 577, 309, 1709, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11987543106079102, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.026662552729249}, {"id": 267, "seek": 106400, "start": 1080.0, "end": 1081.0, "text": " It's live.", "tokens": [51164, 467, 311, 1621, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11987543106079102, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.026662552729249}, {"id": 268, "seek": 106400, "start": 1081.0, "end": 1084.0, "text": " So we have some backup plans in case things.", "tokens": [51214, 407, 321, 362, 512, 14807, 5482, 294, 1389, 721, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11987543106079102, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.026662552729249}, {"id": 269, "seek": 106400, "start": 1084.0, "end": 1088.0, "text": " We have some kind of problem, but I'll try not to jinx this here.", "tokens": [51364, 492, 362, 512, 733, 295, 1154, 11, 457, 286, 603, 853, 406, 281, 43528, 87, 341, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11987543106079102, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.026662552729249}, {"id": 270, "seek": 108800, "start": 1089.0, "end": 1096.0, "text": " Flapicuro is a game where the go, I think his name is ghosty actually.", "tokens": [50414, 3235, 569, 299, 7052, 307, 257, 1216, 689, 264, 352, 11, 286, 519, 702, 1315, 307, 8359, 88, 767, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17725991780779002, "compression_ratio": 1.6328125, "no_speech_prob": 0.15448632836341858}, {"id": 271, "seek": 108800, "start": 1096.0, "end": 1097.0, "text": " Okay.", "tokens": [50764, 1033, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17725991780779002, "compression_ratio": 1.6328125, "no_speech_prob": 0.15448632836341858}, {"id": 272, "seek": 108800, "start": 1097.0, "end": 1102.0, "text": " So ghosty is going to be able to scroll through a side scrolling game.", "tokens": [50814, 407, 8359, 88, 307, 516, 281, 312, 1075, 281, 11369, 807, 257, 1252, 29053, 1216, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17725991780779002, "compression_ratio": 1.6328125, "no_speech_prob": 0.15448632836341858}, {"id": 273, "seek": 108800, "start": 1102.0, "end": 1104.0, "text": " And when we push spacebar, he'll flap up in the air.", "tokens": [51064, 400, 562, 321, 2944, 1901, 5356, 11, 415, 603, 30781, 493, 294, 264, 1988, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17725991780779002, "compression_ratio": 1.6328125, "no_speech_prob": 0.15448632836341858}, {"id": 274, "seek": 108800, "start": 1104.0, "end": 1109.0, "text": " When we let go the spacebar, he'll drop down and he's got to avoid obstacles and get a score.", "tokens": [51164, 1133, 321, 718, 352, 264, 1901, 5356, 11, 415, 603, 3270, 760, 293, 415, 311, 658, 281, 5042, 17735, 293, 483, 257, 6175, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17725991780779002, "compression_ratio": 1.6328125, "no_speech_prob": 0.15448632836341858}, {"id": 275, "seek": 108800, "start": 1109.0, "end": 1117.0, "text": " The point here is less about the implementation details that we pick or what language or hopefully the game actually works.", "tokens": [51414, 440, 935, 510, 307, 1570, 466, 264, 11420, 4365, 300, 321, 1888, 420, 437, 2856, 420, 4696, 264, 1216, 767, 1985, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17725991780779002, "compression_ratio": 1.6328125, "no_speech_prob": 0.15448632836341858}, {"id": 276, "seek": 111700, "start": 1117.0, "end": 1123.0, "text": " And whether it's good or not, what we really want to do is focus on the workflow.", "tokens": [50364, 400, 1968, 309, 311, 665, 420, 406, 11, 437, 321, 534, 528, 281, 360, 307, 1879, 322, 264, 20993, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0818313274186911, "compression_ratio": 1.75, "no_speech_prob": 0.024516187608242035}, {"id": 277, "seek": 111700, "start": 1123.0, "end": 1137.0, "text": " And focus on how we can use AI from the very beginning to help us get our requirements and our design and our thinking about our plan and our testing and everything that we need to do in order to build production grade software.", "tokens": [50664, 400, 1879, 322, 577, 321, 393, 764, 7318, 490, 264, 588, 2863, 281, 854, 505, 483, 527, 7728, 293, 527, 1715, 293, 527, 1953, 466, 527, 1393, 293, 527, 4997, 293, 1203, 300, 321, 643, 281, 360, 294, 1668, 281, 1322, 4265, 7204, 4722, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0818313274186911, "compression_ratio": 1.75, "no_speech_prob": 0.024516187608242035}, {"id": 278, "seek": 111700, "start": 1137.0, "end": 1144.0, "text": " So while we're not building production grade software today, we're going to sort of speed through some of the steps.", "tokens": [51364, 407, 1339, 321, 434, 406, 2390, 4265, 7204, 4722, 965, 11, 321, 434, 516, 281, 1333, 295, 3073, 807, 512, 295, 264, 4439, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0818313274186911, "compression_ratio": 1.75, "no_speech_prob": 0.024516187608242035}, {"id": 279, "seek": 114400, "start": 1144.0, "end": 1167.0, "text": " That you walk away today, maybe inspire a little bit to try some of these techniques yourselves and try doing an end to end, you know, task or story from your backlog using this process and using AI from the very beginning of your project.", "tokens": [50364, 663, 291, 1792, 1314, 965, 11, 1310, 15638, 257, 707, 857, 281, 853, 512, 295, 613, 7512, 14791, 293, 853, 884, 364, 917, 281, 917, 11, 291, 458, 11, 5633, 420, 1657, 490, 428, 47364, 1228, 341, 1399, 293, 1228, 7318, 490, 264, 588, 2863, 295, 428, 1716, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2133086582399764, "compression_ratio": 1.4753086419753085, "no_speech_prob": 0.05025852471590042}, {"id": 280, "seek": 116700, "start": 1168.0, "end": 1174.0, "text": " Anything else I should do to tee a plappy caro here, I think it's a pretty straightforward game and we'll get into some of the details.", "tokens": [50414, 11998, 1646, 286, 820, 360, 281, 33863, 257, 499, 27425, 1032, 78, 510, 11, 286, 519, 309, 311, 257, 1238, 15325, 1216, 293, 321, 603, 483, 666, 512, 295, 264, 4365, 13, 50714], "temperature": 0.0, "avg_logprob": -0.21022272109985352, "compression_ratio": 1.7102040816326531, "no_speech_prob": 0.4853655695915222}, {"id": 281, "seek": 116700, "start": 1174.0, "end": 1183.0, "text": " Yeah, we're going to try and get it to work on here as well as a mobile phone, so we'll switch to the development tools and see if it works on a mobile, but try and get to work first and then we'll go from there.", "tokens": [50714, 865, 11, 321, 434, 516, 281, 853, 293, 483, 309, 281, 589, 322, 510, 382, 731, 382, 257, 6013, 2593, 11, 370, 321, 603, 3679, 281, 264, 3250, 3873, 293, 536, 498, 309, 1985, 322, 257, 6013, 11, 457, 853, 293, 483, 281, 589, 700, 293, 550, 321, 603, 352, 490, 456, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21022272109985352, "compression_ratio": 1.7102040816326531, "no_speech_prob": 0.4853655695915222}, {"id": 282, "seek": 116700, "start": 1183.0, "end": 1188.0, "text": " All right, let's see how we get on.", "tokens": [51164, 1057, 558, 11, 718, 311, 536, 577, 321, 483, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21022272109985352, "compression_ratio": 1.7102040816326531, "no_speech_prob": 0.4853655695915222}, {"id": 283, "seek": 116700, "start": 1188.0, "end": 1190.0, "text": " Okay, see if this works. Oh great.", "tokens": [51414, 1033, 11, 536, 498, 341, 1985, 13, 876, 869, 13, 51514], "temperature": 0.0, "avg_logprob": -0.21022272109985352, "compression_ratio": 1.7102040816326531, "no_speech_prob": 0.4853655695915222}, {"id": 284, "seek": 119000, "start": 1191.0, "end": 1198.0, "text": " Okay, so to build this, we're going to just use one prompt, so this is the prompt I'm going to use, we're going to walk spend a little bit of time going through it.", "tokens": [50414, 1033, 11, 370, 281, 1322, 341, 11, 321, 434, 516, 281, 445, 764, 472, 12391, 11, 370, 341, 307, 264, 12391, 286, 478, 516, 281, 764, 11, 321, 434, 516, 281, 1792, 3496, 257, 707, 857, 295, 565, 516, 807, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13065290451049805, "compression_ratio": 1.7850877192982457, "no_speech_prob": 0.1667223423719406}, {"id": 285, "seek": 119000, "start": 1198.0, "end": 1210.0, "text": " So the goal is not to do any prompt engineering or have any secrets or spirit baked into this, we're going to have one clear prompt and then we're going to go through the AI DLC process, which has been shortened to fit into the session today.", "tokens": [50764, 407, 264, 3387, 307, 406, 281, 360, 604, 12391, 7043, 420, 362, 604, 14093, 420, 3797, 19453, 666, 341, 11, 321, 434, 516, 281, 362, 472, 1850, 12391, 293, 550, 321, 434, 516, 281, 352, 807, 264, 7318, 30272, 1399, 11, 597, 575, 668, 45183, 281, 3318, 666, 264, 5481, 965, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13065290451049805, "compression_ratio": 1.7850877192982457, "no_speech_prob": 0.1667223423719406}, {"id": 286, "seek": 121000, "start": 1210.0, "end": 1220.0, "text": " So the first thing we're going to do is we're going to use Derek mentioned in the presentation, a lot of the LLMs trained, you know, a latent knowledge that it's got when it was trained.", "tokens": [50364, 407, 264, 700, 551, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 764, 22887, 2835, 294, 264, 5860, 11, 257, 688, 295, 264, 441, 43, 26386, 8895, 11, 291, 458, 11, 257, 48994, 3601, 300, 309, 311, 658, 562, 309, 390, 8895, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14728890787256826, "compression_ratio": 1.9191489361702128, "no_speech_prob": 0.13081097602844238}, {"id": 287, "seek": 121000, "start": 1220.0, "end": 1233.0, "text": " So a lot of this, it's aware of and it can actually make a lot of assumptions based on this, but we still want to guide it with proper requirements, business requirements, you know, functional requirements and functional requirements and then implementation plans.", "tokens": [50864, 407, 257, 688, 295, 341, 11, 309, 311, 3650, 295, 293, 309, 393, 767, 652, 257, 688, 295, 17695, 2361, 322, 341, 11, 457, 321, 920, 528, 281, 5934, 309, 365, 2296, 7728, 11, 1606, 7728, 11, 291, 458, 11, 11745, 7728, 293, 11745, 7728, 293, 550, 11420, 5482, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14728890787256826, "compression_ratio": 1.9191489361702128, "no_speech_prob": 0.13081097602844238}, {"id": 288, "seek": 123300, "start": 1233.0, "end": 1239.0, "text": " So the first thing is, I'm going to say, I want to build a browser based game called Flapicuro for a live demo at AWS reinvent.", "tokens": [50364, 407, 264, 700, 551, 307, 11, 286, 478, 516, 281, 584, 11, 286, 528, 281, 1322, 257, 11185, 2361, 1216, 1219, 3235, 569, 299, 7052, 337, 257, 1621, 10723, 412, 17650, 33477, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19755229243525751, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.03568016737699509}, {"id": 289, "seek": 123300, "start": 1239.0, "end": 1252.0, "text": " We're going to follow the AI DLC methodology and workflow and we're going to ask the LLM to ask us clarifying questions, so that it can then provide the answers as inputs for each phase.", "tokens": [50664, 492, 434, 516, 281, 1524, 264, 7318, 30272, 24850, 293, 20993, 293, 321, 434, 516, 281, 1029, 264, 441, 43, 44, 281, 1029, 505, 6093, 5489, 1651, 11, 370, 300, 309, 393, 550, 2893, 264, 6338, 382, 15743, 337, 1184, 5574, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19755229243525751, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.03568016737699509}, {"id": 290, "seek": 125200, "start": 1252.0, "end": 1263.0, "text": " I want to show you some mermaid diagrams, so when it actually builds out the far structure and the implementation plan, you can see what that looks like and if you look in the IDE, it actually gives you a really good visual of that.", "tokens": [50364, 286, 528, 281, 855, 291, 512, 43146, 36709, 11, 370, 562, 309, 767, 15182, 484, 264, 1400, 3877, 293, 264, 11420, 1393, 11, 291, 393, 536, 437, 300, 1542, 411, 293, 498, 291, 574, 294, 264, 40930, 11, 309, 767, 2709, 291, 257, 534, 665, 5056, 295, 300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10670609526581816, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.2857121229171753}, {"id": 291, "seek": 125200, "start": 1263.0, "end": 1272.0, "text": " Because we're doing this in the CLI, I'm just going to use a mermaid webpage that we've got and we'll show you all the architecture diagrams that it does.", "tokens": [50914, 1436, 321, 434, 884, 341, 294, 264, 12855, 40, 11, 286, 478, 445, 516, 281, 764, 257, 43146, 37852, 300, 321, 600, 658, 293, 321, 603, 855, 291, 439, 264, 9482, 36709, 300, 309, 775, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10670609526581816, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.2857121229171753}, {"id": 292, "seek": 127200, "start": 1272.0, "end": 1283.0, "text": " So we've streamlined the process, obviously to fit into the session today, when we run this with our customers, it's usually a one day workshop sometimes too, but we're going to get this to work in 45 minutes.", "tokens": [50364, 407, 321, 600, 48155, 264, 1399, 11, 2745, 281, 3318, 666, 264, 5481, 965, 11, 562, 321, 1190, 341, 365, 527, 4581, 11, 309, 311, 2673, 257, 472, 786, 13541, 2171, 886, 11, 457, 321, 434, 516, 281, 483, 341, 281, 589, 294, 6905, 2077, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1354098955790202, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.11197155714035034}, {"id": 293, "seek": 127200, "start": 1283.0, "end": 1292.0, "text": " And then we're going to maintain interactive questions and answer throughout the process and we want to save the files in the working directory so it doesn't put it everywhere.", "tokens": [50914, 400, 550, 321, 434, 516, 281, 6909, 15141, 1651, 293, 1867, 3710, 264, 1399, 293, 321, 528, 281, 3155, 264, 7098, 294, 264, 1364, 21120, 370, 309, 1177, 380, 829, 309, 5315, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1354098955790202, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.11197155714035034}, {"id": 294, "seek": 127200, "start": 1292.0, "end": 1300.0, "text": " So I'm being quite specific with it, but we still have to, we're still going to do a quieter guidance and give it the themes and everything like that.", "tokens": [51364, 407, 286, 478, 885, 1596, 2685, 365, 309, 11, 457, 321, 920, 362, 281, 11, 321, 434, 920, 516, 281, 360, 257, 43339, 10056, 293, 976, 309, 264, 13544, 293, 1203, 411, 300, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1354098955790202, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.11197155714035034}, {"id": 295, "seek": 130200, "start": 1303.0, "end": 1307.0, "text": " The question was, are we using questions as you go?", "tokens": [50414, 440, 1168, 390, 11, 366, 321, 1228, 1651, 382, 291, 352, 30, 50614], "temperature": 0.0, "avg_logprob": -0.2888726880473475, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.08730973303318024}, {"id": 296, "seek": 130200, "start": 1307.0, "end": 1309.0, "text": " Take questions that we go.", "tokens": [50614, 3664, 1651, 300, 321, 352, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2888726880473475, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.08730973303318024}, {"id": 297, "seek": 130200, "start": 1309.0, "end": 1315.0, "text": " I think while the agent is thinking, I think we'll have some time.", "tokens": [50714, 286, 519, 1339, 264, 9461, 307, 1953, 11, 286, 519, 321, 603, 362, 512, 565, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2888726880473475, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.08730973303318024}, {"id": 298, "seek": 130200, "start": 1315.0, "end": 1316.0, "text": " Yeah.", "tokens": [51014, 865, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2888726880473475, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.08730973303318024}, {"id": 299, "seek": 130200, "start": 1316.0, "end": 1319.0, "text": " Especially through the bill phase, it takes a good five, seven minutes.", "tokens": [51064, 8545, 807, 264, 2961, 5574, 11, 309, 2516, 257, 665, 1732, 11, 3407, 2077, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2888726880473475, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.08730973303318024}, {"id": 300, "seek": 130200, "start": 1319.0, "end": 1320.0, "text": " Yeah.", "tokens": [51214, 865, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2888726880473475, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.08730973303318024}, {"id": 301, "seek": 130200, "start": 1320.0, "end": 1321.0, "text": " It can be two then.", "tokens": [51264, 467, 393, 312, 732, 550, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2888726880473475, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.08730973303318024}, {"id": 302, "seek": 130200, "start": 1321.0, "end": 1327.0, "text": " Okay. So we're going to go ahead and execute this and English cost.", "tokens": [51314, 1033, 13, 407, 321, 434, 516, 281, 352, 2286, 293, 14483, 341, 293, 3669, 2063, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2888726880473475, "compression_ratio": 1.5240384615384615, "no_speech_prob": 0.08730973303318024}, {"id": 303, "seek": 132700, "start": 1327.0, "end": 1334.0, "text": " So the first stage is going to be the analysis phase or we're going to gather all the requirements.", "tokens": [50364, 407, 264, 700, 3233, 307, 516, 281, 312, 264, 5215, 5574, 420, 321, 434, 516, 281, 5448, 439, 264, 7728, 13, 50714], "temperature": 0.0, "avg_logprob": -0.162424492113518, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.060695912688970566}, {"id": 304, "seek": 132700, "start": 1334.0, "end": 1339.0, "text": " And in the real world, this will be your business requirements that you might have within your organization.", "tokens": [50714, 400, 294, 264, 957, 1002, 11, 341, 486, 312, 428, 1606, 7728, 300, 291, 1062, 362, 1951, 428, 4475, 13, 50964], "temperature": 0.0, "avg_logprob": -0.162424492113518, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.060695912688970566}, {"id": 305, "seek": 132700, "start": 1339.0, "end": 1342.0, "text": " I give this agent permission to do that.", "tokens": [50964, 286, 976, 341, 9461, 11226, 281, 360, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.162424492113518, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.060695912688970566}, {"id": 306, "seek": 132700, "start": 1342.0, "end": 1353.0, "text": " So it'll be things like, you know, what sort of third party systems do you need to use, what sort of logging or anything like that that you might have if you have any security", "tokens": [51114, 407, 309, 603, 312, 721, 411, 11, 291, 458, 11, 437, 1333, 295, 2636, 3595, 3652, 360, 291, 643, 281, 764, 11, 437, 1333, 295, 27991, 420, 1340, 411, 300, 300, 291, 1062, 362, 498, 291, 362, 604, 3825, 51664], "temperature": 0.0, "avg_logprob": -0.162424492113518, "compression_ratio": 1.7708333333333333, "no_speech_prob": 0.060695912688970566}, {"id": 307, "seek": 135300, "start": 1353.0, "end": 1358.0, "text": " postures that you need to follow, things like that, that will be part of your business requirements.", "tokens": [50364, 2183, 1303, 300, 291, 643, 281, 1524, 11, 721, 411, 300, 11, 300, 486, 312, 644, 295, 428, 1606, 7728, 13, 50614], "temperature": 0.0, "avg_logprob": -0.18133905198838976, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.03222128003835678}, {"id": 308, "seek": 135300, "start": 1358.0, "end": 1368.0, "text": " So here we go, it's going to create it a, and I will make this bigger.", "tokens": [50614, 407, 510, 321, 352, 11, 309, 311, 516, 281, 1884, 309, 257, 11, 293, 286, 486, 652, 341, 3801, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18133905198838976, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.03222128003835678}, {"id": 309, "seek": 135300, "start": 1368.0, "end": 1375.0, "text": " If anyone has any suggestions, we can take them as well as be go through this, although it'll just be me filling these out as we go along.", "tokens": [51114, 759, 2878, 575, 604, 13396, 11, 321, 393, 747, 552, 382, 731, 382, 312, 352, 807, 341, 11, 4878, 309, 603, 445, 312, 385, 10623, 613, 484, 382, 321, 352, 2051, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18133905198838976, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.03222128003835678}, {"id": 310, "seek": 135300, "start": 1375.0, "end": 1380.0, "text": " And as Derek mentioned each time we run this, we get a slightly different result, so it's not always the same.", "tokens": [51464, 400, 382, 22887, 2835, 1184, 565, 321, 1190, 341, 11, 321, 483, 257, 4748, 819, 1874, 11, 370, 309, 311, 406, 1009, 264, 912, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18133905198838976, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.03222128003835678}, {"id": 311, "seek": 138000, "start": 1380.0, "end": 1389.0, "text": " The questions do come back a bit different, but when you use the full AI DLC, you get very specific and then you tend to get the correct response.", "tokens": [50364, 440, 1651, 360, 808, 646, 257, 857, 819, 11, 457, 562, 291, 764, 264, 1577, 7318, 30272, 11, 291, 483, 588, 2685, 293, 550, 291, 3928, 281, 483, 264, 3006, 4134, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1665696840028505, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.05790546536445618}, {"id": 312, "seek": 138000, "start": 1389.0, "end": 1391.0, "text": " Just to elaborate if I can, Karen.", "tokens": [50814, 1449, 281, 20945, 498, 286, 393, 11, 14834, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1665696840028505, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.05790546536445618}, {"id": 313, "seek": 138000, "start": 1391.0, "end": 1392.0, "text": " Yeah.", "tokens": [50914, 865, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1665696840028505, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.05790546536445618}, {"id": 314, "seek": 138000, "start": 1392.0, "end": 1399.0, "text": " I think what we find is that the frontier models, the LLMs, they're eager to please, right?", "tokens": [50964, 286, 519, 437, 321, 915, 307, 300, 264, 35853, 5245, 11, 264, 441, 43, 26386, 11, 436, 434, 18259, 281, 1767, 11, 558, 30, 51314], "temperature": 0.0, "avg_logprob": -0.1665696840028505, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.05790546536445618}, {"id": 315, "seek": 138000, "start": 1399.0, "end": 1406.0, "text": " And so if we're vibe coding, often the model will sort of take what you've asked it to do and start running with it and start building stuff,", "tokens": [51314, 400, 370, 498, 321, 434, 14606, 17720, 11, 2049, 264, 2316, 486, 1333, 295, 747, 437, 291, 600, 2351, 309, 281, 360, 293, 722, 2614, 365, 309, 293, 722, 2390, 1507, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1665696840028505, "compression_ratio": 1.5650557620817844, "no_speech_prob": 0.05790546536445618}, {"id": 316, "seek": 140600, "start": 1406.0, "end": 1412.0, "text": " rather than sort of stepping back and thinking critically, like maybe a senior engineer might step back and say, hey, before we build this,", "tokens": [50364, 2831, 813, 1333, 295, 16821, 646, 293, 1953, 22797, 11, 411, 1310, 257, 7965, 11403, 1062, 1823, 646, 293, 584, 11, 4177, 11, 949, 321, 1322, 341, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10328308514186314, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.07644785940647125}, {"id": 317, "seek": 140600, "start": 1412.0, "end": 1416.0, "text": " I've got like 15 questions about, hey, what did you think about security?", "tokens": [50664, 286, 600, 658, 411, 2119, 1651, 466, 11, 4177, 11, 437, 630, 291, 519, 466, 3825, 30, 50864], "temperature": 0.0, "avg_logprob": -0.10328308514186314, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.07644785940647125}, {"id": 318, "seek": 140600, "start": 1416.0, "end": 1418.0, "text": " Did you think about this, you know, what about this edge case, right?", "tokens": [50864, 2589, 291, 519, 466, 341, 11, 291, 458, 11, 437, 466, 341, 4691, 1389, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.10328308514186314, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.07644785940647125}, {"id": 319, "seek": 140600, "start": 1418.0, "end": 1421.0, "text": " And that's kind of the conversation that we want to be having.", "tokens": [50964, 400, 300, 311, 733, 295, 264, 3761, 300, 321, 528, 281, 312, 1419, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10328308514186314, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.07644785940647125}, {"id": 320, "seek": 140600, "start": 1421.0, "end": 1429.0, "text": " And so really, what we've got loaded into the context here is a lot of specific instructions saying, hey, don't skip any of these steps.", "tokens": [51114, 400, 370, 534, 11, 437, 321, 600, 658, 13210, 666, 264, 4319, 510, 307, 257, 688, 295, 2685, 9415, 1566, 11, 4177, 11, 500, 380, 10023, 604, 295, 613, 4439, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10328308514186314, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.07644785940647125}, {"id": 321, "seek": 140600, "start": 1429.0, "end": 1433.0, "text": " If we don't know the answer yet, let's get the answer before we keep moving.", "tokens": [51514, 759, 321, 500, 380, 458, 264, 1867, 1939, 11, 718, 311, 483, 264, 1867, 949, 321, 1066, 2684, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10328308514186314, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.07644785940647125}, {"id": 322, "seek": 143300, "start": 1433.0, "end": 1441.0, "text": " And so the first step here where we've told Kiro, hey, I want to build this thing is, okay, I've got a bunch of questions.", "tokens": [50364, 400, 370, 264, 700, 1823, 510, 689, 321, 600, 1907, 591, 5182, 11, 4177, 11, 286, 528, 281, 1322, 341, 551, 307, 11, 1392, 11, 286, 600, 658, 257, 3840, 295, 1651, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10846808624267579, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.02743523381650448}, {"id": 323, "seek": 143300, "start": 1441.0, "end": 1444.0, "text": " Please answer those for me before we continue.", "tokens": [50764, 2555, 1867, 729, 337, 385, 949, 321, 2354, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10846808624267579, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.02743523381650448}, {"id": 324, "seek": 143300, "start": 1444.0, "end": 1449.0, "text": " So we'll fill those out here, but it should keep asking us until it's clear.", "tokens": [50914, 407, 321, 603, 2836, 729, 484, 510, 11, 457, 309, 820, 1066, 3365, 505, 1826, 309, 311, 1850, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10846808624267579, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.02743523381650448}, {"id": 325, "seek": 143300, "start": 1449.0, "end": 1450.0, "text": " Yeah, exactly.", "tokens": [51164, 865, 11, 2293, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10846808624267579, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.02743523381650448}, {"id": 326, "seek": 143300, "start": 1450.0, "end": 1451.0, "text": " Yeah.", "tokens": [51214, 865, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10846808624267579, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.02743523381650448}, {"id": 327, "seek": 143300, "start": 1451.0, "end": 1454.0, "text": " And these questions are predefined ones.", "tokens": [51264, 400, 613, 1651, 366, 659, 37716, 2306, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10846808624267579, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.02743523381650448}, {"id": 328, "seek": 143300, "start": 1454.0, "end": 1456.0, "text": " This isn't like a final list.", "tokens": [51414, 639, 1943, 380, 411, 257, 2572, 1329, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10846808624267579, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.02743523381650448}, {"id": 329, "seek": 143300, "start": 1456.0, "end": 1460.0, "text": " If you have something when you're doing this for an enterprise app, you can go and actually put those in there.", "tokens": [51514, 759, 291, 362, 746, 562, 291, 434, 884, 341, 337, 364, 14132, 724, 11, 291, 393, 352, 293, 767, 829, 729, 294, 456, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10846808624267579, "compression_ratio": 1.601423487544484, "no_speech_prob": 0.02743523381650448}, {"id": 330, "seek": 146000, "start": 1460.0, "end": 1464.0, "text": " So there's a question you feel that your hasn't, you know, met your organizations needs.", "tokens": [50364, 407, 456, 311, 257, 1168, 291, 841, 300, 428, 6132, 380, 11, 291, 458, 11, 1131, 428, 6150, 2203, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 331, "seek": 146000, "start": 1464.0, "end": 1467.0, "text": " That's something that you can just insert in there too.", "tokens": [50564, 663, 311, 746, 300, 291, 393, 445, 8969, 294, 456, 886, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 332, "seek": 146000, "start": 1467.0, "end": 1469.0, "text": " It doesn't have to be, it won't freak out.", "tokens": [50714, 467, 1177, 380, 362, 281, 312, 11, 309, 1582, 380, 21853, 484, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 333, "seek": 146000, "start": 1469.0, "end": 1471.0, "text": " And I'll show you an example of that.", "tokens": [50814, 400, 286, 603, 855, 291, 364, 1365, 295, 300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 334, "seek": 146000, "start": 1471.0, "end": 1475.0, "text": " So the way I answer these questions, I'm supposed to fill them in down here.", "tokens": [50914, 407, 264, 636, 286, 1867, 613, 1651, 11, 286, 478, 3442, 281, 2836, 552, 294, 760, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 335, "seek": 146000, "start": 1475.0, "end": 1476.0, "text": " I'm just going to say yes, no.", "tokens": [51114, 286, 478, 445, 516, 281, 584, 2086, 11, 572, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 336, "seek": 146000, "start": 1476.0, "end": 1479.0, "text": " And then for this one, I'll fill them all in down here for question two.", "tokens": [51164, 400, 550, 337, 341, 472, 11, 286, 603, 2836, 552, 439, 294, 760, 510, 337, 1168, 732, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 337, "seek": 146000, "start": 1479.0, "end": 1483.0, "text": " And we'll go through it like that and see all the agent deals with that.", "tokens": [51314, 400, 321, 603, 352, 807, 309, 411, 300, 293, 536, 439, 264, 9461, 11215, 365, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 338, "seek": 146000, "start": 1483.0, "end": 1487.0, "text": " So the first question, we wanted to continuously fly.", "tokens": [51514, 407, 264, 700, 1168, 11, 321, 1415, 281, 15684, 3603, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1403394993519623, "compression_ratio": 1.819112627986348, "no_speech_prob": 0.028682250529527664}, {"id": 339, "seek": 148700, "start": 1487.0, "end": 1493.0, "text": " And it says all only when clicking, when pressing the, with space bar.", "tokens": [50364, 400, 309, 1619, 439, 787, 562, 9697, 11, 562, 12417, 264, 11, 365, 1901, 2159, 13, 50664], "temperature": 0.0, "avg_logprob": -0.33378817711347414, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.045789722353219986}, {"id": 340, "seek": 148700, "start": 1493.0, "end": 1497.0, "text": " So I'll say, yeah, press.", "tokens": [50664, 407, 286, 603, 584, 11, 1338, 11, 1886, 13, 50864], "temperature": 0.0, "avg_logprob": -0.33378817711347414, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.045789722353219986}, {"id": 341, "seek": 148700, "start": 1497.0, "end": 1502.0, "text": " I'll just say a button.", "tokens": [50864, 286, 603, 445, 584, 257, 2960, 13, 51114], "temperature": 0.0, "avg_logprob": -0.33378817711347414, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.045789722353219986}, {"id": 342, "seek": 148700, "start": 1502.0, "end": 1504.0, "text": " Press button to fly.", "tokens": [51114, 6776, 2960, 281, 3603, 13, 51214], "temperature": 0.0, "avg_logprob": -0.33378817711347414, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.045789722353219986}, {"id": 343, "seek": 148700, "start": 1504.0, "end": 1508.0, "text": " And then for the easy difficult, easy medium or hard.", "tokens": [51214, 400, 550, 337, 264, 1858, 2252, 11, 1858, 6399, 420, 1152, 13, 51414], "temperature": 0.0, "avg_logprob": -0.33378817711347414, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.045789722353219986}, {"id": 344, "seek": 148700, "start": 1508.0, "end": 1516.0, "text": " I'm going to say easy has large larger, yeah, it's a large larger.", "tokens": [51414, 286, 478, 516, 281, 584, 1858, 575, 2416, 4833, 11, 1338, 11, 309, 311, 257, 2416, 4833, 13, 51814], "temperature": 0.0, "avg_logprob": -0.33378817711347414, "compression_ratio": 1.5595238095238095, "no_speech_prob": 0.045789722353219986}, {"id": 345, "seek": 151600, "start": 1517.0, "end": 1529.0, "text": " Optical gaps, getting harder as the game progresses.", "tokens": [50414, 21455, 804, 15031, 11, 1242, 6081, 382, 264, 1216, 41929, 13, 51014], "temperature": 0.0, "avg_logprob": -0.20173590833490546, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.003862626850605011}, {"id": 346, "seek": 151600, "start": 1529.0, "end": 1530.0, "text": " There we go.", "tokens": [51014, 821, 321, 352, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20173590833490546, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.003862626850605011}, {"id": 347, "seek": 151600, "start": 1530.0, "end": 1532.0, "text": " And should the game speed increase over time?", "tokens": [51064, 400, 820, 264, 1216, 3073, 3488, 670, 565, 30, 51164], "temperature": 0.0, "avg_logprob": -0.20173590833490546, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.003862626850605011}, {"id": 348, "seek": 151600, "start": 1532.0, "end": 1535.0, "text": " I'll just get that a flat yes.", "tokens": [51164, 286, 603, 445, 483, 300, 257, 4962, 2086, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20173590833490546, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.003862626850605011}, {"id": 349, "seek": 151600, "start": 1535.0, "end": 1538.0, "text": " This takes quite a bit of time when you're building out an enterprise app,", "tokens": [51314, 639, 2516, 1596, 257, 857, 295, 565, 562, 291, 434, 2390, 484, 364, 14132, 724, 11, 51464], "temperature": 0.0, "avg_logprob": -0.20173590833490546, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.003862626850605011}, {"id": 350, "seek": 151600, "start": 1538.0, "end": 1540.0, "text": " but I hope it will get through this quite quickly.", "tokens": [51464, 457, 286, 1454, 309, 486, 483, 807, 341, 1596, 2661, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20173590833490546, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.003862626850605011}, {"id": 351, "seek": 151600, "start": 1540.0, "end": 1543.0, "text": " So then we get to the visual size of this too.", "tokens": [51564, 407, 550, 321, 483, 281, 264, 5056, 2744, 295, 341, 886, 13, 51714], "temperature": 0.0, "avg_logprob": -0.20173590833490546, "compression_ratio": 1.4928909952606635, "no_speech_prob": 0.003862626850605011}, {"id": 352, "seek": 154300, "start": 1543.0, "end": 1546.0, "text": " So what I want to do is I'll change this a little bit.", "tokens": [50364, 407, 437, 286, 528, 281, 360, 307, 286, 603, 1319, 341, 257, 707, 857, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2396585215692935, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.019803589209914207}, {"id": 353, "seek": 154300, "start": 1546.0, "end": 1550.0, "text": " We're going to use maybe some of the AWS colours.", "tokens": [50514, 492, 434, 516, 281, 764, 1310, 512, 295, 264, 17650, 16484, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2396585215692935, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.019803589209914207}, {"id": 354, "seek": 154300, "start": 1550.0, "end": 1553.0, "text": " I'll go with orange and black.", "tokens": [50714, 286, 603, 352, 365, 7671, 293, 2211, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2396585215692935, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.019803589209914207}, {"id": 355, "seek": 154300, "start": 1553.0, "end": 1554.0, "text": " I'll go with a Halloween theme.", "tokens": [50864, 286, 603, 352, 365, 257, 13860, 6314, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2396585215692935, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.019803589209914207}, {"id": 356, "seek": 154300, "start": 1554.0, "end": 1557.0, "text": " I think it's recently Halloween.", "tokens": [50914, 286, 519, 309, 311, 3938, 13860, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2396585215692935, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.019803589209914207}, {"id": 357, "seek": 154300, "start": 1557.0, "end": 1561.0, "text": " And say Las Vegas theme as well.", "tokens": [51064, 400, 584, 10663, 15841, 6314, 382, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2396585215692935, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.019803589209914207}, {"id": 358, "seek": 154300, "start": 1561.0, "end": 1567.0, "text": " Let's see how it deals with that.", "tokens": [51264, 961, 311, 536, 577, 309, 11215, 365, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2396585215692935, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.019803589209914207}, {"id": 359, "seek": 154300, "start": 1567.0, "end": 1569.0, "text": " One at a time was there.", "tokens": [51564, 1485, 412, 257, 565, 390, 456, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2396585215692935, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.019803589209914207}, {"id": 360, "seek": 156900, "start": 1569.0, "end": 1573.0, "text": " Do you want the Kira character to be a simple shapes, bright, or custom design?", "tokens": [50364, 1144, 291, 528, 264, 591, 4271, 2517, 281, 312, 257, 2199, 10854, 11, 4730, 11, 420, 2375, 1715, 30, 50564], "temperature": 0.0, "avg_logprob": -0.29860508826471144, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.06502573192119598}, {"id": 361, "seek": 156900, "start": 1573.0, "end": 1574.0, "text": " So Kira is a ghost.", "tokens": [50564, 407, 591, 4271, 307, 257, 8359, 13, 50614], "temperature": 0.0, "avg_logprob": -0.29860508826471144, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.06502573192119598}, {"id": 362, "seek": 156900, "start": 1574.0, "end": 1582.0, "text": " So we're going to use a purple, purple, ghost character.", "tokens": [50614, 407, 321, 434, 516, 281, 764, 257, 9656, 11, 9656, 11, 8359, 2517, 13, 51014], "temperature": 0.0, "avg_logprob": -0.29860508826471144, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.06502573192119598}, {"id": 363, "seek": 156900, "start": 1582.0, "end": 1585.0, "text": " Should the obstacles be classy?", "tokens": [51014, 6454, 264, 17735, 312, 43989, 30, 51164], "temperature": 0.0, "avg_logprob": -0.29860508826471144, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.06502573192119598}, {"id": 364, "seek": 156900, "start": 1585.0, "end": 1588.0, "text": " You know, the classic, obviously it's got that late in knowledge.", "tokens": [51164, 509, 458, 11, 264, 7230, 11, 2745, 309, 311, 658, 300, 3469, 294, 3601, 13, 51314], "temperature": 0.0, "avg_logprob": -0.29860508826471144, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.06502573192119598}, {"id": 365, "seek": 156900, "start": 1588.0, "end": 1590.0, "text": " It knows exactly what that game is.", "tokens": [51314, 467, 3255, 2293, 437, 300, 1216, 307, 13, 51414], "temperature": 0.0, "avg_logprob": -0.29860508826471144, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.06502573192119598}, {"id": 366, "seek": 156900, "start": 1590.0, "end": 1594.0, "text": " So let's go with AWS themed.", "tokens": [51414, 407, 718, 311, 352, 365, 17650, 33920, 13, 51614], "temperature": 0.0, "avg_logprob": -0.29860508826471144, "compression_ratio": 1.5336538461538463, "no_speech_prob": 0.06502573192119598}, {"id": 367, "seek": 159400, "start": 1595.0, "end": 1602.0, "text": " And it's got that from the prompt that we did with saying that we're doing an AWS reinvent presentation.", "tokens": [50414, 400, 309, 311, 658, 300, 490, 264, 12391, 300, 321, 630, 365, 1566, 300, 321, 434, 884, 364, 17650, 33477, 5860, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18638063318589154, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.12440672516822815}, {"id": 368, "seek": 159400, "start": 1602.0, "end": 1612.0, "text": " And then we want to say use say AI and coding services from AWS.", "tokens": [50764, 400, 550, 321, 528, 281, 584, 764, 584, 7318, 293, 17720, 3328, 490, 17650, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18638063318589154, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.12440672516822815}, {"id": 369, "seek": 159400, "start": 1612.0, "end": 1613.0, "text": " There we go.", "tokens": [51264, 821, 321, 352, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18638063318589154, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.12440672516822815}, {"id": 370, "seek": 159400, "start": 1613.0, "end": 1615.0, "text": " And then scoring in leaderboard.", "tokens": [51314, 400, 550, 22358, 294, 5263, 3787, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18638063318589154, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.12440672516822815}, {"id": 371, "seek": 159400, "start": 1615.0, "end": 1618.0, "text": " We're kind of scoring features which you like.", "tokens": [51414, 492, 434, 733, 295, 22358, 4122, 597, 291, 411, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18638063318589154, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.12440672516822815}, {"id": 372, "seek": 159400, "start": 1618.0, "end": 1621.0, "text": " I'll take suggestions if you want to shout them out too.", "tokens": [51564, 286, 603, 747, 13396, 498, 291, 528, 281, 8043, 552, 484, 886, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18638063318589154, "compression_ratio": 1.5870646766169154, "no_speech_prob": 0.12440672516822815}, {"id": 373, "seek": 162100, "start": 1621.0, "end": 1624.0, "text": " I'll go with all these.", "tokens": [50364, 286, 603, 352, 365, 439, 613, 13, 50514], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 374, "seek": 162100, "start": 1624.0, "end": 1625.0, "text": " So simple score.", "tokens": [50514, 407, 2199, 6175, 13, 50564], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 375, "seek": 162100, "start": 1625.0, "end": 1627.0, "text": " Or do you want to use a leaderboard.", "tokens": [50564, 1610, 360, 291, 528, 281, 764, 257, 5263, 3787, 13, 50664], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 376, "seek": 162100, "start": 1627.0, "end": 1629.0, "text": " So we'll just go with simple for now.", "tokens": [50664, 407, 321, 603, 445, 352, 365, 2199, 337, 586, 13, 50764], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 377, "seek": 162100, "start": 1629.0, "end": 1635.0, "text": " And then here we're just going to use local storage because we don't have anything like that configured.", "tokens": [50764, 400, 550, 510, 321, 434, 445, 516, 281, 764, 2654, 6725, 570, 321, 500, 380, 362, 1340, 411, 300, 30538, 13, 51064], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 378, "seek": 162100, "start": 1635.0, "end": 1640.0, "text": " And in the real world, obviously it's looking for, you know, if you're using an API gateway,", "tokens": [51064, 400, 294, 264, 957, 1002, 11, 2745, 309, 311, 1237, 337, 11, 291, 458, 11, 498, 291, 434, 1228, 364, 9362, 28532, 11, 51314], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 379, "seek": 162100, "start": 1640.0, "end": 1641.0, "text": " you need something like cloud front.", "tokens": [51314, 291, 643, 746, 411, 4588, 1868, 13, 51364], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 380, "seek": 162100, "start": 1641.0, "end": 1644.0, "text": " If you want this game to be globally available,", "tokens": [51364, 759, 291, 528, 341, 1216, 281, 312, 18958, 2435, 11, 51514], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 381, "seek": 162100, "start": 1644.0, "end": 1646.0, "text": " close to your customers in different regions.", "tokens": [51514, 1998, 281, 428, 4581, 294, 819, 10682, 13, 51614], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 382, "seek": 162100, "start": 1646.0, "end": 1648.0, "text": " That's something you'd specify over here.", "tokens": [51614, 663, 311, 746, 291, 1116, 16500, 670, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.25968845745989383, "compression_ratio": 1.6643835616438356, "no_speech_prob": 0.13911744952201843}, {"id": 383, "seek": 164800, "start": 1648.0, "end": 1650.0, "text": " For the purposes of this, we're going to do this.", "tokens": [50364, 1171, 264, 9932, 295, 341, 11, 321, 434, 516, 281, 360, 341, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15810940140172056, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.04336947947740555}, {"id": 384, "seek": 164800, "start": 1650.0, "end": 1652.0, "text": " We'll say yes to that.", "tokens": [50464, 492, 603, 584, 2086, 281, 300, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15810940140172056, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.04336947947740555}, {"id": 385, "seek": 164800, "start": 1652.0, "end": 1654.0, "text": " And then how long do you want it to be?", "tokens": [50564, 400, 550, 577, 938, 360, 291, 528, 309, 281, 312, 30, 50664], "temperature": 0.0, "avg_logprob": -0.15810940140172056, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.04336947947740555}, {"id": 386, "seek": 164800, "start": 1654.0, "end": 1658.0, "text": " I'll just go one to two minutes.", "tokens": [50664, 286, 603, 445, 352, 472, 281, 732, 2077, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15810940140172056, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.04336947947740555}, {"id": 387, "seek": 164800, "start": 1658.0, "end": 1662.0, "text": " And I'll say yes to that.", "tokens": [50864, 400, 286, 603, 584, 2086, 281, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15810940140172056, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.04336947947740555}, {"id": 388, "seek": 164800, "start": 1662.0, "end": 1667.0, "text": " And on a mobile phone.", "tokens": [51064, 400, 322, 257, 6013, 2593, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15810940140172056, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.04336947947740555}, {"id": 389, "seek": 164800, "start": 1667.0, "end": 1669.0, "text": " Test that towards the end.", "tokens": [51314, 9279, 300, 3030, 264, 917, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15810940140172056, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.04336947947740555}, {"id": 390, "seek": 166900, "start": 1669.0, "end": 1679.0, "text": " I like the ambition level here.", "tokens": [50364, 286, 411, 264, 22814, 1496, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 391, "seek": 166900, "start": 1679.0, "end": 1680.0, "text": " Yeah.", "tokens": [50864, 865, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 392, "seek": 166900, "start": 1680.0, "end": 1682.0, "text": " You're having a lot of features.", "tokens": [50914, 509, 434, 1419, 257, 688, 295, 4122, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 393, "seek": 166900, "start": 1682.0, "end": 1684.0, "text": " HTML and Java.", "tokens": [51014, 17995, 293, 10745, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 394, "seek": 166900, "start": 1684.0, "end": 1687.0, "text": " Let's use those two for this purpose of this.", "tokens": [51114, 961, 311, 764, 729, 732, 337, 341, 4334, 295, 341, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 395, "seek": 166900, "start": 1687.0, "end": 1688.0, "text": " Single file.", "tokens": [51264, 31248, 3991, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 396, "seek": 166900, "start": 1688.0, "end": 1692.0, "text": " I think we'll use multiple files because I want to have proper structure for this.", "tokens": [51314, 286, 519, 321, 603, 764, 3866, 7098, 570, 286, 528, 281, 362, 2296, 3877, 337, 341, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 397, "seek": 166900, "start": 1692.0, "end": 1694.0, "text": " So in case we need to go back and change something,", "tokens": [51514, 407, 294, 1389, 321, 643, 281, 352, 646, 293, 1319, 746, 11, 51614], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 398, "seek": 166900, "start": 1694.0, "end": 1697.0, "text": " or we want to document any of these decisions and come back and ask why we did that.", "tokens": [51614, 420, 321, 528, 281, 4166, 604, 295, 613, 5327, 293, 808, 646, 293, 1029, 983, 321, 630, 300, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15643360545334306, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.030804140493273735}, {"id": 399, "seek": 169700, "start": 1697.0, "end": 1698.0, "text": " It's all nicely documented.", "tokens": [50364, 467, 311, 439, 9594, 23007, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 400, "seek": 169700, "start": 1698.0, "end": 1703.0, "text": " So go with multiple files for that.", "tokens": [50414, 407, 352, 365, 3866, 7098, 337, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 401, "seek": 169700, "start": 1703.0, "end": 1705.0, "text": " Yeah.", "tokens": [50664, 865, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 402, "seek": 169700, "start": 1705.0, "end": 1706.0, "text": " Good catch.", "tokens": [50764, 2205, 3745, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 403, "seek": 169700, "start": 1706.0, "end": 1708.0, "text": " It's pretty good at picking up all my bad typos.", "tokens": [50814, 467, 311, 1238, 665, 412, 8867, 493, 439, 452, 1578, 2125, 329, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 404, "seek": 169700, "start": 1708.0, "end": 1713.0, "text": " So I'll see how it deals with that one.", "tokens": [50914, 407, 286, 603, 536, 577, 309, 11215, 365, 300, 472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 405, "seek": 169700, "start": 1713.0, "end": 1715.0, "text": " I'll leave it alone.", "tokens": [51164, 286, 603, 1856, 309, 3312, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 406, "seek": 169700, "start": 1715.0, "end": 1717.0, "text": " Any AWS services want to showcase?", "tokens": [51264, 2639, 17650, 3328, 528, 281, 20388, 30, 51364], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 407, "seek": 169700, "start": 1717.0, "end": 1719.0, "text": " Anything to do there?", "tokens": [51364, 11998, 281, 360, 456, 30, 51464], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 408, "seek": 169700, "start": 1719.0, "end": 1721.0, "text": " I don't think it knows about curiosity.", "tokens": [51464, 286, 500, 380, 519, 309, 3255, 466, 18769, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 409, "seek": 169700, "start": 1721.0, "end": 1724.0, "text": " It's got that MCP server to go and fetch things like that.", "tokens": [51564, 467, 311, 658, 300, 376, 20049, 7154, 281, 352, 293, 23673, 721, 411, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2821595253200706, "compression_ratio": 1.51528384279476, "no_speech_prob": 0.10472586750984192}, {"id": 410, "seek": 172400, "start": 1724.0, "end": 1727.0, "text": " I'm a proud workmail user, but it's probably not.", "tokens": [50364, 286, 478, 257, 4570, 589, 11799, 4195, 11, 457, 309, 311, 1391, 406, 13, 50514], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 411, "seek": 172400, "start": 1727.0, "end": 1728.0, "text": " Workmail.", "tokens": [50514, 6603, 11799, 13, 50564], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 412, "seek": 172400, "start": 1728.0, "end": 1730.0, "text": " So I'm not appropriate.", "tokens": [50564, 407, 286, 478, 406, 6854, 13, 50664], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 413, "seek": 172400, "start": 1730.0, "end": 1731.0, "text": " Yeah.", "tokens": [50664, 865, 13, 50714], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 414, "seek": 172400, "start": 1731.0, "end": 1732.0, "text": " Sorry.", "tokens": [50714, 4919, 13, 50764], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 415, "seek": 172400, "start": 1732.0, "end": 1733.0, "text": " Amplify.", "tokens": [50764, 2012, 564, 2505, 13, 50814], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 416, "seek": 172400, "start": 1733.0, "end": 1734.0, "text": " Do that.", "tokens": [50814, 1144, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 417, "seek": 172400, "start": 1734.0, "end": 1736.0, "text": " Let's see what it does there.", "tokens": [50864, 961, 311, 536, 437, 309, 775, 456, 13, 50964], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 418, "seek": 172400, "start": 1736.0, "end": 1742.0, "text": " And I'll put maybe bedrock since it's going to be running on that.", "tokens": [50964, 400, 286, 603, 829, 1310, 2901, 17799, 1670, 309, 311, 516, 281, 312, 2614, 322, 300, 13, 51264], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 419, "seek": 172400, "start": 1742.0, "end": 1743.0, "text": " Okay.", "tokens": [51264, 1033, 13, 51314], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 420, "seek": 172400, "start": 1743.0, "end": 1744.0, "text": " Let's try those.", "tokens": [51314, 961, 311, 853, 729, 13, 51364], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 421, "seek": 172400, "start": 1744.0, "end": 1747.0, "text": " And then start string instructions.", "tokens": [51364, 400, 550, 722, 6798, 9415, 13, 51514], "temperature": 0.0, "avg_logprob": -0.317844955288634, "compression_ratio": 1.4438502673796791, "no_speech_prob": 0.24830180406570435}, {"id": 422, "seek": 174700, "start": 1747.0, "end": 1757.0, "text": " Say yes.", "tokens": [50364, 6463, 2086, 13, 50864], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 423, "seek": 174700, "start": 1757.0, "end": 1759.0, "text": " We're just suggesting for Amplify and bedrock.", "tokens": [50864, 492, 434, 445, 18094, 337, 2012, 564, 2505, 293, 2901, 17799, 13, 50964], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 424, "seek": 174700, "start": 1759.0, "end": 1763.0, "text": " Maybe just make a note that we need a way to test locally.", "tokens": [50964, 2704, 445, 652, 257, 3637, 300, 321, 643, 257, 636, 281, 1500, 16143, 13, 51164], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 425, "seek": 174700, "start": 1763.0, "end": 1764.0, "text": " Okay.", "tokens": [51164, 1033, 13, 51214], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 426, "seek": 174700, "start": 1764.0, "end": 1767.0, "text": " Since we're not going to have to find time to deploy to an AWS environment.", "tokens": [51214, 4162, 321, 434, 406, 516, 281, 362, 281, 915, 565, 281, 7274, 281, 364, 17650, 2823, 13, 51364], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 427, "seek": 174700, "start": 1767.0, "end": 1768.0, "text": " Yeah.", "tokens": [51364, 865, 13, 51414], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 428, "seek": 174700, "start": 1768.0, "end": 1769.0, "text": " Cool.", "tokens": [51414, 8561, 13, 51464], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 429, "seek": 174700, "start": 1769.0, "end": 1770.0, "text": " All right.", "tokens": [51464, 1057, 558, 13, 51514], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 430, "seek": 174700, "start": 1770.0, "end": 1771.0, "text": " Restock button.", "tokens": [51514, 13094, 1560, 2960, 13, 51564], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 431, "seek": 174700, "start": 1771.0, "end": 1772.0, "text": " I'll just say yes.", "tokens": [51564, 286, 603, 445, 584, 2086, 13, 51614], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 432, "seek": 174700, "start": 1772.0, "end": 1774.0, "text": " And then we'll move on to the next one.", "tokens": [51614, 400, 550, 321, 603, 1286, 322, 281, 264, 958, 472, 13, 51714], "temperature": 0.0, "avg_logprob": -0.31392568588256836, "compression_ratio": 1.4341463414634146, "no_speech_prob": 0.2909284234046936}, {"id": 433, "seek": 177400, "start": 1775.0, "end": 1777.0, "text": " Mobile support.", "tokens": [50414, 22625, 1406, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 434, "seek": 177400, "start": 1777.0, "end": 1781.0, "text": " Support for mobile browser.", "tokens": [50514, 18073, 337, 6013, 11185, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 435, "seek": 177400, "start": 1781.0, "end": 1782.0, "text": " So I think.", "tokens": [50714, 407, 286, 519, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 436, "seek": 177400, "start": 1782.0, "end": 1786.0, "text": " And then maybe want to use maybe Chrome and Firefox.", "tokens": [50764, 400, 550, 1310, 528, 281, 764, 1310, 15327, 293, 46613, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 437, "seek": 177400, "start": 1786.0, "end": 1788.0, "text": " So I think there are two common use ones.", "tokens": [50964, 407, 286, 519, 456, 366, 732, 2689, 764, 2306, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 438, "seek": 177400, "start": 1788.0, "end": 1790.0, "text": " Chrome and Firefox.", "tokens": [51064, 15327, 293, 46613, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 439, "seek": 177400, "start": 1790.0, "end": 1791.0, "text": " Okay.", "tokens": [51164, 1033, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 440, "seek": 177400, "start": 1791.0, "end": 1792.0, "text": " Great.", "tokens": [51214, 3769, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 441, "seek": 177400, "start": 1792.0, "end": 1793.0, "text": " And that's it.", "tokens": [51264, 400, 300, 311, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 442, "seek": 177400, "start": 1793.0, "end": 1796.0, "text": " So we'll go ahead and save that.", "tokens": [51314, 407, 321, 603, 352, 2286, 293, 3155, 300, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 443, "seek": 177400, "start": 1796.0, "end": 1797.0, "text": " Close that up.", "tokens": [51464, 16346, 300, 493, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 444, "seek": 177400, "start": 1797.0, "end": 1799.0, "text": " And then we can say.", "tokens": [51514, 400, 550, 321, 393, 584, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 445, "seek": 177400, "start": 1799.0, "end": 1800.0, "text": " Done.", "tokens": [51614, 18658, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2830139963250411, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.015399564057588577}, {"id": 446, "seek": 180000, "start": 1800.0, "end": 1802.0, "text": " And again, we're still using that single prompt.", "tokens": [50364, 400, 797, 11, 321, 434, 920, 1228, 300, 2167, 12391, 13, 50464], "temperature": 0.0, "avg_logprob": -0.18399729627244016, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.02282024919986725}, {"id": 447, "seek": 180000, "start": 1802.0, "end": 1813.0, "text": " And it should now go ahead read those answers and come back and start building out the design phase.", "tokens": [50464, 400, 309, 820, 586, 352, 2286, 1401, 729, 6338, 293, 808, 646, 293, 722, 2390, 484, 264, 1715, 5574, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18399729627244016, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.02282024919986725}, {"id": 448, "seek": 180000, "start": 1813.0, "end": 1819.0, "text": " And the whole process of this is we're bringing the agent and the LLM into the decision making process.", "tokens": [51014, 400, 264, 1379, 1399, 295, 341, 307, 321, 434, 5062, 264, 9461, 293, 264, 441, 43, 44, 666, 264, 3537, 1455, 1399, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18399729627244016, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.02282024919986725}, {"id": 449, "seek": 180000, "start": 1819.0, "end": 1820.0, "text": " So yeah.", "tokens": [51314, 407, 1338, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18399729627244016, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.02282024919986725}, {"id": 450, "seek": 180000, "start": 1820.0, "end": 1824.0, "text": " So about that, saving a lot of time.", "tokens": [51364, 407, 466, 300, 11, 6816, 257, 688, 295, 565, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18399729627244016, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.02282024919986725}, {"id": 451, "seek": 180000, "start": 1824.0, "end": 1828.0, "text": " And just to note that we were just doing a lot of.", "tokens": [51564, 400, 445, 281, 3637, 300, 321, 645, 445, 884, 257, 688, 295, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18399729627244016, "compression_ratio": 1.6587677725118484, "no_speech_prob": 0.02282024919986725}, {"id": 452, "seek": 182800, "start": 1828.0, "end": 1832.0, "text": " What I would call classic product manager work unless software development.", "tokens": [50364, 708, 286, 576, 818, 7230, 1674, 6598, 589, 5969, 4722, 3250, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18081331253051758, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.02707730606198311}, {"id": 453, "seek": 182800, "start": 1832.0, "end": 1834.0, "text": " But it's a very important obviously.", "tokens": [50564, 583, 309, 311, 257, 588, 1021, 2745, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18081331253051758, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.02707730606198311}, {"id": 454, "seek": 182800, "start": 1834.0, "end": 1837.0, "text": " And one thing that we're finding is that.", "tokens": [50664, 400, 472, 551, 300, 321, 434, 5006, 307, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18081331253051758, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.02707730606198311}, {"id": 455, "seek": 182800, "start": 1837.0, "end": 1843.0, "text": " Getting together cross functional teams that maybe not might not work together next to each other day by day.", "tokens": [50814, 13674, 1214, 3278, 11745, 5491, 300, 1310, 406, 1062, 406, 589, 1214, 958, 281, 1184, 661, 786, 538, 786, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18081331253051758, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.02707730606198311}, {"id": 456, "seek": 182800, "start": 1843.0, "end": 1848.0, "text": " But getting the product managers and the stakeholders along with the software developers.", "tokens": [51114, 583, 1242, 264, 1674, 14084, 293, 264, 17779, 2051, 365, 264, 4722, 8849, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18081331253051758, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.02707730606198311}, {"id": 457, "seek": 182800, "start": 1848.0, "end": 1850.0, "text": " Maybe QA ops.", "tokens": [51364, 2704, 1249, 32, 44663, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18081331253051758, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.02707730606198311}, {"id": 458, "seek": 182800, "start": 1850.0, "end": 1851.0, "text": " Right.", "tokens": [51464, 1779, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18081331253051758, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.02707730606198311}, {"id": 459, "seek": 182800, "start": 1851.0, "end": 1854.0, "text": " Getting that cross functional team that's going to be responsible for this thing.", "tokens": [51514, 13674, 300, 3278, 11745, 1469, 300, 311, 516, 281, 312, 6250, 337, 341, 551, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18081331253051758, "compression_ratio": 1.7644787644787645, "no_speech_prob": 0.02707730606198311}, {"id": 460, "seek": 185400, "start": 1854.0, "end": 1857.0, "text": " Sometimes physically together, or at least on the same call.", "tokens": [50364, 4803, 9762, 1214, 11, 420, 412, 1935, 322, 264, 912, 818, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1111365727015904, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.011251216754317284}, {"id": 461, "seek": 185400, "start": 1857.0, "end": 1863.0, "text": " And working together on answering these questions can help teams get this right up front.", "tokens": [50514, 400, 1364, 1214, 322, 13430, 613, 1651, 393, 854, 5491, 483, 341, 558, 493, 1868, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1111365727015904, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.011251216754317284}, {"id": 462, "seek": 185400, "start": 1863.0, "end": 1870.0, "text": " And skip a lot of back and forth that we might traditionally do where we need to email or cut a ticket to someone to find out.", "tokens": [50814, 400, 10023, 257, 688, 295, 646, 293, 5220, 300, 321, 1062, 19067, 360, 689, 321, 643, 281, 3796, 420, 1723, 257, 10550, 281, 1580, 281, 915, 484, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1111365727015904, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.011251216754317284}, {"id": 463, "seek": 185400, "start": 1870.0, "end": 1878.0, "text": " And so if we're able to do this collaboratively up front, we can dramatically cut down on the time needed for us to get all of this right.", "tokens": [51164, 400, 370, 498, 321, 434, 1075, 281, 360, 341, 16555, 356, 493, 1868, 11, 321, 393, 17548, 1723, 760, 322, 264, 565, 2978, 337, 505, 281, 483, 439, 295, 341, 558, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1111365727015904, "compression_ratio": 1.71900826446281, "no_speech_prob": 0.011251216754317284}, {"id": 464, "seek": 187800, "start": 1878.0, "end": 1885.0, "text": " And the agent in this case is really our helper to make sure that we're documenting it properly.", "tokens": [50364, 400, 264, 9461, 294, 341, 1389, 307, 534, 527, 36133, 281, 652, 988, 300, 321, 434, 42360, 309, 6108, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17335567474365235, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.13209131360054016}, {"id": 465, "seek": 187800, "start": 1885.0, "end": 1886.0, "text": " Yeah.", "tokens": [50714, 865, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17335567474365235, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.13209131360054016}, {"id": 466, "seek": 187800, "start": 1886.0, "end": 1887.0, "text": " Thanks.", "tokens": [50764, 2561, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17335567474365235, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.13209131360054016}, {"id": 467, "seek": 187800, "start": 1887.0, "end": 1891.0, "text": " So now we've moved moving on from what to build.", "tokens": [50814, 407, 586, 321, 600, 4259, 2684, 322, 490, 437, 281, 1322, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17335567474365235, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.13209131360054016}, {"id": 468, "seek": 187800, "start": 1891.0, "end": 1894.0, "text": " And now we're going on to how we're actually going to go ahead and build that.", "tokens": [51014, 400, 586, 321, 434, 516, 322, 281, 577, 321, 434, 767, 516, 281, 352, 2286, 293, 1322, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17335567474365235, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.13209131360054016}, {"id": 469, "seek": 187800, "start": 1894.0, "end": 1898.0, "text": " So it's taken some of the suggestions of SageMaker, amplify us so that they're in bedrock.", "tokens": [51164, 407, 309, 311, 2726, 512, 295, 264, 13396, 295, 33812, 44, 4003, 11, 41174, 505, 370, 300, 436, 434, 294, 2901, 17799, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17335567474365235, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.13209131360054016}, {"id": 470, "seek": 187800, "start": 1898.0, "end": 1902.0, "text": " And now it's going to come through and ask us how are we going to do these.", "tokens": [51364, 400, 586, 309, 311, 516, 281, 808, 807, 293, 1029, 505, 577, 366, 321, 516, 281, 360, 613, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17335567474365235, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.13209131360054016}, {"id": 471, "seek": 187800, "start": 1902.0, "end": 1907.0, "text": " And if you think about these and now you're more technical requirements, we've defined the business requirements in the first phase.", "tokens": [51564, 400, 498, 291, 519, 466, 613, 293, 586, 291, 434, 544, 6191, 7728, 11, 321, 600, 7642, 264, 1606, 7728, 294, 264, 700, 5574, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17335567474365235, "compression_ratio": 1.769736842105263, "no_speech_prob": 0.13209131360054016}, {"id": 472, "seek": 190700, "start": 1907.0, "end": 1910.0, "text": " We're moving on to the more of the technical stuff right now.", "tokens": [50364, 492, 434, 2684, 322, 281, 264, 544, 295, 264, 6191, 1507, 558, 586, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 473, "seek": 190700, "start": 1910.0, "end": 1915.0, "text": " So early on in here, it's given us using that agent that we can figure.", "tokens": [50514, 407, 2440, 322, 294, 510, 11, 309, 311, 2212, 505, 1228, 300, 9461, 300, 321, 393, 2573, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 474, "seek": 190700, "start": 1915.0, "end": 1918.0, "text": " We told that to have the Mermaid MSP server.", "tokens": [50764, 492, 1907, 300, 281, 362, 264, 376, 32124, 7395, 47, 7154, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 475, "seek": 190700, "start": 1918.0, "end": 1921.0, "text": " It's gone in and actually built out some Mermaid diagrams for us.", "tokens": [50914, 467, 311, 2780, 294, 293, 767, 3094, 484, 512, 376, 32124, 36709, 337, 505, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 476, "seek": 190700, "start": 1921.0, "end": 1923.0, "text": " So I'll show you what that looks like.", "tokens": [51064, 407, 286, 603, 855, 291, 437, 300, 1542, 411, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 477, "seek": 190700, "start": 1923.0, "end": 1925.0, "text": " Hopefully that looks good on the screen.", "tokens": [51164, 10429, 300, 1542, 665, 322, 264, 2568, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 478, "seek": 190700, "start": 1925.0, "end": 1926.0, "text": " There we go.", "tokens": [51264, 821, 321, 352, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 479, "seek": 190700, "start": 1926.0, "end": 1931.0, "text": " So it's actually already documented and drawn out what it thinks this is going to look like.", "tokens": [51314, 407, 309, 311, 767, 1217, 23007, 293, 10117, 484, 437, 309, 7309, 341, 307, 516, 281, 574, 411, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 480, "seek": 190700, "start": 1931.0, "end": 1933.0, "text": " And do you remember that one prompt?", "tokens": [51564, 400, 360, 291, 1604, 300, 472, 12391, 30, 51664], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 481, "seek": 190700, "start": 1933.0, "end": 1936.0, "text": " Do you want everything in a single file or do you want to split up?", "tokens": [51664, 1144, 291, 528, 1203, 294, 257, 2167, 3991, 420, 360, 291, 528, 281, 7472, 493, 30, 51814], "temperature": 0.0, "avg_logprob": -0.13747898519855656, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.057465810328722}, {"id": 482, "seek": 193600, "start": 1936.0, "end": 1938.0, "text": " And it's going to hit and split that up for us as well.", "tokens": [50364, 400, 309, 311, 516, 281, 2045, 293, 7472, 300, 493, 337, 505, 382, 731, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 483, "seek": 193600, "start": 1938.0, "end": 1940.0, "text": " It's specific files for different things.", "tokens": [50464, 467, 311, 2685, 7098, 337, 819, 721, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 484, "seek": 193600, "start": 1940.0, "end": 1944.0, "text": " So we have JavaScript files and HTML files.", "tokens": [50564, 407, 321, 362, 15778, 7098, 293, 17995, 7098, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 485, "seek": 193600, "start": 1944.0, "end": 1947.0, "text": " So you can go in and make changes later.", "tokens": [50764, 407, 291, 393, 352, 294, 293, 652, 2962, 1780, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 486, "seek": 193600, "start": 1947.0, "end": 1950.0, "text": " So this is really useful for things like that too.", "tokens": [50914, 407, 341, 307, 534, 4420, 337, 721, 411, 300, 886, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 487, "seek": 193600, "start": 1950.0, "end": 1953.0, "text": " Sorry.", "tokens": [51064, 4919, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 488, "seek": 193600, "start": 1953.0, "end": 1958.0, "text": " I'll get back there.", "tokens": [51214, 286, 603, 483, 646, 456, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 489, "seek": 193600, "start": 1958.0, "end": 1960.0, "text": " Okay. And then if we look over here.", "tokens": [51464, 1033, 13, 400, 550, 498, 321, 574, 670, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 490, "seek": 193600, "start": 1960.0, "end": 1962.0, "text": " So let's have a look at this one here too.", "tokens": [51564, 407, 718, 311, 362, 257, 574, 412, 341, 472, 510, 886, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 491, "seek": 193600, "start": 1962.0, "end": 1964.0, "text": " It shows you the different components.", "tokens": [51664, 467, 3110, 291, 264, 819, 6677, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1752138819013323, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.008623912930488586}, {"id": 492, "seek": 196400, "start": 1964.0, "end": 1971.0, "text": " So we've got hope it here.", "tokens": [50364, 407, 321, 600, 658, 1454, 309, 510, 13, 50714], "temperature": 0.0, "avg_logprob": -0.23396380943588063, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.021297499537467957}, {"id": 493, "seek": 196400, "start": 1971.0, "end": 1973.0, "text": " Sorry, I'll lift my muscle home.", "tokens": [50714, 4919, 11, 286, 603, 5533, 452, 8679, 1280, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23396380943588063, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.021297499537467957}, {"id": 494, "seek": 196400, "start": 1973.0, "end": 1980.0, "text": " What a bit easier with that.", "tokens": [50814, 708, 257, 857, 3571, 365, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23396380943588063, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.021297499537467957}, {"id": 495, "seek": 196400, "start": 1980.0, "end": 1981.0, "text": " There we go.", "tokens": [51164, 821, 321, 352, 13, 51214], "temperature": 0.0, "avg_logprob": -0.23396380943588063, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.021297499537467957}, {"id": 496, "seek": 196400, "start": 1981.0, "end": 1984.0, "text": " So it's showing you the different layers.", "tokens": [51214, 407, 309, 311, 4099, 291, 264, 819, 7914, 13, 51364], "temperature": 0.0, "avg_logprob": -0.23396380943588063, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.021297499537467957}, {"id": 497, "seek": 196400, "start": 1984.0, "end": 1987.0, "text": " It's going to build out and how that's going to look.", "tokens": [51364, 467, 311, 516, 281, 1322, 484, 293, 577, 300, 311, 516, 281, 574, 13, 51514], "temperature": 0.0, "avg_logprob": -0.23396380943588063, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.021297499537467957}, {"id": 498, "seek": 196400, "start": 1987.0, "end": 1988.0, "text": " Look at on the screen.", "tokens": [51514, 2053, 412, 322, 264, 2568, 13, 51564], "temperature": 0.0, "avg_logprob": -0.23396380943588063, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.021297499537467957}, {"id": 499, "seek": 196400, "start": 1988.0, "end": 1989.0, "text": " Go a bit bigger.", "tokens": [51564, 1037, 257, 857, 3801, 13, 51614], "temperature": 0.0, "avg_logprob": -0.23396380943588063, "compression_ratio": 1.427710843373494, "no_speech_prob": 0.021297499537467957}, {"id": 500, "seek": 199400, "start": 1995.0, "end": 1997.0, "text": " So we find this really, really useful.", "tokens": [50414, 407, 321, 915, 341, 534, 11, 534, 4420, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 501, "seek": 199400, "start": 1997.0, "end": 2001.0, "text": " Customers have told us that this saves a lot of time with the documentation.", "tokens": [50514, 16649, 433, 362, 1907, 505, 300, 341, 19155, 257, 688, 295, 565, 365, 264, 14333, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 502, "seek": 199400, "start": 2001.0, "end": 2003.0, "text": " And how that's going to look.", "tokens": [50714, 400, 577, 300, 311, 516, 281, 574, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 503, "seek": 199400, "start": 2003.0, "end": 2005.0, "text": " And they can use that for all the different change controls.", "tokens": [50814, 400, 436, 393, 764, 300, 337, 439, 264, 819, 1319, 9003, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 504, "seek": 199400, "start": 2005.0, "end": 2008.0, "text": " And when they submit tickets to have this all built out.", "tokens": [50914, 400, 562, 436, 10315, 12628, 281, 362, 341, 439, 3094, 484, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 505, "seek": 199400, "start": 2008.0, "end": 2009.0, "text": " So we're getting all of that.", "tokens": [51064, 407, 321, 434, 1242, 439, 295, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 506, "seek": 199400, "start": 2009.0, "end": 2011.0, "text": " You can remember I said local storage early.", "tokens": [51114, 509, 393, 1604, 286, 848, 2654, 6725, 2440, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 507, "seek": 199400, "start": 2011.0, "end": 2013.0, "text": " We're not going to deploy that to anything else.", "tokens": [51214, 492, 434, 406, 516, 281, 7274, 300, 281, 1340, 1646, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 508, "seek": 199400, "start": 2013.0, "end": 2014.0, "text": " So that's kind of there.", "tokens": [51314, 407, 300, 311, 733, 295, 456, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 509, "seek": 199400, "start": 2014.0, "end": 2017.0, "text": " We already talked about how we're going to do the obstacles.", "tokens": [51364, 492, 1217, 2825, 466, 577, 321, 434, 516, 281, 360, 264, 17735, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 510, "seek": 199400, "start": 2017.0, "end": 2019.0, "text": " It's going to fill all that in the scoring.", "tokens": [51514, 467, 311, 516, 281, 2836, 439, 300, 294, 264, 22358, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 511, "seek": 199400, "start": 2019.0, "end": 2022.0, "text": " I said do that as we pass the obstacles.", "tokens": [51614, 286, 848, 360, 300, 382, 321, 1320, 264, 17735, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15689928908097117, "compression_ratio": 1.7942122186495177, "no_speech_prob": 0.10299411416053772}, {"id": 512, "seek": 202200, "start": 2022.0, "end": 2026.0, "text": " The difficulty controller from the easy medium and hard.", "tokens": [50364, 440, 10360, 10561, 490, 264, 1858, 6399, 293, 1152, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16813616130663, "compression_ratio": 1.5654205607476634, "no_speech_prob": 0.009237514808773994}, {"id": 513, "seek": 202200, "start": 2026.0, "end": 2029.0, "text": " So all of that's all in there already.", "tokens": [50564, 407, 439, 295, 300, 311, 439, 294, 456, 1217, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16813616130663, "compression_ratio": 1.5654205607476634, "no_speech_prob": 0.009237514808773994}, {"id": 514, "seek": 202200, "start": 2029.0, "end": 2030.0, "text": " Okay.", "tokens": [50714, 1033, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16813616130663, "compression_ratio": 1.5654205607476634, "no_speech_prob": 0.009237514808773994}, {"id": 515, "seek": 202200, "start": 2030.0, "end": 2034.0, "text": " So let's go back in and fill out that.", "tokens": [50764, 407, 718, 311, 352, 646, 294, 293, 2836, 484, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16813616130663, "compression_ratio": 1.5654205607476634, "no_speech_prob": 0.009237514808773994}, {"id": 516, "seek": 202200, "start": 2034.0, "end": 2039.0, "text": " Fill out the rest of that for the technical requirements.", "tokens": [50964, 25315, 484, 264, 1472, 295, 300, 337, 264, 6191, 7728, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16813616130663, "compression_ratio": 1.5654205607476634, "no_speech_prob": 0.009237514808773994}, {"id": 517, "seek": 202200, "start": 2039.0, "end": 2040.0, "text": " Okay.", "tokens": [51214, 1033, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16813616130663, "compression_ratio": 1.5654205607476634, "no_speech_prob": 0.009237514808773994}, {"id": 518, "seek": 202200, "start": 2040.0, "end": 2044.0, "text": " So the first question is which AI coding services did appear as obstacles.", "tokens": [51264, 407, 264, 700, 1168, 307, 597, 7318, 17720, 3328, 630, 4204, 382, 17735, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16813616130663, "compression_ratio": 1.5654205607476634, "no_speech_prob": 0.009237514808773994}, {"id": 519, "seek": 202200, "start": 2044.0, "end": 2051.0, "text": " So I'm just going to say use all and use them randomly.", "tokens": [51464, 407, 286, 478, 445, 516, 281, 584, 764, 439, 293, 764, 552, 16979, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16813616130663, "compression_ratio": 1.5654205607476634, "no_speech_prob": 0.009237514808773994}, {"id": 520, "seek": 205100, "start": 2051.0, "end": 2056.0, "text": " How should the difficulty levels differ?", "tokens": [50364, 1012, 820, 264, 10360, 4358, 743, 30, 50614], "temperature": 0.0, "avg_logprob": -0.16076933983528968, "compression_ratio": 1.596, "no_speech_prob": 0.016829654574394226}, {"id": 521, "seek": 205100, "start": 2056.0, "end": 2062.0, "text": " Now I found building this our testing this is it usually the gravity is way too strong.", "tokens": [50614, 823, 286, 1352, 2390, 341, 527, 4997, 341, 307, 309, 2673, 264, 12110, 307, 636, 886, 2068, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16076933983528968, "compression_ratio": 1.596, "no_speech_prob": 0.016829654574394226}, {"id": 522, "seek": 205100, "start": 2062.0, "end": 2067.0, "text": " So the board just falls like a stone no matter how quickly I press this base bar.", "tokens": [50914, 407, 264, 3150, 445, 8804, 411, 257, 7581, 572, 1871, 577, 2661, 286, 1886, 341, 3096, 2159, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16076933983528968, "compression_ratio": 1.596, "no_speech_prob": 0.016829654574394226}, {"id": 523, "seek": 205100, "start": 2067.0, "end": 2069.0, "text": " And the gaps are way too short.", "tokens": [51164, 400, 264, 15031, 366, 636, 886, 2099, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16076933983528968, "compression_ratio": 1.596, "no_speech_prob": 0.016829654574394226}, {"id": 524, "seek": 205100, "start": 2069.0, "end": 2072.0, "text": " So I can never get past the first couple of hurdles.", "tokens": [51264, 407, 286, 393, 1128, 483, 1791, 264, 700, 1916, 295, 48387, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16076933983528968, "compression_ratio": 1.596, "no_speech_prob": 0.016829654574394226}, {"id": 525, "seek": 205100, "start": 2072.0, "end": 2078.0, "text": " So I might go in here from my previous knowledge and tell it to be to be a little bit better with that.", "tokens": [51414, 407, 286, 1062, 352, 294, 510, 490, 452, 3894, 3601, 293, 980, 309, 281, 312, 281, 312, 257, 707, 857, 1101, 365, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16076933983528968, "compression_ratio": 1.596, "no_speech_prob": 0.016829654574394226}, {"id": 526, "seek": 207800, "start": 2078.0, "end": 2090.0, "text": " So I'll say start slower logic gaps and less gravity.", "tokens": [50364, 407, 286, 603, 584, 722, 14009, 9952, 15031, 293, 1570, 12110, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18320267775963092, "compression_ratio": 1.355263157894737, "no_speech_prob": 0.024008577689528465}, {"id": 527, "seek": 207800, "start": 2090.0, "end": 2096.0, "text": " And then I want to use 60 frames per second.", "tokens": [50964, 400, 550, 286, 528, 281, 764, 4060, 12083, 680, 1150, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18320267775963092, "compression_ratio": 1.355263157894737, "no_speech_prob": 0.024008577689528465}, {"id": 528, "seek": 207800, "start": 2096.0, "end": 2100.0, "text": " And then now we've got the visuals.", "tokens": [51264, 400, 550, 586, 321, 600, 658, 264, 26035, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18320267775963092, "compression_ratio": 1.355263157894737, "no_speech_prob": 0.024008577689528465}, {"id": 529, "seek": 207800, "start": 2100.0, "end": 2105.0, "text": " So shall I generate a simple CSS canvas or do we have any image assets?", "tokens": [51464, 407, 4393, 286, 8460, 257, 2199, 24387, 16267, 420, 360, 321, 362, 604, 3256, 9769, 30, 51714], "temperature": 0.0, "avg_logprob": -0.18320267775963092, "compression_ratio": 1.355263157894737, "no_speech_prob": 0.024008577689528465}, {"id": 530, "seek": 210500, "start": 2105.0, "end": 2108.0, "text": " I start with a normal simple CSS canvas.", "tokens": [50364, 286, 722, 365, 257, 2710, 2199, 24387, 16267, 13, 50514], "temperature": 0.0, "avg_logprob": -0.18001190034469755, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.06613796204328537}, {"id": 531, "seek": 210500, "start": 2108.0, "end": 2113.0, "text": " But then you can go back in download specific assets or any audio images that you've got for the game.", "tokens": [50514, 583, 550, 291, 393, 352, 646, 294, 5484, 2685, 9769, 420, 604, 6278, 5267, 300, 291, 600, 658, 337, 264, 1216, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18001190034469755, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.06613796204328537}, {"id": 532, "seek": 210500, "start": 2113.0, "end": 2114.0, "text": " And you can put that in.", "tokens": [50764, 400, 291, 393, 829, 300, 294, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18001190034469755, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.06613796204328537}, {"id": 533, "seek": 210500, "start": 2114.0, "end": 2118.0, "text": " But it does a pretty good job of filling just coming up with stuff itself.", "tokens": [50814, 583, 309, 775, 257, 1238, 665, 1691, 295, 10623, 445, 1348, 493, 365, 1507, 2564, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18001190034469755, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.06613796204328537}, {"id": 534, "seek": 210500, "start": 2118.0, "end": 2121.0, "text": " So we'll go with CSS.", "tokens": [51014, 407, 321, 603, 352, 365, 24387, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18001190034469755, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.06613796204328537}, {"id": 535, "seek": 210500, "start": 2121.0, "end": 2123.0, "text": " And then ghost animation.", "tokens": [51164, 400, 550, 8359, 9603, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18001190034469755, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.06613796204328537}, {"id": 536, "seek": 210500, "start": 2123.0, "end": 2126.0, "text": " Do you want to static bobbing, flipping wings.", "tokens": [51264, 1144, 291, 528, 281, 13437, 27292, 4324, 11, 26886, 11405, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18001190034469755, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.06613796204328537}, {"id": 537, "seek": 210500, "start": 2126.0, "end": 2130.0, "text": " Let's animate the ghost.", "tokens": [51414, 961, 311, 36439, 264, 8359, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18001190034469755, "compression_ratio": 1.5446808510638297, "no_speech_prob": 0.06613796204328537}, {"id": 538, "seek": 213000, "start": 2131.0, "end": 2138.0, "text": " What it's applying.", "tokens": [50414, 708, 309, 311, 9275, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2775079126227392, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.05553022399544716}, {"id": 539, "seek": 213000, "start": 2138.0, "end": 2142.0, "text": " Let's use official AWS icons.", "tokens": [50764, 961, 311, 764, 4783, 17650, 23308, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2775079126227392, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.05553022399544716}, {"id": 540, "seek": 213000, "start": 2142.0, "end": 2144.0, "text": " I don't think it's going to be able to do that.", "tokens": [50964, 286, 500, 380, 519, 309, 311, 516, 281, 312, 1075, 281, 360, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2775079126227392, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.05553022399544716}, {"id": 541, "seek": 213000, "start": 2144.0, "end": 2145.0, "text": " And this will give it to them.", "tokens": [51064, 400, 341, 486, 976, 309, 281, 552, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2775079126227392, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.05553022399544716}, {"id": 542, "seek": 213000, "start": 2145.0, "end": 2147.0, "text": " But it's going to try and draw them up.", "tokens": [51114, 583, 309, 311, 516, 281, 853, 293, 2642, 552, 493, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2775079126227392, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.05553022399544716}, {"id": 543, "seek": 213000, "start": 2147.0, "end": 2151.0, "text": " Typically using mode uses something like that too.", "tokens": [51214, 23129, 1228, 4391, 4960, 746, 411, 300, 886, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2775079126227392, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.05553022399544716}, {"id": 544, "seek": 213000, "start": 2151.0, "end": 2153.0, "text": " Sound effects.", "tokens": [51414, 14673, 5065, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2775079126227392, "compression_ratio": 1.4268292682926829, "no_speech_prob": 0.05553022399544716}, {"id": 545, "seek": 215300, "start": 2153.0, "end": 2156.0, "text": " I'll just say use.", "tokens": [50364, 286, 603, 445, 584, 764, 13, 50514], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 546, "seek": 215300, "start": 2156.0, "end": 2159.0, "text": " Oh, suggested effects.", "tokens": [50514, 876, 11, 10945, 5065, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 547, "seek": 215300, "start": 2159.0, "end": 2163.0, "text": " Discup interest of time.", "tokens": [50664, 19839, 1010, 1179, 295, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 548, "seek": 215300, "start": 2163.0, "end": 2164.0, "text": " Here we go.", "tokens": [50864, 1692, 321, 352, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 549, "seek": 215300, "start": 2164.0, "end": 2165.0, "text": " And then this question.", "tokens": [50914, 400, 550, 341, 1168, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 550, "seek": 215300, "start": 2165.0, "end": 2166.0, "text": " Yeah.", "tokens": [50964, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 551, "seek": 215300, "start": 2166.0, "end": 2167.0, "text": " The bedroke integration.", "tokens": [51014, 440, 2901, 340, 330, 10980, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 552, "seek": 215300, "start": 2167.0, "end": 2170.0, "text": " How should Amazon bedrope be integrated dynamically.", "tokens": [51064, 1012, 820, 6795, 2901, 340, 494, 312, 10919, 43492, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 553, "seek": 215300, "start": 2170.0, "end": 2172.0, "text": " AI power difficulty adjustments.", "tokens": [51214, 7318, 1347, 10360, 18624, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 554, "seek": 215300, "start": 2172.0, "end": 2173.0, "text": " General rate.", "tokens": [51314, 6996, 3314, 13, 51364], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 555, "seek": 215300, "start": 2173.0, "end": 2174.0, "text": " What do you think?", "tokens": [51364, 708, 360, 291, 519, 30, 51414], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 556, "seek": 215300, "start": 2174.0, "end": 2175.0, "text": " This is going to be interesting.", "tokens": [51414, 639, 307, 516, 281, 312, 1880, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 557, "seek": 215300, "start": 2175.0, "end": 2178.0, "text": " I think those are all future future enhancements.", "tokens": [51464, 286, 519, 729, 366, 439, 2027, 2027, 11985, 1117, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 558, "seek": 215300, "start": 2178.0, "end": 2179.0, "text": " Yeah.", "tokens": [51614, 865, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 559, "seek": 215300, "start": 2179.0, "end": 2180.0, "text": " Yeah.", "tokens": [51664, 865, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 560, "seek": 215300, "start": 2180.0, "end": 2181.0, "text": " Probably doesn't matter for this demo.", "tokens": [51714, 9210, 1177, 380, 1871, 337, 341, 10723, 13, 51764], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 561, "seek": 215300, "start": 2181.0, "end": 2182.0, "text": " Yeah.", "tokens": [51764, 865, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3040509993030179, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.05014026165008545}, {"id": 562, "seek": 218200, "start": 2182.0, "end": 2192.0, "text": " And then here's the file structure that it's going to go ahead and create for you.", "tokens": [50364, 400, 550, 510, 311, 264, 3991, 3877, 300, 309, 311, 516, 281, 352, 2286, 293, 1884, 337, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17554030126454878, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.010799404233694077}, {"id": 563, "seek": 218200, "start": 2192.0, "end": 2194.0, "text": " So instead of creating that one big file, he saw that diagram.", "tokens": [50864, 407, 2602, 295, 4084, 300, 472, 955, 3991, 11, 415, 1866, 300, 10686, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17554030126454878, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.010799404233694077}, {"id": 564, "seek": 218200, "start": 2194.0, "end": 2198.0, "text": " It's going to go ahead and break these down for you in the different files that you're going to use.", "tokens": [50964, 467, 311, 516, 281, 352, 2286, 293, 1821, 613, 760, 337, 291, 294, 264, 819, 7098, 300, 291, 434, 516, 281, 764, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17554030126454878, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.010799404233694077}, {"id": 565, "seek": 218200, "start": 2198.0, "end": 2203.0, "text": " And that means you can come back in and put in your own images and your own sounds when you want to do so.", "tokens": [51164, 400, 300, 1355, 291, 393, 808, 646, 294, 293, 829, 294, 428, 1065, 5267, 293, 428, 1065, 3263, 562, 291, 528, 281, 360, 370, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17554030126454878, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.010799404233694077}, {"id": 566, "seek": 218200, "start": 2203.0, "end": 2205.0, "text": " I'll just say.", "tokens": [51414, 286, 603, 445, 584, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17554030126454878, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.010799404233694077}, {"id": 567, "seek": 220500, "start": 2205.0, "end": 2212.0, "text": " Okay.", "tokens": [50364, 1033, 13, 50714], "temperature": 0.0, "avg_logprob": -0.6174261728922527, "compression_ratio": 0.7837837837837838, "no_speech_prob": 0.06373101472854614}, {"id": 568, "seek": 220500, "start": 2212.0, "end": 2219.0, "text": " And I hope it's set by.", "tokens": [50714, 400, 286, 1454, 309, 311, 992, 538, 13, 51064], "temperature": 0.0, "avg_logprob": -0.6174261728922527, "compression_ratio": 0.7837837837837838, "no_speech_prob": 0.06373101472854614}, {"id": 569, "seek": 223500, "start": 2236.0, "end": 2248.0, "text": " So again, we're almost kind of like speed running through the steps and almost five coding in a way right by not engaging more here and doing a much richer discussion in the interest of time.", "tokens": [50414, 407, 797, 11, 321, 434, 1920, 733, 295, 411, 3073, 2614, 807, 264, 4439, 293, 1920, 1732, 17720, 294, 257, 636, 558, 538, 406, 11268, 544, 510, 293, 884, 257, 709, 29021, 5017, 294, 264, 1179, 295, 565, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15517320304081358, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.06170425936579704}, {"id": 570, "seek": 223500, "start": 2248.0, "end": 2251.0, "text": " But, you know, again, I think you can imagine.", "tokens": [51014, 583, 11, 291, 458, 11, 797, 11, 286, 519, 291, 393, 3811, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15517320304081358, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.06170425936579704}, {"id": 571, "seek": 223500, "start": 2251.0, "end": 2260.0, "text": " This is where, in my opinion, the interesting work is going to happen is around these decisions that we're making around technology selection.", "tokens": [51164, 639, 307, 689, 11, 294, 452, 4800, 11, 264, 1880, 589, 307, 516, 281, 1051, 307, 926, 613, 5327, 300, 321, 434, 1455, 926, 2899, 9450, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15517320304081358, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.06170425936579704}, {"id": 572, "seek": 226000, "start": 2260.0, "end": 2274.0, "text": " And what should we do first and what can, you know, what's a P0 versus a P1 feature and what's our architecture going to look like, you know, this is where we as humans should be spending our time.", "tokens": [50364, 400, 437, 820, 321, 360, 700, 293, 437, 393, 11, 291, 458, 11, 437, 311, 257, 430, 15, 5717, 257, 430, 16, 4111, 293, 437, 311, 527, 9482, 516, 281, 574, 411, 11, 291, 458, 11, 341, 307, 689, 321, 382, 6255, 820, 312, 6434, 527, 565, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09900031306526878, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.057172741740942}, {"id": 573, "seek": 226000, "start": 2274.0, "end": 2283.0, "text": " And if we get this right up front, the actual implementation phase is almost the easy part, hopefully, because we've already done all of this hard work up front.", "tokens": [51064, 400, 498, 321, 483, 341, 558, 493, 1868, 11, 264, 3539, 11420, 5574, 307, 1920, 264, 1858, 644, 11, 4696, 11, 570, 321, 600, 1217, 1096, 439, 295, 341, 1152, 589, 493, 1868, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09900031306526878, "compression_ratio": 1.609865470852018, "no_speech_prob": 0.057172741740942}, {"id": 574, "seek": 228300, "start": 2283.0, "end": 2286.0, "text": " And so if you're not already, I would encourage you.", "tokens": [50364, 400, 370, 498, 291, 434, 406, 1217, 11, 286, 576, 5373, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16693596792693186, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0838555321097374}, {"id": 575, "seek": 228300, "start": 2286.0, "end": 2291.0, "text": " If you try to add you'll see, you know, spend a good amount of time up front in these phases.", "tokens": [50514, 759, 291, 853, 281, 909, 291, 603, 536, 11, 291, 458, 11, 3496, 257, 665, 2372, 295, 565, 493, 1868, 294, 613, 18764, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16693596792693186, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0838555321097374}, {"id": 576, "seek": 228300, "start": 2291.0, "end": 2293.0, "text": " Get this as refined as you can.", "tokens": [50764, 3240, 341, 382, 26201, 382, 291, 393, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16693596792693186, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0838555321097374}, {"id": 577, "seek": 228300, "start": 2293.0, "end": 2301.0, "text": " And then see how the implementation step works out for you.", "tokens": [50864, 400, 550, 536, 577, 264, 11420, 1823, 1985, 484, 337, 291, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16693596792693186, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0838555321097374}, {"id": 578, "seek": 228300, "start": 2301.0, "end": 2302.0, "text": " Yeah.", "tokens": [51264, 865, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16693596792693186, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0838555321097374}, {"id": 579, "seek": 228300, "start": 2302.0, "end": 2309.0, "text": " So we'll link at the end to the GitHub repo, but our teams have put a good amount of work over the last few months.", "tokens": [51314, 407, 321, 603, 2113, 412, 264, 917, 281, 264, 23331, 49040, 11, 457, 527, 5491, 362, 829, 257, 665, 2372, 295, 589, 670, 264, 1036, 1326, 2493, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16693596792693186, "compression_ratio": 1.5859030837004404, "no_speech_prob": 0.0838555321097374}, {"id": 580, "seek": 230900, "start": 2309.0, "end": 2312.0, "text": " Again, working with lots of different customers to refine this.", "tokens": [50364, 3764, 11, 1364, 365, 3195, 295, 819, 4581, 281, 33906, 341, 13, 50514], "temperature": 0.0, "avg_logprob": -0.0881445496170609, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.10259885340929031}, {"id": 581, "seek": 230900, "start": 2312.0, "end": 2319.0, "text": " We're using conditional context loading because we have a finite amount of context window space.", "tokens": [50514, 492, 434, 1228, 27708, 4319, 15114, 570, 321, 362, 257, 19362, 2372, 295, 4319, 4910, 1901, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0881445496170609, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.10259885340929031}, {"id": 582, "seek": 230900, "start": 2319.0, "end": 2322.0, "text": " There's an initial context file that we loaded.", "tokens": [50864, 821, 311, 364, 5883, 4319, 3991, 300, 321, 13210, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0881445496170609, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.10259885340929031}, {"id": 583, "seek": 230900, "start": 2322.0, "end": 2327.0, "text": " That kind of kicks off things with the agent and tells it, hey, you're using this workflow.", "tokens": [51014, 663, 733, 295, 21293, 766, 721, 365, 264, 9461, 293, 5112, 309, 11, 4177, 11, 291, 434, 1228, 341, 20993, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0881445496170609, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.10259885340929031}, {"id": 584, "seek": 230900, "start": 2327.0, "end": 2329.0, "text": " You're going to use these various specific steps.", "tokens": [51264, 509, 434, 516, 281, 764, 613, 3683, 2685, 4439, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0881445496170609, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.10259885340929031}, {"id": 585, "seek": 230900, "start": 2329.0, "end": 2335.0, "text": " And then depending on what you're doing, if you're doing green field or if you're extending an existing application,", "tokens": [51364, 400, 550, 5413, 322, 437, 291, 434, 884, 11, 498, 291, 434, 884, 3092, 2519, 420, 498, 291, 434, 24360, 364, 6741, 3861, 11, 51664], "temperature": 0.0, "avg_logprob": -0.0881445496170609, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.10259885340929031}, {"id": 586, "seek": 233500, "start": 2335.0, "end": 2341.0, "text": " if you want to do microservices, there's a bunch of decision trees that it'll make and say, oh, we're using it.", "tokens": [50364, 498, 291, 528, 281, 360, 15547, 47480, 11, 456, 311, 257, 3840, 295, 3537, 5852, 300, 309, 603, 652, 293, 584, 11, 1954, 11, 321, 434, 1228, 309, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14293545263784904, "compression_ratio": 1.6504065040650406, "no_speech_prob": 0.017492542043328285}, {"id": 587, "seek": 233500, "start": 2341.0, "end": 2343.0, "text": " We're going to extend a legacy application.", "tokens": [50664, 492, 434, 516, 281, 10101, 257, 11711, 3861, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14293545263784904, "compression_ratio": 1.6504065040650406, "no_speech_prob": 0.017492542043328285}, {"id": 588, "seek": 233500, "start": 2343.0, "end": 2350.0, "text": " It'll actually load in additional instruction sets to make sure that we're doing things in an orderly way.", "tokens": [50764, 467, 603, 767, 3677, 294, 4497, 10951, 6352, 281, 652, 988, 300, 321, 434, 884, 721, 294, 364, 1668, 356, 636, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14293545263784904, "compression_ratio": 1.6504065040650406, "no_speech_prob": 0.017492542043328285}, {"id": 589, "seek": 233500, "start": 2350.0, "end": 2356.0, "text": " So there's a lot behind the scenes here that's getting loaded in as needed.", "tokens": [51114, 407, 456, 311, 257, 688, 2261, 264, 8026, 510, 300, 311, 1242, 13210, 294, 382, 2978, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14293545263784904, "compression_ratio": 1.6504065040650406, "no_speech_prob": 0.017492542043328285}, {"id": 590, "seek": 233500, "start": 2356.0, "end": 2357.0, "text": " Yep.", "tokens": [51414, 7010, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14293545263784904, "compression_ratio": 1.6504065040650406, "no_speech_prob": 0.017492542043328285}, {"id": 591, "seek": 233500, "start": 2357.0, "end": 2360.0, "text": " Yep. There's just the one marked down file is the entry point.", "tokens": [51464, 7010, 13, 821, 311, 445, 264, 472, 12658, 760, 3991, 307, 264, 8729, 935, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14293545263784904, "compression_ratio": 1.6504065040650406, "no_speech_prob": 0.017492542043328285}, {"id": 592, "seek": 236000, "start": 2360.0, "end": 2366.0, "text": " And it has instructions that says, conditionally load these other files depending on what you're doing.", "tokens": [50364, 400, 309, 575, 9415, 300, 1619, 11, 4188, 379, 3677, 613, 661, 7098, 5413, 322, 437, 291, 434, 884, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13896635666634272, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.17481134831905365}, {"id": 593, "seek": 236000, "start": 2366.0, "end": 2372.0, "text": " We found that approach works well to keep the context window, not too busy.", "tokens": [50664, 492, 1352, 300, 3109, 1985, 731, 281, 1066, 264, 4319, 4910, 11, 406, 886, 5856, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13896635666634272, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.17481134831905365}, {"id": 594, "seek": 236000, "start": 2372.0, "end": 2373.0, "text": " Yeah.", "tokens": [50964, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13896635666634272, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.17481134831905365}, {"id": 595, "seek": 236000, "start": 2373.0, "end": 2378.0, "text": " And then to your point, the other thing you can do is you can have different agents for different phases of your development as well.", "tokens": [51014, 400, 550, 281, 428, 935, 11, 264, 661, 551, 291, 393, 360, 307, 291, 393, 362, 819, 12554, 337, 819, 18764, 295, 428, 3250, 382, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13896635666634272, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.17481134831905365}, {"id": 596, "seek": 236000, "start": 2378.0, "end": 2385.0, "text": " So if you're doing front end development back end development, you can have different agents with different MCP servers, different resources.", "tokens": [51264, 407, 498, 291, 434, 884, 1868, 917, 3250, 646, 917, 3250, 11, 291, 393, 362, 819, 12554, 365, 819, 8797, 47, 15909, 11, 819, 3593, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13896635666634272, "compression_ratio": 1.8221343873517786, "no_speech_prob": 0.17481134831905365}, {"id": 597, "seek": 238500, "start": 2385.0, "end": 2390.0, "text": " And that way you can just switch between those agents as easily as I showed you there.", "tokens": [50364, 400, 300, 636, 291, 393, 445, 3679, 1296, 729, 12554, 382, 3612, 382, 286, 4712, 291, 456, 13, 50614], "temperature": 0.0, "avg_logprob": -0.20359527530954846, "compression_ratio": 1.723127035830619, "no_speech_prob": 0.12376123666763306}, {"id": 598, "seek": 238500, "start": 2390.0, "end": 2391.0, "text": " Yeah.", "tokens": [50614, 865, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20359527530954846, "compression_ratio": 1.723127035830619, "no_speech_prob": 0.12376123666763306}, {"id": 599, "seek": 238500, "start": 2391.0, "end": 2395.0, "text": " And that way, that's a good way of getting a project done without loading everything up.", "tokens": [50664, 400, 300, 636, 11, 300, 311, 257, 665, 636, 295, 1242, 257, 1716, 1096, 1553, 15114, 1203, 493, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20359527530954846, "compression_ratio": 1.723127035830619, "no_speech_prob": 0.12376123666763306}, {"id": 600, "seek": 238500, "start": 2395.0, "end": 2396.0, "text": " So yeah.", "tokens": [50864, 407, 1338, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20359527530954846, "compression_ratio": 1.723127035830619, "no_speech_prob": 0.12376123666763306}, {"id": 601, "seek": 238500, "start": 2396.0, "end": 2397.0, "text": " Okay.", "tokens": [50914, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20359527530954846, "compression_ratio": 1.723127035830619, "no_speech_prob": 0.12376123666763306}, {"id": 602, "seek": 238500, "start": 2397.0, "end": 2401.0, "text": " So now this is the part where we move to the actual implementation phase.", "tokens": [50964, 407, 586, 341, 307, 264, 644, 689, 321, 1286, 281, 264, 3539, 11420, 5574, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20359527530954846, "compression_ratio": 1.723127035830619, "no_speech_prob": 0.12376123666763306}, {"id": 603, "seek": 238500, "start": 2401.0, "end": 2406.0, "text": " It's 11 written any code yet, but now we're going to, you know, this thing you can start having it, you know,", "tokens": [51164, 467, 311, 2975, 3720, 604, 3089, 1939, 11, 457, 586, 321, 434, 516, 281, 11, 291, 458, 11, 341, 551, 291, 393, 722, 1419, 309, 11, 291, 458, 11, 51414], "temperature": 0.0, "avg_logprob": -0.20359527530954846, "compression_ratio": 1.723127035830619, "no_speech_prob": 0.12376123666763306}, {"id": 604, "seek": 238500, "start": 2406.0, "end": 2413.0, "text": " georeticates for you and you can have your junior developers go ahead and run this based on the architecture diagrams that your senior developers or", "tokens": [51414, 1519, 26262, 299, 1024, 337, 291, 293, 291, 393, 362, 428, 16195, 8849, 352, 2286, 293, 1190, 341, 2361, 322, 264, 9482, 36709, 300, 428, 7965, 8849, 420, 51764], "temperature": 0.0, "avg_logprob": -0.20359527530954846, "compression_ratio": 1.723127035830619, "no_speech_prob": 0.12376123666763306}, {"id": 605, "seek": 241300, "start": 2413.0, "end": 2415.0, "text": " Kero has built for you.", "tokens": [50364, 591, 2032, 575, 3094, 337, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.16580902657857755, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0509389191865921}, {"id": 606, "seek": 241300, "start": 2415.0, "end": 2421.0, "text": " So again, we're just going to go through these real quick and see what we can get from all of it.", "tokens": [50464, 407, 797, 11, 321, 434, 445, 516, 281, 352, 807, 613, 957, 1702, 293, 536, 437, 321, 393, 483, 490, 439, 295, 309, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16580902657857755, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0509389191865921}, {"id": 607, "seek": 241300, "start": 2421.0, "end": 2423.0, "text": " So a proof to implement.", "tokens": [50764, 407, 257, 8177, 281, 4445, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16580902657857755, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0509389191865921}, {"id": 608, "seek": 241300, "start": 2423.0, "end": 2429.0, "text": " Yes, we're just going to go through the the reservation size, link all the modules together.", "tokens": [50864, 1079, 11, 321, 434, 445, 516, 281, 352, 807, 264, 264, 28922, 2744, 11, 2113, 439, 264, 16679, 1214, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16580902657857755, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0509389191865921}, {"id": 609, "seek": 241300, "start": 2429.0, "end": 2436.0, "text": " And then here, if you have any specific styling that your organization might use, this is a, you know, a good place where you can do that.", "tokens": [51164, 400, 550, 510, 11, 498, 291, 362, 604, 2685, 27944, 300, 428, 4475, 1062, 764, 11, 341, 307, 257, 11, 291, 458, 11, 257, 665, 1081, 689, 291, 393, 360, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16580902657857755, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0509389191865921}, {"id": 610, "seek": 241300, "start": 2436.0, "end": 2440.0, "text": " And if you're building an enterprise app, so that's something you can do there as well.", "tokens": [51514, 400, 498, 291, 434, 2390, 364, 14132, 724, 11, 370, 300, 311, 746, 291, 393, 360, 456, 382, 731, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16580902657857755, "compression_ratio": 1.7388059701492538, "no_speech_prob": 0.0509389191865921}, {"id": 611, "seek": 244000, "start": 2440.0, "end": 2448.0, "text": " And if there's anything, you know, you'd be sitting with different teams across your business going through all of this and figuring out, is this the right thing to do, but it's not captured in here.", "tokens": [50364, 400, 498, 456, 311, 1340, 11, 291, 458, 11, 291, 1116, 312, 3798, 365, 819, 5491, 2108, 428, 1606, 516, 807, 439, 295, 341, 293, 15213, 484, 11, 307, 341, 264, 558, 551, 281, 360, 11, 457, 309, 311, 406, 11828, 294, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13978398735843486, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.01658247411251068}, {"id": 612, "seek": 244000, "start": 2448.0, "end": 2452.0, "text": " You can obviously add it in there and change some of those files as well.", "tokens": [50764, 509, 393, 2745, 909, 309, 294, 456, 293, 1319, 512, 295, 729, 7098, 382, 731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13978398735843486, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.01658247411251068}, {"id": 613, "seek": 244000, "start": 2452.0, "end": 2454.0, "text": " Then we go through that as well.", "tokens": [50964, 1396, 321, 352, 807, 300, 382, 731, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13978398735843486, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.01658247411251068}, {"id": 614, "seek": 244000, "start": 2454.0, "end": 2455.0, "text": " Yeah.", "tokens": [51064, 865, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13978398735843486, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.01658247411251068}, {"id": 615, "seek": 244000, "start": 2455.0, "end": 2457.0, "text": " So we are going to use JavaScript and HTML.", "tokens": [51114, 407, 321, 366, 516, 281, 764, 15778, 293, 17995, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13978398735843486, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.01658247411251068}, {"id": 616, "seek": 244000, "start": 2457.0, "end": 2459.0, "text": " I think we decided that was going to be the best thing.", "tokens": [51214, 286, 519, 321, 3047, 300, 390, 516, 281, 312, 264, 1151, 551, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13978398735843486, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.01658247411251068}, {"id": 617, "seek": 244000, "start": 2459.0, "end": 2460.0, "text": " So we're going to go ahead with that.", "tokens": [51314, 407, 321, 434, 516, 281, 352, 2286, 365, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13978398735843486, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.01658247411251068}, {"id": 618, "seek": 244000, "start": 2460.0, "end": 2465.0, "text": " It's just asking for that final verification before we do anything.", "tokens": [51364, 467, 311, 445, 3365, 337, 300, 2572, 30206, 949, 321, 360, 1340, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13978398735843486, "compression_ratio": 1.7266666666666666, "no_speech_prob": 0.01658247411251068}, {"id": 619, "seek": 246500, "start": 2466.0, "end": 2470.0, "text": " Yep.", "tokens": [50414, 7010, 13, 50614], "temperature": 0.0, "avg_logprob": -0.22825539786860627, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.06830140948295593}, {"id": 620, "seek": 246500, "start": 2470.0, "end": 2473.0, "text": " There is a GMCP and so I was just thinking the same thing.", "tokens": [50614, 821, 307, 257, 16609, 20049, 293, 370, 286, 390, 445, 1953, 264, 912, 551, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22825539786860627, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.06830140948295593}, {"id": 621, "seek": 246500, "start": 2473.0, "end": 2479.0, "text": " Once you get to this step in the real world, each of these might be more complicated and take more time.", "tokens": [50764, 3443, 291, 483, 281, 341, 1823, 294, 264, 957, 1002, 11, 1184, 295, 613, 1062, 312, 544, 6179, 293, 747, 544, 565, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22825539786860627, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.06830140948295593}, {"id": 622, "seek": 246500, "start": 2479.0, "end": 2486.0, "text": " So you can imagine each of these becoming a story that goes on to the gerabord and gets pulled down for sure.", "tokens": [51064, 407, 291, 393, 3811, 1184, 295, 613, 5617, 257, 1657, 300, 1709, 322, 281, 264, 5713, 455, 765, 293, 2170, 7373, 760, 337, 988, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22825539786860627, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.06830140948295593}, {"id": 623, "seek": 246500, "start": 2486.0, "end": 2492.0, "text": " That could be as simple as us taking this file and saying, go, go create gerastories for each of these.", "tokens": [51414, 663, 727, 312, 382, 2199, 382, 505, 1940, 341, 3991, 293, 1566, 11, 352, 11, 352, 1884, 5713, 525, 2083, 337, 1184, 295, 613, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22825539786860627, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.06830140948295593}, {"id": 624, "seek": 246500, "start": 2492.0, "end": 2493.0, "text": " Yeah.", "tokens": [51714, 865, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22825539786860627, "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.06830140948295593}, {"id": 625, "seek": 249300, "start": 2493.0, "end": 2497.0, "text": " I'll be honest, this is different from the other ones that I've done before.", "tokens": [50364, 286, 603, 312, 3245, 11, 341, 307, 819, 490, 264, 661, 2306, 300, 286, 600, 1096, 949, 13, 50564], "temperature": 0.0, "avg_logprob": -0.20596110026041667, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.025886772200465202}, {"id": 626, "seek": 249300, "start": 2497.0, "end": 2501.0, "text": " Normally it's another bunch of questions, but here it's just basically saying you've provided me enough.", "tokens": [50564, 17424, 309, 311, 1071, 3840, 295, 1651, 11, 457, 510, 309, 311, 445, 1936, 1566, 291, 600, 5649, 385, 1547, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20596110026041667, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.025886772200465202}, {"id": 627, "seek": 249300, "start": 2501.0, "end": 2503.0, "text": " And just give me the verification for that.", "tokens": [50764, 400, 445, 976, 385, 264, 30206, 337, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20596110026041667, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.025886772200465202}, {"id": 628, "seek": 249300, "start": 2503.0, "end": 2505.0, "text": " So just answering yes.", "tokens": [50864, 407, 445, 13430, 2086, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20596110026041667, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.025886772200465202}, {"id": 629, "seek": 249300, "start": 2505.0, "end": 2508.0, "text": " Well, I'll have a look at this diagram that it's given us.", "tokens": [50964, 1042, 11, 286, 603, 362, 257, 574, 412, 341, 10686, 300, 309, 311, 2212, 505, 13, 51114], "temperature": 0.0, "avg_logprob": -0.20596110026041667, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.025886772200465202}, {"id": 630, "seek": 249300, "start": 2508.0, "end": 2511.0, "text": " This should be the implementation diagram.", "tokens": [51114, 639, 820, 312, 264, 11420, 10686, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20596110026041667, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.025886772200465202}, {"id": 631, "seek": 251100, "start": 2511.0, "end": 2524.0, "text": " I didn't grab everything that I have for some reason.", "tokens": [50364, 286, 994, 380, 4444, 1203, 300, 286, 362, 337, 512, 1778, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3551147335865458, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.10720236599445343}, {"id": 632, "seek": 251100, "start": 2524.0, "end": 2527.0, "text": " I thought I had it all.", "tokens": [51014, 286, 1194, 286, 632, 309, 439, 13, 51164], "temperature": 0.0, "avg_logprob": -0.3551147335865458, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.10720236599445343}, {"id": 633, "seek": 251100, "start": 2527.0, "end": 2528.0, "text": " Yeah, I did.", "tokens": [51164, 865, 11, 286, 630, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3551147335865458, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.10720236599445343}, {"id": 634, "seek": 251100, "start": 2528.0, "end": 2530.0, "text": " Didn't like that.", "tokens": [51214, 11151, 380, 411, 300, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3551147335865458, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.10720236599445343}, {"id": 635, "seek": 251100, "start": 2530.0, "end": 2537.0, "text": " I'll try it once more.", "tokens": [51314, 286, 603, 853, 309, 1564, 544, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3551147335865458, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.10720236599445343}, {"id": 636, "seek": 251100, "start": 2537.0, "end": 2540.0, "text": " I suggest here in the interest of time that we,", "tokens": [51664, 286, 3402, 510, 294, 264, 1179, 295, 565, 300, 321, 11, 51814], "temperature": 0.0, "avg_logprob": -0.3551147335865458, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.10720236599445343}, {"id": 637, "seek": 254000, "start": 2540.0, "end": 2543.0, "text": " yeah, we are.", "tokens": [50364, 1338, 11, 321, 366, 13, 50514], "temperature": 0.0, "avg_logprob": -0.23484259033203125, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.03323669731616974}, {"id": 638, "seek": 254000, "start": 2543.0, "end": 2546.0, "text": " Okay.", "tokens": [50514, 1033, 13, 50664], "temperature": 0.0, "avg_logprob": -0.23484259033203125, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.03323669731616974}, {"id": 639, "seek": 254000, "start": 2546.0, "end": 2550.0, "text": " And you can see when it gets to this part, the agents are really thinking about how it's going to generate the testing.", "tokens": [50664, 400, 291, 393, 536, 562, 309, 2170, 281, 341, 644, 11, 264, 12554, 366, 534, 1953, 466, 577, 309, 311, 516, 281, 8460, 264, 4997, 13, 50864], "temperature": 0.0, "avg_logprob": -0.23484259033203125, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.03323669731616974}, {"id": 640, "seek": 254000, "start": 2550.0, "end": 2555.0, "text": " So it's going to have functional testing and any sort of unit testing and stuff you might want to do for your code.", "tokens": [50864, 407, 309, 311, 516, 281, 362, 11745, 4997, 293, 604, 1333, 295, 4985, 4997, 293, 1507, 291, 1062, 528, 281, 360, 337, 428, 3089, 13, 51114], "temperature": 0.0, "avg_logprob": -0.23484259033203125, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.03323669731616974}, {"id": 641, "seek": 254000, "start": 2555.0, "end": 2558.0, "text": " It'll generate that in the next phase too.", "tokens": [51114, 467, 603, 8460, 300, 294, 264, 958, 5574, 886, 13, 51264], "temperature": 0.0, "avg_logprob": -0.23484259033203125, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.03323669731616974}, {"id": 642, "seek": 254000, "start": 2558.0, "end": 2561.0, "text": " But it's already thinking ahead how am I going to test this into end.", "tokens": [51264, 583, 309, 311, 1217, 1953, 2286, 577, 669, 286, 516, 281, 1500, 341, 666, 917, 13, 51414], "temperature": 0.0, "avg_logprob": -0.23484259033203125, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.03323669731616974}, {"id": 643, "seek": 254000, "start": 2561.0, "end": 2567.0, "text": " You know, this is quite a, you know, obviously the different I've said to it use five box and chrome.", "tokens": [51414, 509, 458, 11, 341, 307, 1596, 257, 11, 291, 458, 11, 2745, 264, 819, 286, 600, 848, 281, 309, 764, 1732, 2424, 293, 33120, 13, 51714], "temperature": 0.0, "avg_logprob": -0.23484259033203125, "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.03323669731616974}, {"id": 644, "seek": 256700, "start": 2567.0, "end": 2569.0, "text": " Because it's going to go on mobile phones.", "tokens": [50364, 1436, 309, 311, 516, 281, 352, 322, 6013, 10216, 13, 50464], "temperature": 0.0, "avg_logprob": -0.18166139390733507, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.007972386665642262}, {"id": 645, "seek": 256700, "start": 2569.0, "end": 2574.0, "text": " It's figured that some people are going to have iPhones and it's put Safari in there as well, which is great.", "tokens": [50464, 467, 311, 8932, 300, 512, 561, 366, 516, 281, 362, 43793, 293, 309, 311, 829, 43820, 294, 456, 382, 731, 11, 597, 307, 869, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18166139390733507, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.007972386665642262}, {"id": 646, "seek": 256700, "start": 2574.0, "end": 2576.0, "text": " Chrome mobile chrome desktop.", "tokens": [50714, 15327, 6013, 33120, 14502, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18166139390733507, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.007972386665642262}, {"id": 647, "seek": 256700, "start": 2576.0, "end": 2577.0, "text": " So yeah.", "tokens": [50814, 407, 1338, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18166139390733507, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.007972386665642262}, {"id": 648, "seek": 256700, "start": 2577.0, "end": 2582.0, "text": " So we'll go ahead and save that.", "tokens": [50864, 407, 321, 603, 352, 2286, 293, 3155, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18166139390733507, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.007972386665642262}, {"id": 649, "seek": 259700, "start": 2598.0, "end": 2604.0, "text": " Okay, now it's going to go ahead and create all the different files for us to actually build a project.", "tokens": [50414, 1033, 11, 586, 309, 311, 516, 281, 352, 2286, 293, 1884, 439, 264, 819, 7098, 337, 505, 281, 767, 1322, 257, 1716, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16201799710591633, "compression_ratio": 1.5992779783393503, "no_speech_prob": 0.04165852814912796}, {"id": 650, "seek": 259700, "start": 2604.0, "end": 2605.0, "text": " And there we go.", "tokens": [50714, 400, 456, 321, 352, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16201799710591633, "compression_ratio": 1.5992779783393503, "no_speech_prob": 0.04165852814912796}, {"id": 651, "seek": 259700, "start": 2605.0, "end": 2611.0, "text": " It's going to create the assets file, the CSS files, the JavaScript files, and hopefully we'll get it working again.", "tokens": [50764, 467, 311, 516, 281, 1884, 264, 9769, 3991, 11, 264, 24387, 7098, 11, 264, 15778, 7098, 11, 293, 4696, 321, 603, 483, 309, 1364, 797, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16201799710591633, "compression_ratio": 1.5992779783393503, "no_speech_prob": 0.04165852814912796}, {"id": 652, "seek": 259700, "start": 2611.0, "end": 2613.0, "text": " So this is where it takes a few minutes.", "tokens": [51064, 407, 341, 307, 689, 309, 2516, 257, 1326, 2077, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16201799710591633, "compression_ratio": 1.5992779783393503, "no_speech_prob": 0.04165852814912796}, {"id": 653, "seek": 259700, "start": 2613.0, "end": 2614.0, "text": " Five or seven minutes.", "tokens": [51164, 9436, 420, 3407, 2077, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16201799710591633, "compression_ratio": 1.5992779783393503, "no_speech_prob": 0.04165852814912796}, {"id": 654, "seek": 259700, "start": 2614.0, "end": 2615.0, "text": " It works.", "tokens": [51214, 467, 1985, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16201799710591633, "compression_ratio": 1.5992779783393503, "no_speech_prob": 0.04165852814912796}, {"id": 655, "seek": 259700, "start": 2615.0, "end": 2622.0, "text": " And we spent a lot of time on requirements, design, technology selection, not coding, right?", "tokens": [51264, 400, 321, 4418, 257, 688, 295, 565, 322, 7728, 11, 1715, 11, 2899, 9450, 11, 406, 17720, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.16201799710591633, "compression_ratio": 1.5992779783393503, "no_speech_prob": 0.04165852814912796}, {"id": 656, "seek": 259700, "start": 2622.0, "end": 2625.0, "text": " We've got 16 minutes left in the hour.", "tokens": [51614, 492, 600, 658, 3165, 2077, 1411, 294, 264, 1773, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16201799710591633, "compression_ratio": 1.5992779783393503, "no_speech_prob": 0.04165852814912796}, {"id": 657, "seek": 262500, "start": 2625.0, "end": 2628.0, "text": " We'll see how we get on.", "tokens": [50364, 492, 603, 536, 577, 321, 483, 322, 13, 50514], "temperature": 0.0, "avg_logprob": -0.18654312207860854, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.009173480793833733}, {"id": 658, "seek": 262500, "start": 2628.0, "end": 2635.0, "text": " But again, it's just sort of the almost the opposite of, of vibe coding and starting with build.", "tokens": [50514, 583, 797, 11, 309, 311, 445, 1333, 295, 264, 1920, 264, 6182, 295, 11, 295, 14606, 17720, 293, 2891, 365, 1322, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18654312207860854, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.009173480793833733}, {"id": 659, "seek": 262500, "start": 2635.0, "end": 2643.0, "text": " And this was really the, the thing that we wanted to impart is it's, it's time well spent doing this up front.", "tokens": [50864, 400, 341, 390, 534, 264, 11, 264, 551, 300, 321, 1415, 281, 32177, 307, 309, 311, 11, 309, 311, 565, 731, 4418, 884, 341, 493, 1868, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18654312207860854, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.009173480793833733}, {"id": 660, "seek": 262500, "start": 2643.0, "end": 2646.0, "text": " A ideal C is one framework.", "tokens": [51264, 316, 7157, 383, 307, 472, 8388, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18654312207860854, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.009173480793833733}, {"id": 661, "seek": 262500, "start": 2646.0, "end": 2649.0, "text": " Again, there are other approaches.", "tokens": [51414, 3764, 11, 456, 366, 661, 11587, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18654312207860854, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.009173480793833733}, {"id": 662, "seek": 262500, "start": 2649.0, "end": 2651.0, "text": " You could build something yourself.", "tokens": [51564, 509, 727, 1322, 746, 1803, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18654312207860854, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.009173480793833733}, {"id": 663, "seek": 262500, "start": 2651.0, "end": 2653.0, "text": " You might consider forking what we've got.", "tokens": [51664, 509, 1062, 1949, 337, 5092, 437, 321, 600, 658, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18654312207860854, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.009173480793833733}, {"id": 664, "seek": 265300, "start": 2653.0, "end": 2656.0, "text": " You might consider, in fact, send us a merger request.", "tokens": [50364, 509, 1062, 1949, 11, 294, 1186, 11, 2845, 505, 257, 48002, 5308, 13, 50514], "temperature": 0.0, "avg_logprob": -0.3218088150024414, "compression_ratio": 1.3643410852713178, "no_speech_prob": 0.049540117383003235}, {"id": 665, "seek": 265300, "start": 2656.0, "end": 2659.0, "text": " If you find a way to do to improve on the process.", "tokens": [50514, 759, 291, 915, 257, 636, 281, 360, 281, 3470, 322, 264, 1399, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3218088150024414, "compression_ratio": 1.3643410852713178, "no_speech_prob": 0.049540117383003235}, {"id": 666, "seek": 265300, "start": 2659.0, "end": 2664.0, "text": " But the idea is to have a plan going into the, into the process.", "tokens": [50664, 583, 264, 1558, 307, 281, 362, 257, 1393, 516, 666, 264, 11, 666, 264, 1399, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3218088150024414, "compression_ratio": 1.3643410852713178, "no_speech_prob": 0.049540117383003235}, {"id": 667, "seek": 265300, "start": 2664.0, "end": 2665.0, "text": " Yeah.", "tokens": [50914, 865, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3218088150024414, "compression_ratio": 1.3643410852713178, "no_speech_prob": 0.049540117383003235}, {"id": 668, "seek": 269500, "start": 2695.0, "end": 2705.0, "text": " To summarize the question, is this process.", "tokens": [50364, 1407, 20858, 264, 1168, 11, 307, 341, 1399, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21968201060353973, "compression_ratio": 1.4881516587677726, "no_speech_prob": 0.039511896669864655}, {"id": 669, "seek": 269500, "start": 2705.0, "end": 2709.0, "text": " It's been linear, right, where we do all these steps in order.", "tokens": [50864, 467, 311, 668, 8213, 11, 558, 11, 689, 321, 360, 439, 613, 4439, 294, 1668, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21968201060353973, "compression_ratio": 1.4881516587677726, "no_speech_prob": 0.039511896669864655}, {"id": 670, "seek": 269500, "start": 2709.0, "end": 2714.0, "text": " And in reality, in real life, we might start implementing phase one and say actually I changed my mind.", "tokens": [51064, 400, 294, 4103, 11, 294, 957, 993, 11, 321, 1062, 722, 18114, 5574, 472, 293, 584, 767, 286, 3105, 452, 1575, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21968201060353973, "compression_ratio": 1.4881516587677726, "no_speech_prob": 0.039511896669864655}, {"id": 671, "seek": 269500, "start": 2714.0, "end": 2716.0, "text": " I want to change tweeter requirements.", "tokens": [51314, 286, 528, 281, 1319, 6986, 2398, 7728, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21968201060353973, "compression_ratio": 1.4881516587677726, "no_speech_prob": 0.039511896669864655}, {"id": 672, "seek": 269500, "start": 2716.0, "end": 2722.0, "text": " I think you certainly can go back to one of the previous stages.", "tokens": [51414, 286, 519, 291, 3297, 393, 352, 646, 281, 472, 295, 264, 3894, 10232, 13, 51714], "temperature": 0.0, "avg_logprob": -0.21968201060353973, "compression_ratio": 1.4881516587677726, "no_speech_prob": 0.039511896669864655}, {"id": 673, "seek": 272200, "start": 2722.0, "end": 2728.0, "text": " Edit the requirements and then ask the tool to update the design for example.", "tokens": [50364, 33241, 264, 7728, 293, 550, 1029, 264, 2290, 281, 5623, 264, 1715, 337, 1365, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12145695596371058, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.08091079443693161}, {"id": 674, "seek": 272200, "start": 2728.0, "end": 2732.0, "text": " I don't think I have any, any real shortcuts for that.", "tokens": [50664, 286, 500, 380, 519, 286, 362, 604, 11, 604, 957, 34620, 337, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12145695596371058, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.08091079443693161}, {"id": 675, "seek": 272200, "start": 2732.0, "end": 2736.0, "text": " I think we've got to pay the price at some point.", "tokens": [50864, 286, 519, 321, 600, 658, 281, 1689, 264, 3218, 412, 512, 935, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12145695596371058, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.08091079443693161}, {"id": 676, "seek": 272200, "start": 2736.0, "end": 2740.0, "text": " If we, if we want to change the requirements, we're going to have to go through those steps again.", "tokens": [51064, 759, 321, 11, 498, 321, 528, 281, 1319, 264, 7728, 11, 321, 434, 516, 281, 362, 281, 352, 807, 729, 4439, 797, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12145695596371058, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.08091079443693161}, {"id": 677, "seek": 272200, "start": 2740.0, "end": 2745.0, "text": " I think that the, by getting the right people in the room up front.", "tokens": [51264, 286, 519, 300, 264, 11, 538, 1242, 264, 558, 561, 294, 264, 1808, 493, 1868, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12145695596371058, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.08091079443693161}, {"id": 678, "seek": 272200, "start": 2745.0, "end": 2748.0, "text": " We can hopefully minimize that, that back and forth,", "tokens": [51514, 492, 393, 4696, 17522, 300, 11, 300, 646, 293, 5220, 11, 51664], "temperature": 0.0, "avg_logprob": -0.12145695596371058, "compression_ratio": 1.6820083682008369, "no_speech_prob": 0.08091079443693161}, {"id": 679, "seek": 274800, "start": 2748.0, "end": 2750.0, "text": " in terms of what the requirements should be.", "tokens": [50364, 294, 2115, 295, 437, 264, 7728, 820, 312, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15097225353281984, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.04202231392264366}, {"id": 680, "seek": 274800, "start": 2750.0, "end": 2756.0, "text": " In terms of iterating on the design and the implementation.", "tokens": [50464, 682, 2115, 295, 17138, 990, 322, 264, 1715, 293, 264, 11420, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15097225353281984, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.04202231392264366}, {"id": 681, "seek": 274800, "start": 2756.0, "end": 2760.0, "text": " One of the benefits is we can move pretty quickly here.", "tokens": [50764, 1485, 295, 264, 5311, 307, 321, 393, 1286, 1238, 2661, 510, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15097225353281984, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.04202231392264366}, {"id": 682, "seek": 274800, "start": 2760.0, "end": 2769.0, "text": " And so the iteration step, whereas in previous lives, that might be like a whole sprint that we just used to build something.", "tokens": [50964, 400, 370, 264, 24784, 1823, 11, 9735, 294, 3894, 2909, 11, 300, 1062, 312, 411, 257, 1379, 25075, 300, 321, 445, 1143, 281, 1322, 746, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15097225353281984, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.04202231392264366}, {"id": 683, "seek": 274800, "start": 2769.0, "end": 2771.0, "text": " We've got to start over again.", "tokens": [51414, 492, 600, 658, 281, 722, 670, 797, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15097225353281984, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.04202231392264366}, {"id": 684, "seek": 274800, "start": 2771.0, "end": 2775.0, "text": " We could potentially make that cycle time quite a bit shorter.", "tokens": [51514, 492, 727, 7263, 652, 300, 6586, 565, 1596, 257, 857, 11639, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15097225353281984, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.04202231392264366}, {"id": 685, "seek": 277500, "start": 2775.0, "end": 2780.0, "text": " But I don't have a way around, you know, needing to go back and, and.", "tokens": [50364, 583, 286, 500, 380, 362, 257, 636, 926, 11, 291, 458, 11, 18006, 281, 352, 646, 293, 11, 293, 13, 50614], "temperature": 0.0, "avg_logprob": -0.5674659158581885, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.24016505479812622}, {"id": 686, "seek": 277500, "start": 2780.0, "end": 2782.0, "text": " I know it's trying to go back.", "tokens": [50614, 286, 458, 309, 311, 1382, 281, 352, 646, 13, 50714], "temperature": 0.0, "avg_logprob": -0.5674659158581885, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.24016505479812622}, {"id": 687, "seek": 277500, "start": 2782.0, "end": 2794.0, "text": " So I thought that like, if you really think of this game, document what you did, that I'll look at the change in our mind.", "tokens": [50714, 407, 286, 1194, 300, 411, 11, 498, 291, 534, 519, 295, 341, 1216, 11, 4166, 437, 291, 630, 11, 300, 286, 603, 574, 412, 264, 1319, 294, 527, 1575, 13, 51314], "temperature": 0.0, "avg_logprob": -0.5674659158581885, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.24016505479812622}, {"id": 688, "seek": 277500, "start": 2794.0, "end": 2798.0, "text": " And then, like, we're looking at the, you know, specific, self-harmonic changes, specific changes.", "tokens": [51314, 400, 550, 11, 411, 11, 321, 434, 1237, 412, 264, 11, 291, 458, 11, 2685, 11, 2698, 12, 5854, 3317, 299, 2962, 11, 2685, 2962, 13, 51514], "temperature": 0.0, "avg_logprob": -0.5674659158581885, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.24016505479812622}, {"id": 689, "seek": 277500, "start": 2798.0, "end": 2801.0, "text": " So it's like, like a cycle process.", "tokens": [51514, 407, 309, 311, 411, 11, 411, 257, 6586, 1399, 13, 51664], "temperature": 0.0, "avg_logprob": -0.5674659158581885, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.24016505479812622}, {"id": 690, "seek": 280100, "start": 2801.0, "end": 2802.0, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.21436513359866924, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.07565230876207352}, {"id": 691, "seek": 280100, "start": 2802.0, "end": 2814.0, "text": " So just to repeat the question, the question is about.", "tokens": [50414, 407, 445, 281, 7149, 264, 1168, 11, 264, 1168, 307, 466, 13, 51014], "temperature": 0.0, "avg_logprob": -0.21436513359866924, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.07565230876207352}, {"id": 692, "seek": 280100, "start": 2814.0, "end": 2823.0, "text": " Being able to save the context and go back and edit it later and be able to iterate based on what we've learned so far.", "tokens": [51014, 8891, 1075, 281, 3155, 264, 4319, 293, 352, 646, 293, 8129, 309, 1780, 293, 312, 1075, 281, 44497, 2361, 322, 437, 321, 600, 3264, 370, 1400, 13, 51464], "temperature": 0.0, "avg_logprob": -0.21436513359866924, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.07565230876207352}, {"id": 693, "seek": 280100, "start": 2823.0, "end": 2830.0, "text": " So we're writing all of the design decisions into, into Markdown files.", "tokens": [51464, 407, 321, 434, 3579, 439, 295, 264, 1715, 5327, 666, 11, 666, 3934, 5093, 7098, 13, 51814], "temperature": 0.0, "avg_logprob": -0.21436513359866924, "compression_ratio": 1.5089820359281436, "no_speech_prob": 0.07565230876207352}, {"id": 694, "seek": 283000, "start": 2830.0, "end": 2837.0, "text": " All of the Q&A and all of the descriptions of the requirements and design and such.", "tokens": [50364, 1057, 295, 264, 1249, 5, 32, 293, 439, 295, 264, 24406, 295, 264, 7728, 293, 1715, 293, 1270, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11589180981671368, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.07553917169570923}, {"id": 695, "seek": 283000, "start": 2837.0, "end": 2843.0, "text": " There's also an audit log that automatically gets written of all the questions and decisions that were made.", "tokens": [50714, 821, 311, 611, 364, 17748, 3565, 300, 6772, 2170, 3720, 295, 439, 264, 1651, 293, 5327, 300, 645, 1027, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11589180981671368, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.07553917169570923}, {"id": 696, "seek": 283000, "start": 2843.0, "end": 2848.0, "text": " One thing that we've seen is just taking all of these files and attaching them back into the jurisdictions.", "tokens": [51014, 1485, 551, 300, 321, 600, 1612, 307, 445, 1940, 439, 295, 613, 7098, 293, 39074, 552, 646, 666, 264, 37958, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11589180981671368, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.07553917169570923}, {"id": 697, "seek": 283000, "start": 2848.0, "end": 2858.0, "text": " And so anybody that wants to go back and understand why do we make this decision or, you know, what was used to, as input here, we can go back and reproduce it.", "tokens": [51264, 400, 370, 4472, 300, 2738, 281, 352, 646, 293, 1223, 983, 360, 321, 652, 341, 3537, 420, 11, 291, 458, 11, 437, 390, 1143, 281, 11, 382, 4846, 510, 11, 321, 393, 352, 646, 293, 29501, 309, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11589180981671368, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.07553917169570923}, {"id": 698, "seek": 285800, "start": 2858.0, "end": 2862.0, "text": " We've also seen, you know, the context window can get full.", "tokens": [50364, 492, 600, 611, 1612, 11, 291, 458, 11, 264, 4319, 4910, 393, 483, 1577, 13, 50564], "temperature": 0.0, "avg_logprob": -0.125776160847057, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.032486703246831894}, {"id": 699, "seek": 285800, "start": 2862.0, "end": 2868.0, "text": " And so in between each of these steps, we've actually cleared the context and then started fresh again.", "tokens": [50564, 400, 370, 294, 1296, 1184, 295, 613, 4439, 11, 321, 600, 767, 19725, 264, 4319, 293, 550, 1409, 4451, 797, 13, 50864], "temperature": 0.0, "avg_logprob": -0.125776160847057, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.032486703246831894}, {"id": 700, "seek": 285800, "start": 2868.0, "end": 2876.0, "text": " And that kind of proves to ourselves that everything that's needed to reproduce what we've got is stored in these Markdown files.", "tokens": [50864, 400, 300, 733, 295, 25019, 281, 4175, 300, 1203, 300, 311, 2978, 281, 29501, 437, 321, 600, 658, 307, 12187, 294, 613, 3934, 5093, 7098, 13, 51264], "temperature": 0.0, "avg_logprob": -0.125776160847057, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.032486703246831894}, {"id": 701, "seek": 285800, "start": 2876.0, "end": 2880.0, "text": " Yep.", "tokens": [51264, 7010, 13, 51464], "temperature": 0.0, "avg_logprob": -0.125776160847057, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.032486703246831894}, {"id": 702, "seek": 285800, "start": 2880.0, "end": 2882.0, "text": " And that's in our custom agent.", "tokens": [51464, 400, 300, 311, 294, 527, 2375, 9461, 13, 51564], "temperature": 0.0, "avg_logprob": -0.125776160847057, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.032486703246831894}, {"id": 703, "seek": 285800, "start": 2882.0, "end": 2883.0, "text": " Yep.", "tokens": [51564, 7010, 13, 51614], "temperature": 0.0, "avg_logprob": -0.125776160847057, "compression_ratio": 1.6105769230769231, "no_speech_prob": 0.032486703246831894}, {"id": 704, "seek": 288300, "start": 2883.0, "end": 2887.0, "text": " And so in theory, we should be able to go back to any step in any time.", "tokens": [50364, 400, 370, 294, 5261, 11, 321, 820, 312, 1075, 281, 352, 646, 281, 604, 1823, 294, 604, 565, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3526270572955792, "compression_ratio": 1.5823293172690762, "no_speech_prob": 0.026825644075870514}, {"id": 705, "seek": 288300, "start": 2887.0, "end": 2889.0, "text": " Use one of these as artifacts.", "tokens": [50564, 8278, 472, 295, 613, 382, 24617, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3526270572955792, "compression_ratio": 1.5823293172690762, "no_speech_prob": 0.026825644075870514}, {"id": 706, "seek": 288300, "start": 2889.0, "end": 2893.0, "text": " Make a tweak and move forward from there.", "tokens": [50664, 4387, 257, 29879, 293, 1286, 2128, 490, 456, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3526270572955792, "compression_ratio": 1.5823293172690762, "no_speech_prob": 0.026825644075870514}, {"id": 707, "seek": 288300, "start": 2893.0, "end": 2899.0, "text": " How does this take place for us using the power of our IDV?", "tokens": [50864, 1012, 775, 341, 747, 1081, 337, 505, 1228, 264, 1347, 295, 527, 7348, 53, 30, 51164], "temperature": 0.0, "avg_logprob": -0.3526270572955792, "compression_ratio": 1.5823293172690762, "no_speech_prob": 0.026825644075870514}, {"id": 708, "seek": 288300, "start": 2899.0, "end": 2906.0, "text": " Because I use it and it follows very similar phases where we have the design requirements and tasks.", "tokens": [51164, 1436, 286, 764, 309, 293, 309, 10002, 588, 2531, 18764, 689, 321, 362, 264, 1715, 7728, 293, 9608, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3526270572955792, "compression_ratio": 1.5823293172690762, "no_speech_prob": 0.026825644075870514}, {"id": 709, "seek": 288300, "start": 2906.0, "end": 2909.0, "text": " And here you have the analysis design.", "tokens": [51514, 400, 510, 291, 362, 264, 5215, 1715, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3526270572955792, "compression_ratio": 1.5823293172690762, "no_speech_prob": 0.026825644075870514}, {"id": 710, "seek": 288300, "start": 2909.0, "end": 2912.0, "text": " Do you think one is more powerful than the other?", "tokens": [51664, 1144, 291, 519, 472, 307, 544, 4005, 813, 264, 661, 30, 51814], "temperature": 0.0, "avg_logprob": -0.3526270572955792, "compression_ratio": 1.5823293172690762, "no_speech_prob": 0.026825644075870514}, {"id": 711, "seek": 291200, "start": 2912.0, "end": 2916.0, "text": " Or depending like if we use it, you would dig your own best intelligence.", "tokens": [50364, 1610, 5413, 411, 498, 321, 764, 309, 11, 291, 576, 2528, 428, 1065, 1151, 7599, 13, 50564], "temperature": 0.0, "avg_logprob": -0.34541893005371094, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.005961346440017223}, {"id": 712, "seek": 291200, "start": 2916.0, "end": 2920.0, "text": " And in that case, we would first be more powerful than the spec.", "tokens": [50564, 400, 294, 300, 1389, 11, 321, 576, 700, 312, 544, 4005, 813, 264, 1608, 13, 50764], "temperature": 0.0, "avg_logprob": -0.34541893005371094, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.005961346440017223}, {"id": 713, "seek": 291200, "start": 2920.0, "end": 2926.0, "text": " The question was, which is sort of the better approach.", "tokens": [50764, 440, 1168, 390, 11, 597, 307, 1333, 295, 264, 1101, 3109, 13, 51064], "temperature": 0.0, "avg_logprob": -0.34541893005371094, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.005961346440017223}, {"id": 714, "seek": 291200, "start": 2926.0, "end": 2928.0, "text": " Care ID has spectrum and development.", "tokens": [51064, 9532, 7348, 575, 11143, 293, 3250, 13, 51164], "temperature": 0.0, "avg_logprob": -0.34541893005371094, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.005961346440017223}, {"id": 715, "seek": 291200, "start": 2928.0, "end": 2932.0, "text": " We're doing this ADLC workflow in the CLI.", "tokens": [51164, 492, 434, 884, 341, 9135, 14766, 20993, 294, 264, 12855, 40, 13, 51364], "temperature": 0.0, "avg_logprob": -0.34541893005371094, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.005961346440017223}, {"id": 716, "seek": 291200, "start": 2932.0, "end": 2936.0, "text": " They have similar bits of functionality.", "tokens": [51364, 814, 362, 2531, 9239, 295, 14980, 13, 51564], "temperature": 0.0, "avg_logprob": -0.34541893005371094, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.005961346440017223}, {"id": 717, "seek": 291200, "start": 2936.0, "end": 2940.0, "text": " We're moving fast in all of this.", "tokens": [51564, 492, 434, 2684, 2370, 294, 439, 295, 341, 13, 51764], "temperature": 0.0, "avg_logprob": -0.34541893005371094, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.005961346440017223}, {"id": 718, "seek": 294000, "start": 2940.0, "end": 2948.0, "text": " The IDE, I would say, spectrum and development is less customizable at this point.", "tokens": [50364, 440, 40930, 11, 286, 576, 584, 11, 11143, 293, 3250, 307, 1570, 47922, 412, 341, 935, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15010310192497409, "compression_ratio": 1.6008064516129032, "no_speech_prob": 0.012010375037789345}, {"id": 719, "seek": 294000, "start": 2948.0, "end": 2950.0, "text": " But you're going to get what you get.", "tokens": [50764, 583, 291, 434, 516, 281, 483, 437, 291, 483, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15010310192497409, "compression_ratio": 1.6008064516129032, "no_speech_prob": 0.012010375037789345}, {"id": 720, "seek": 294000, "start": 2950.0, "end": 2954.0, "text": " And for a lot of customers, they really like it and they're using it.", "tokens": [50864, 400, 337, 257, 688, 295, 4581, 11, 436, 534, 411, 309, 293, 436, 434, 1228, 309, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15010310192497409, "compression_ratio": 1.6008064516129032, "no_speech_prob": 0.012010375037789345}, {"id": 721, "seek": 294000, "start": 2954.0, "end": 2956.0, "text": " This is much more configurable.", "tokens": [51064, 639, 307, 709, 544, 22192, 712, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15010310192497409, "compression_ratio": 1.6008064516129032, "no_speech_prob": 0.012010375037789345}, {"id": 722, "seek": 294000, "start": 2956.0, "end": 2964.0, "text": " The ADLC approach or an approach like this gives you the ability to tailor this process with your developer.", "tokens": [51164, 440, 9135, 14766, 3109, 420, 364, 3109, 411, 341, 2709, 291, 264, 3485, 281, 33068, 341, 1399, 365, 428, 10754, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15010310192497409, "compression_ratio": 1.6008064516129032, "no_speech_prob": 0.012010375037789345}, {"id": 723, "seek": 294000, "start": 2964.0, "end": 2968.0, "text": " Development teams over time and make it as granular and specific.", "tokens": [51564, 15041, 5491, 670, 565, 293, 652, 309, 382, 39962, 293, 2685, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15010310192497409, "compression_ratio": 1.6008064516129032, "no_speech_prob": 0.012010375037789345}, {"id": 724, "seek": 296800, "start": 2968.0, "end": 2972.0, "text": " Always do it this way, don't do it that way as you want.", "tokens": [50364, 11270, 360, 309, 341, 636, 11, 500, 380, 360, 309, 300, 636, 382, 291, 528, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11475254240490142, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.016634272411465645}, {"id": 725, "seek": 296800, "start": 2972.0, "end": 2976.0, "text": " In return, the process is more complicated.", "tokens": [50564, 682, 2736, 11, 264, 1399, 307, 544, 6179, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11475254240490142, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.016634272411465645}, {"id": 726, "seek": 296800, "start": 2976.0, "end": 2978.0, "text": " It's going to take more time.", "tokens": [50764, 467, 311, 516, 281, 747, 544, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11475254240490142, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.016634272411465645}, {"id": 727, "seek": 296800, "start": 2978.0, "end": 2980.0, "text": " We're skipping a bunch of steps.", "tokens": [50864, 492, 434, 31533, 257, 3840, 295, 4439, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11475254240490142, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.016634272411465645}, {"id": 728, "seek": 296800, "start": 2980.0, "end": 2984.0, "text": " If we were going through the full thing and we hadn't told it, we were doing a demo.", "tokens": [50964, 759, 321, 645, 516, 807, 264, 1577, 551, 293, 321, 8782, 380, 1907, 309, 11, 321, 645, 884, 257, 10723, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11475254240490142, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.016634272411465645}, {"id": 729, "seek": 296800, "start": 2984.0, "end": 2989.0, "text": " It would be documenting tons of user stories and asking what personas should we be doing.", "tokens": [51164, 467, 576, 312, 42360, 9131, 295, 4195, 3676, 293, 3365, 437, 12019, 820, 321, 312, 884, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11475254240490142, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.016634272411465645}, {"id": 730, "seek": 296800, "start": 2989.0, "end": 2991.0, "text": " So it'll go a lot deeper than this.", "tokens": [51414, 407, 309, 603, 352, 257, 688, 7731, 813, 341, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11475254240490142, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.016634272411465645}, {"id": 731, "seek": 299100, "start": 2991.0, "end": 3004.0, "text": " And so I would suggest that maybe as a starting point or for features that are a little bit more.", "tokens": [50364, 400, 370, 286, 576, 3402, 300, 1310, 382, 257, 2891, 935, 420, 337, 4122, 300, 366, 257, 707, 857, 544, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15539371645128405, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.06744106858968735}, {"id": 732, "seek": 299100, "start": 3004.0, "end": 3010.0, "text": " I don't want to say straight forward, but I would say a clear IDE is a good starting point.", "tokens": [51014, 286, 500, 380, 528, 281, 584, 2997, 2128, 11, 457, 286, 576, 584, 257, 1850, 40930, 307, 257, 665, 2891, 935, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15539371645128405, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.06744106858968735}, {"id": 733, "seek": 299100, "start": 3010.0, "end": 3019.0, "text": " But if you're looking for more ability to customize or the ability to get a few levels deeper into the requirements,", "tokens": [51314, 583, 498, 291, 434, 1237, 337, 544, 3485, 281, 19734, 420, 264, 3485, 281, 483, 257, 1326, 4358, 7731, 666, 264, 7728, 11, 51764], "temperature": 0.0, "avg_logprob": -0.15539371645128405, "compression_ratio": 1.5612244897959184, "no_speech_prob": 0.06744106858968735}, {"id": 734, "seek": 301900, "start": 3019.0, "end": 3021.0, "text": " I would suggest that this is a good approach.", "tokens": [50364, 286, 576, 3402, 300, 341, 307, 257, 665, 3109, 13, 50464], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 735, "seek": 301900, "start": 3021.0, "end": 3023.0, "text": " But I also think that you'll see you over time.", "tokens": [50464, 583, 286, 611, 519, 300, 291, 603, 536, 291, 670, 565, 13, 50564], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 736, "seek": 301900, "start": 3023.0, "end": 3027.0, "text": " We'll continue to build out that spectrum and workflow in the IDE.", "tokens": [50564, 492, 603, 2354, 281, 1322, 484, 300, 11143, 293, 20993, 294, 264, 40930, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 737, "seek": 301900, "start": 3027.0, "end": 3031.0, "text": " And that'll get more and more features as well.", "tokens": [50764, 400, 300, 603, 483, 544, 293, 544, 4122, 382, 731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 738, "seek": 301900, "start": 3031.0, "end": 3032.0, "text": " How are we doing here?", "tokens": [50964, 1012, 366, 321, 884, 510, 30, 51014], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 739, "seek": 301900, "start": 3032.0, "end": 3033.0, "text": " I built quite quickly.", "tokens": [51014, 286, 3094, 1596, 2661, 13, 51064], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 740, "seek": 301900, "start": 3033.0, "end": 3035.0, "text": " There was no contention on Bidrock.", "tokens": [51064, 821, 390, 572, 660, 1251, 322, 363, 327, 17799, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 741, "seek": 301900, "start": 3035.0, "end": 3037.0, "text": " I think we're good there.", "tokens": [51164, 286, 519, 321, 434, 665, 456, 13, 51264], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 742, "seek": 301900, "start": 3037.0, "end": 3038.0, "text": " I might have to see if it works.", "tokens": [51264, 286, 1062, 362, 281, 536, 498, 309, 1985, 13, 51314], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 743, "seek": 301900, "start": 3038.0, "end": 3039.0, "text": " Yeah, well that's.", "tokens": [51314, 865, 11, 731, 300, 311, 13, 51364], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 744, "seek": 301900, "start": 3039.0, "end": 3043.0, "text": " So it did build us a implementation plan.", "tokens": [51364, 407, 309, 630, 1322, 505, 257, 11420, 1393, 13, 51564], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 745, "seek": 301900, "start": 3043.0, "end": 3046.0, "text": " And then let me close that down and bring up the other ones.", "tokens": [51564, 400, 550, 718, 385, 1998, 300, 760, 293, 1565, 493, 264, 661, 2306, 13, 51714], "temperature": 0.0, "avg_logprob": -0.23847920259983418, "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.07507570087909698}, {"id": 746, "seek": 304600, "start": 3047.0, "end": 3049.0, "text": " You know, all the coding was complete.", "tokens": [50414, 509, 458, 11, 439, 264, 17720, 390, 3566, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 747, "seek": 304600, "start": 3049.0, "end": 3052.0, "text": " You get a full report of everything that was done.", "tokens": [50514, 509, 483, 257, 1577, 2275, 295, 1203, 300, 390, 1096, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 748, "seek": 304600, "start": 3052.0, "end": 3054.0, "text": " How much code was written.", "tokens": [50664, 1012, 709, 3089, 390, 3720, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 749, "seek": 304600, "start": 3054.0, "end": 3056.0, "text": " All the features that were implemented.", "tokens": [50764, 1057, 264, 4122, 300, 645, 12270, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 750, "seek": 304600, "start": 3056.0, "end": 3058.0, "text": " The stuff that we walked through in the beginning.", "tokens": [50864, 440, 1507, 300, 321, 7628, 807, 294, 264, 2863, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 751, "seek": 304600, "start": 3058.0, "end": 3061.0, "text": " And yeah, tells you how to test it.", "tokens": [50964, 400, 1338, 11, 5112, 291, 577, 281, 1500, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 752, "seek": 304600, "start": 3061.0, "end": 3064.0, "text": " And it also gives you a whole testing plan as well.", "tokens": [51114, 400, 309, 611, 2709, 291, 257, 1379, 4997, 1393, 382, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 753, "seek": 304600, "start": 3064.0, "end": 3066.0, "text": " This is quite a basic one, obviously.", "tokens": [51264, 639, 307, 1596, 257, 3875, 472, 11, 2745, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 754, "seek": 304600, "start": 3066.0, "end": 3070.0, "text": " But you can obviously imagine what it will produce when you do a proper document.", "tokens": [51364, 583, 291, 393, 2745, 3811, 437, 309, 486, 5258, 562, 291, 360, 257, 2296, 4166, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 755, "seek": 304600, "start": 3070.0, "end": 3073.0, "text": " So yeah, with that, let's see what happens.", "tokens": [51564, 407, 1338, 11, 365, 300, 11, 718, 311, 536, 437, 2314, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14117199374783423, "compression_ratio": 1.6570397111913358, "no_speech_prob": 0.25099560618400574}, {"id": 756, "seek": 307300, "start": 3073.0, "end": 3075.0, "text": " I think it's just this file here.", "tokens": [50364, 286, 519, 309, 311, 445, 341, 3991, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 757, "seek": 307300, "start": 3075.0, "end": 3076.0, "text": " We can go.", "tokens": [50464, 492, 393, 352, 13, 50514], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 758, "seek": 307300, "start": 3076.0, "end": 3077.0, "text": " So there we go.", "tokens": [50514, 407, 456, 321, 352, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 759, "seek": 307300, "start": 3077.0, "end": 3079.0, "text": " We got the start screen.", "tokens": [50564, 492, 658, 264, 722, 2568, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 760, "seek": 307300, "start": 3079.0, "end": 3081.0, "text": " Look, they're not so far.", "tokens": [50664, 2053, 11, 436, 434, 406, 370, 1400, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 761, "seek": 307300, "start": 3081.0, "end": 3083.0, "text": " This is the worst part.", "tokens": [50764, 639, 307, 264, 5855, 644, 13, 50864], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 762, "seek": 307300, "start": 3083.0, "end": 3085.0, "text": " Easy.", "tokens": [50864, 16002, 13, 50964], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 763, "seek": 307300, "start": 3085.0, "end": 3086.0, "text": " There we go.", "tokens": [50964, 821, 321, 352, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 764, "seek": 307300, "start": 3086.0, "end": 3087.0, "text": " I think it wasn't too bad.", "tokens": [51014, 286, 519, 309, 2067, 380, 886, 1578, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 765, "seek": 307300, "start": 3087.0, "end": 3088.0, "text": " Okay, nice.", "tokens": [51064, 1033, 11, 1481, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 766, "seek": 307300, "start": 3088.0, "end": 3090.0, "text": " The bedrock thing that wasn't as impressive as I thought.", "tokens": [51114, 440, 2901, 17799, 551, 300, 2067, 380, 382, 8992, 382, 286, 1194, 13, 51214], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 767, "seek": 307300, "start": 3090.0, "end": 3092.0, "text": " I think that's a pack man goes there.", "tokens": [51214, 286, 519, 300, 311, 257, 2844, 587, 1709, 456, 13, 51314], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 768, "seek": 307300, "start": 3092.0, "end": 3094.0, "text": " Yeah, it doesn't actually look.", "tokens": [51314, 865, 11, 309, 1177, 380, 767, 574, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 769, "seek": 307300, "start": 3094.0, "end": 3096.0, "text": " But it's picking up scores.", "tokens": [51414, 583, 309, 311, 8867, 493, 13444, 13, 51514], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 770, "seek": 307300, "start": 3096.0, "end": 3099.0, "text": " I don't know if there's any sound on that.", "tokens": [51514, 286, 500, 380, 458, 498, 456, 311, 604, 1626, 322, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 771, "seek": 307300, "start": 3099.0, "end": 3101.0, "text": " I think we said no sound.", "tokens": [51664, 286, 519, 321, 848, 572, 1626, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22994055715547937, "compression_ratio": 1.672, "no_speech_prob": 0.05229249224066734}, {"id": 772, "seek": 310100, "start": 3101.0, "end": 3105.0, "text": " But they should have been, I can unplug my mic in a sec.", "tokens": [50364, 583, 436, 820, 362, 668, 11, 286, 393, 39456, 452, 3123, 294, 257, 907, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 773, "seek": 310100, "start": 3105.0, "end": 3106.0, "text": " Oh, no.", "tokens": [50564, 876, 11, 572, 13, 50614], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 774, "seek": 310100, "start": 3106.0, "end": 3107.0, "text": " I mean, when we set up the stage.", "tokens": [50614, 286, 914, 11, 562, 321, 992, 493, 264, 3233, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 775, "seek": 310100, "start": 3107.0, "end": 3108.0, "text": " Yeah.", "tokens": [50664, 865, 13, 50714], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 776, "seek": 310100, "start": 3108.0, "end": 3109.0, "text": " That was our mistake.", "tokens": [50714, 663, 390, 527, 6146, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 777, "seek": 310100, "start": 3109.0, "end": 3110.0, "text": " That's actually better than I thought.", "tokens": [50764, 663, 311, 767, 1101, 813, 286, 1194, 13, 50814], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 778, "seek": 310100, "start": 3110.0, "end": 3111.0, "text": " Yeah.", "tokens": [50814, 865, 13, 50864], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 779, "seek": 310100, "start": 3111.0, "end": 3112.0, "text": " A lot of them just didn't work.", "tokens": [50864, 316, 688, 295, 552, 445, 994, 380, 589, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 780, "seek": 310100, "start": 3112.0, "end": 3114.0, "text": " It just sunk like a stone.", "tokens": [50914, 467, 445, 40564, 411, 257, 7581, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 781, "seek": 310100, "start": 3114.0, "end": 3115.0, "text": " Cool.", "tokens": [51014, 8561, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 782, "seek": 310100, "start": 3115.0, "end": 3116.0, "text": " Nice.", "tokens": [51064, 5490, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 783, "seek": 310100, "start": 3116.0, "end": 3118.0, "text": " We've got eight minutes.", "tokens": [51114, 492, 600, 658, 3180, 2077, 13, 51214], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 784, "seek": 310100, "start": 3118.0, "end": 3121.0, "text": " Happy to take more questions and discussion about the process.", "tokens": [51214, 8277, 281, 747, 544, 1651, 293, 5017, 466, 264, 1399, 13, 51364], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 785, "seek": 310100, "start": 3121.0, "end": 3125.0, "text": " Also, I'll be to take a modification request to the game if we want to try that.", "tokens": [51364, 2743, 11, 286, 603, 312, 281, 747, 257, 26747, 5308, 281, 264, 1216, 498, 321, 528, 281, 853, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 786, "seek": 310100, "start": 3125.0, "end": 3127.0, "text": " Yeah.", "tokens": [51564, 865, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 787, "seek": 310100, "start": 3127.0, "end": 3129.0, "text": " Yes.", "tokens": [51664, 1079, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22063826161918912, "compression_ratio": 1.5629629629629629, "no_speech_prob": 0.024617895483970642}, {"id": 788, "seek": 312900, "start": 3129.0, "end": 3133.0, "text": " I will put that up right at the end here.", "tokens": [50364, 286, 486, 829, 300, 493, 558, 412, 264, 917, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2216128394717262, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006584226153790951}, {"id": 789, "seek": 312900, "start": 3133.0, "end": 3140.0, "text": " I'll do some crowd work here.", "tokens": [50564, 286, 603, 360, 512, 6919, 589, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2216128394717262, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006584226153790951}, {"id": 790, "seek": 312900, "start": 3140.0, "end": 3145.0, "text": " So our team recently did a test where we didn't full-care and just had it kind of go crazy", "tokens": [50914, 407, 527, 1469, 3938, 630, 257, 1500, 689, 321, 994, 380, 1577, 12, 5685, 293, 445, 632, 309, 733, 295, 352, 3219, 51164], "temperature": 0.0, "avg_logprob": -0.2216128394717262, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006584226153790951}, {"id": 791, "seek": 312900, "start": 3145.0, "end": 3147.0, "text": " and make a whole web app.", "tokens": [51164, 293, 652, 257, 1379, 3670, 724, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2216128394717262, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006584226153790951}, {"id": 792, "seek": 312900, "start": 3147.0, "end": 3150.0, "text": " My issue with that was that a lot of it was like an accessible.", "tokens": [51264, 1222, 2734, 365, 300, 390, 300, 257, 688, 295, 309, 390, 411, 364, 9515, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2216128394717262, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006584226153790951}, {"id": 793, "seek": 312900, "start": 3150.0, "end": 3153.0, "text": " And like had like there was like window to open things and things like that.", "tokens": [51414, 400, 411, 632, 411, 456, 390, 411, 4910, 281, 1269, 721, 293, 721, 411, 300, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2216128394717262, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006584226153790951}, {"id": 794, "seek": 312900, "start": 3153.0, "end": 3155.0, "text": " And I'm just wondering what recommendations you have.", "tokens": [51564, 400, 286, 478, 445, 6359, 437, 10434, 291, 362, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2216128394717262, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.006584226153790951}, {"id": 795, "seek": 315500, "start": 3155.0, "end": 3159.0, "text": " Like, is it like adding a front end MCP to the front of it?", "tokens": [50364, 1743, 11, 307, 309, 411, 5127, 257, 1868, 917, 8797, 47, 281, 264, 1868, 295, 309, 30, 50564], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 796, "seek": 315500, "start": 3159.0, "end": 3162.0, "text": " Like, what would you recommend to help make sure it kind of followed.", "tokens": [50564, 1743, 11, 437, 576, 291, 2748, 281, 854, 652, 988, 309, 733, 295, 6263, 13, 50714], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 797, "seek": 315500, "start": 3162.0, "end": 3164.0, "text": " We'll connect guidelines.", "tokens": [50714, 492, 603, 1745, 12470, 13, 50814], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 798, "seek": 315500, "start": 3164.0, "end": 3166.0, "text": " Excuse me about accessibility like A11.", "tokens": [50814, 11359, 385, 466, 15002, 411, 316, 5348, 13, 50914], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 799, "seek": 315500, "start": 3166.0, "end": 3167.0, "text": " Yeah.", "tokens": [50914, 865, 13, 50964], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 800, "seek": 315500, "start": 3167.0, "end": 3168.0, "text": " Okay.", "tokens": [50964, 1033, 13, 51014], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 801, "seek": 315500, "start": 3168.0, "end": 3176.0, "text": " How specific were you at the beginning about your accessibility requirements and what was must have nice to have?", "tokens": [51014, 1012, 2685, 645, 291, 412, 264, 2863, 466, 428, 15002, 7728, 293, 437, 390, 1633, 362, 1481, 281, 362, 30, 51414], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 802, "seek": 315500, "start": 3176.0, "end": 3177.0, "text": " I'm not sure.", "tokens": [51414, 286, 478, 406, 988, 13, 51464], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 803, "seek": 315500, "start": 3177.0, "end": 3178.0, "text": " I have delegated this.", "tokens": [51464, 286, 362, 15824, 770, 341, 13, 51514], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 804, "seek": 315500, "start": 3178.0, "end": 3179.0, "text": " I got it.", "tokens": [51514, 286, 658, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 805, "seek": 315500, "start": 3179.0, "end": 3181.0, "text": " So I think that would be my answer.", "tokens": [51564, 407, 286, 519, 300, 576, 312, 452, 1867, 13, 51664], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 806, "seek": 315500, "start": 3181.0, "end": 3182.0, "text": " Right?", "tokens": [51664, 1779, 30, 51714], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 807, "seek": 315500, "start": 3182.0, "end": 3184.0, "text": " The more time we spend up front.", "tokens": [51714, 440, 544, 565, 321, 3496, 493, 1868, 13, 51814], "temperature": 0.0, "avg_logprob": -0.26682144630956284, "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.06573983281850815}, {"id": 808, "seek": 318400, "start": 3184.0, "end": 3189.0, "text": " If this is a web app talking to the UX folks, the stakeholders, the product managers,", "tokens": [50364, 759, 341, 307, 257, 3670, 724, 1417, 281, 264, 40176, 4024, 11, 264, 17779, 11, 264, 1674, 14084, 11, 50614], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 809, "seek": 318400, "start": 3189.0, "end": 3193.0, "text": " getting really into the weeds on what are the non-negotiables for accessibility.", "tokens": [50614, 1242, 534, 666, 264, 26370, 322, 437, 366, 264, 2107, 12, 28561, 8206, 2965, 337, 15002, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 810, "seek": 318400, "start": 3193.0, "end": 3195.0, "text": " And if we can get all of that, you know,", "tokens": [50814, 400, 498, 321, 393, 483, 439, 295, 300, 11, 291, 458, 11, 50914], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 811, "seek": 318400, "start": 3195.0, "end": 3198.0, "text": " decide it up front and document it into one of these markdown files.", "tokens": [50914, 4536, 309, 493, 1868, 293, 4166, 309, 666, 472, 295, 613, 1491, 5093, 7098, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 812, "seek": 318400, "start": 3198.0, "end": 3202.0, "text": " I would expect that we would get, you know, better results there.", "tokens": [51064, 286, 576, 2066, 300, 321, 576, 483, 11, 291, 458, 11, 1101, 3542, 456, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 813, "seek": 318400, "start": 3202.0, "end": 3205.0, "text": " Is there a way to like, there's like a black tag guidelines?", "tokens": [51264, 1119, 456, 257, 636, 281, 411, 11, 456, 311, 411, 257, 2211, 6162, 12470, 30, 51414], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 814, "seek": 318400, "start": 3205.0, "end": 3208.0, "text": " Like, is there a way to like more easily integrate that even?", "tokens": [51414, 1743, 11, 307, 456, 257, 636, 281, 411, 544, 3612, 13365, 300, 754, 30, 51564], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 815, "seek": 318400, "start": 3208.0, "end": 3209.0, "text": " Just as a base.", "tokens": [51564, 1449, 382, 257, 3096, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 816, "seek": 318400, "start": 3209.0, "end": 3211.0, "text": " There's this like actual specs and whatnot.", "tokens": [51614, 821, 311, 341, 411, 3539, 27911, 293, 25882, 13, 51714], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 817, "seek": 318400, "start": 3211.0, "end": 3212.0, "text": " Yep.", "tokens": [51714, 7010, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19299882033775592, "compression_ratio": 1.715210355987055, "no_speech_prob": 0.015085214748978615}, {"id": 818, "seek": 321200, "start": 3213.0, "end": 3219.0, "text": " So, you know, one thing as you're developing your, you know, version of this for your teams.", "tokens": [50414, 407, 11, 291, 458, 11, 472, 551, 382, 291, 434, 6416, 428, 11, 291, 458, 11, 3037, 295, 341, 337, 428, 5491, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1608014829231031, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.005722806788980961}, {"id": 819, "seek": 321200, "start": 3219.0, "end": 3223.0, "text": " Again, you could imagine having a repo of these context files.", "tokens": [50714, 3764, 11, 291, 727, 3811, 1419, 257, 49040, 295, 613, 4319, 7098, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1608014829231031, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.005722806788980961}, {"id": 820, "seek": 321200, "start": 3223.0, "end": 3226.0, "text": " And a custom agent that you publish for the teams and say,", "tokens": [50914, 400, 257, 2375, 9461, 300, 291, 11374, 337, 264, 5491, 293, 584, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1608014829231031, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.005722806788980961}, {"id": 821, "seek": 321200, "start": 3226.0, "end": 3228.0, "text": " use this when you're building a web app.", "tokens": [51064, 764, 341, 562, 291, 434, 2390, 257, 3670, 724, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1608014829231031, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.005722806788980961}, {"id": 822, "seek": 321200, "start": 3228.0, "end": 3231.0, "text": " And in that custom agent, in the context, you could say,", "tokens": [51164, 400, 294, 300, 2375, 9461, 11, 294, 264, 4319, 11, 291, 727, 584, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1608014829231031, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.005722806788980961}, {"id": 823, "seek": 321200, "start": 3231.0, "end": 3235.0, "text": " the following or kind of company standards for accessibility is like,", "tokens": [51314, 264, 3480, 420, 733, 295, 2237, 7787, 337, 15002, 307, 411, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1608014829231031, "compression_ratio": 1.7767441860465116, "no_speech_prob": 0.005722806788980961}, {"id": 824, "seek": 323500, "start": 3235.0, "end": 3242.0, "text": " how should I make sure that you hit these versions of these WCAG requirements.", "tokens": [50364, 577, 820, 286, 652, 988, 300, 291, 2045, 613, 9606, 295, 613, 343, 15515, 38, 7728, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1959010070224978, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.030295923352241516}, {"id": 825, "seek": 323500, "start": 3242.0, "end": 3246.0, "text": " The model is going to have a pretty good understanding of those requirements,", "tokens": [50714, 440, 2316, 307, 516, 281, 362, 257, 1238, 665, 3701, 295, 729, 7728, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1959010070224978, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.030295923352241516}, {"id": 826, "seek": 323500, "start": 3246.0, "end": 3249.0, "text": " baked into its latent knowledge.", "tokens": [50914, 19453, 666, 1080, 48994, 3601, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1959010070224978, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.030295923352241516}, {"id": 827, "seek": 323500, "start": 3249.0, "end": 3252.0, "text": " But if you wanted to really make sure that you nailed it, you could say,", "tokens": [51064, 583, 498, 291, 1415, 281, 534, 652, 988, 300, 291, 30790, 309, 11, 291, 727, 584, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1959010070224978, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.030295923352241516}, {"id": 828, "seek": 323500, "start": 3252.0, "end": 3258.0, "text": " hey, I'm guessing I would be willing to wager that there is a WCAG,", "tokens": [51214, 4177, 11, 286, 478, 17939, 286, 576, 312, 4950, 281, 261, 3557, 300, 456, 307, 257, 343, 15515, 38, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1959010070224978, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.030295923352241516}, {"id": 829, "seek": 323500, "start": 3258.0, "end": 3263.0, "text": " MCP server where you can go, where you can go and grab the actual spec.", "tokens": [51514, 8797, 47, 7154, 689, 291, 393, 352, 11, 689, 291, 393, 352, 293, 4444, 264, 3539, 1608, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1959010070224978, "compression_ratio": 1.654320987654321, "no_speech_prob": 0.030295923352241516}, {"id": 830, "seek": 326300, "start": 3263.0, "end": 3267.0, "text": " So, that would come at the cost of more context that you'd have to load.", "tokens": [50364, 407, 11, 300, 576, 808, 412, 264, 2063, 295, 544, 4319, 300, 291, 1116, 362, 281, 3677, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1391222371464282, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0057186405174434185}, {"id": 831, "seek": 326300, "start": 3267.0, "end": 3272.0, "text": " But you could imagine saying, hey, as part of our verification,", "tokens": [50564, 583, 291, 727, 3811, 1566, 11, 4177, 11, 382, 644, 295, 527, 30206, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1391222371464282, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0057186405174434185}, {"id": 832, "seek": 326300, "start": 3272.0, "end": 3277.0, "text": " I want you to, I want you to make sure that you've built all of this in and actually go test it.", "tokens": [50814, 286, 528, 291, 281, 11, 286, 528, 291, 281, 652, 988, 300, 291, 600, 3094, 439, 295, 341, 294, 293, 767, 352, 1500, 309, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1391222371464282, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0057186405174434185}, {"id": 833, "seek": 326300, "start": 3277.0, "end": 3280.0, "text": " So, you think you'd get a sophisticated as you wanted to.", "tokens": [51064, 407, 11, 291, 519, 291, 1116, 483, 257, 16950, 382, 291, 1415, 281, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1391222371464282, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0057186405174434185}, {"id": 834, "seek": 326300, "start": 3280.0, "end": 3285.0, "text": " And, you know, I would suggest starting by adding that up front into the context", "tokens": [51214, 400, 11, 291, 458, 11, 286, 576, 3402, 2891, 538, 5127, 300, 493, 1868, 666, 264, 4319, 51464], "temperature": 0.0, "avg_logprob": -0.1391222371464282, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0057186405174434185}, {"id": 835, "seek": 326300, "start": 3285.0, "end": 3288.0, "text": " and having it build that in.", "tokens": [51464, 293, 1419, 309, 1322, 300, 294, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1391222371464282, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0057186405174434185}, {"id": 836, "seek": 326300, "start": 3288.0, "end": 3290.0, "text": " Thanks.", "tokens": [51614, 2561, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1391222371464282, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.0057186405174434185}, {"id": 837, "seek": 329000, "start": 3290.0, "end": 3292.0, "text": " That one over here.", "tokens": [50364, 663, 472, 670, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.35637726180854884, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.021667664870619774}, {"id": 838, "seek": 329000, "start": 3292.0, "end": 3295.0, "text": " Yeah.", "tokens": [50464, 865, 13, 50614], "temperature": 0.0, "avg_logprob": -0.35637726180854884, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.021667664870619774}, {"id": 839, "seek": 329000, "start": 3295.0, "end": 3298.0, "text": " I work as a manager of developer experience team,", "tokens": [50614, 286, 589, 382, 257, 6598, 295, 10754, 1752, 1469, 11, 50764], "temperature": 0.0, "avg_logprob": -0.35637726180854884, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.021667664870619774}, {"id": 840, "seek": 329000, "start": 3298.0, "end": 3302.0, "text": " our company and we have like 300 engineers as a whole.", "tokens": [50764, 527, 2237, 293, 321, 362, 411, 6641, 11955, 382, 257, 1379, 13, 50964], "temperature": 0.0, "avg_logprob": -0.35637726180854884, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.021667664870619774}, {"id": 841, "seek": 329000, "start": 3302.0, "end": 3305.0, "text": " And one of the difficulties that we have is like,", "tokens": [50964, 400, 472, 295, 264, 14399, 300, 321, 362, 307, 411, 11, 51114], "temperature": 0.0, "avg_logprob": -0.35637726180854884, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.021667664870619774}, {"id": 842, "seek": 329000, "start": 3305.0, "end": 3309.0, "text": " how are we standardized the AI knowledge,", "tokens": [51114, 577, 366, 321, 31677, 264, 7318, 3601, 11, 51314], "temperature": 0.0, "avg_logprob": -0.35637726180854884, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.021667664870619774}, {"id": 843, "seek": 329000, "start": 3309.0, "end": 3312.0, "text": " because people like to use heroes.", "tokens": [51314, 570, 561, 411, 281, 764, 12332, 13, 51464], "temperature": 0.0, "avg_logprob": -0.35637726180854884, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.021667664870619774}, {"id": 844, "seek": 329000, "start": 3312.0, "end": 3315.0, "text": " Some of those are like to use cursor or cloud code or whatever.", "tokens": [51464, 2188, 295, 729, 366, 411, 281, 764, 28169, 420, 4588, 3089, 420, 2035, 13, 51614], "temperature": 0.0, "avg_logprob": -0.35637726180854884, "compression_ratio": 1.5213270142180095, "no_speech_prob": 0.021667664870619774}, {"id": 845, "seek": 331500, "start": 3315.0, "end": 3320.0, "text": " And when I look at these, it's kind of like a bit more considerate.", "tokens": [50364, 400, 562, 286, 574, 412, 613, 11, 309, 311, 733, 295, 411, 257, 857, 544, 1949, 473, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3521617855037655, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.018114905804395676}, {"id": 846, "seek": 331500, "start": 3320.0, "end": 3323.0, "text": " On Kiro, like, how is the difference with like,", "tokens": [50614, 1282, 591, 5182, 11, 411, 11, 577, 307, 264, 2649, 365, 411, 11, 50764], "temperature": 0.0, "avg_logprob": -0.3521617855037655, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.018114905804395676}, {"id": 847, "seek": 331500, "start": 3323.0, "end": 3328.0, "text": " age it's MD, for example, that's kind of like a standardization independent of the tools.", "tokens": [50764, 3205, 309, 311, 22521, 11, 337, 1365, 11, 300, 311, 733, 295, 411, 257, 3832, 2144, 6695, 295, 264, 3873, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3521617855037655, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.018114905804395676}, {"id": 848, "seek": 331500, "start": 3328.0, "end": 3331.0, "text": " Like, that's, that's a little bit of my question here is like,", "tokens": [51014, 1743, 11, 300, 311, 11, 300, 311, 257, 707, 857, 295, 452, 1168, 510, 307, 411, 11, 51164], "temperature": 0.0, "avg_logprob": -0.3521617855037655, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.018114905804395676}, {"id": 849, "seek": 331500, "start": 3331.0, "end": 3333.0, "text": " how can we make that reusable,", "tokens": [51164, 577, 393, 321, 652, 300, 41807, 11, 51264], "temperature": 0.0, "avg_logprob": -0.3521617855037655, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.018114905804395676}, {"id": 850, "seek": 331500, "start": 3333.0, "end": 3336.0, "text": " instead of the tool that developers choosing,", "tokens": [51264, 2602, 295, 264, 2290, 300, 8849, 10875, 11, 51414], "temperature": 0.0, "avg_logprob": -0.3521617855037655, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.018114905804395676}, {"id": 851, "seek": 331500, "start": 3336.0, "end": 3339.0, "text": " so probably making easier to adopt.", "tokens": [51414, 370, 1391, 1455, 3571, 281, 6878, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3521617855037655, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.018114905804395676}, {"id": 852, "seek": 331500, "start": 3339.0, "end": 3341.0, "text": " Yeah.", "tokens": [51564, 865, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3521617855037655, "compression_ratio": 1.6609442060085837, "no_speech_prob": 0.018114905804395676}, {"id": 853, "seek": 334100, "start": 3341.0, "end": 3347.0, "text": " So we're seeing a lot of innovation in all of the tools in this space.", "tokens": [50364, 407, 321, 434, 2577, 257, 688, 295, 8504, 294, 439, 295, 264, 3873, 294, 341, 1901, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16987760351338518, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.010220193304121494}, {"id": 854, "seek": 334100, "start": 3347.0, "end": 3349.0, "text": " So things are moving fast.", "tokens": [50664, 407, 721, 366, 2684, 2370, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16987760351338518, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.010220193304121494}, {"id": 855, "seek": 334100, "start": 3349.0, "end": 3352.0, "text": " Agents that MD seems to be a de facto standard that's emerging.", "tokens": [50764, 2725, 791, 300, 22521, 2544, 281, 312, 257, 368, 42225, 3832, 300, 311, 14989, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16987760351338518, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.010220193304121494}, {"id": 856, "seek": 334100, "start": 3352.0, "end": 3354.0, "text": " That's good.", "tokens": [50914, 663, 311, 665, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16987760351338518, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.010220193304121494}, {"id": 857, "seek": 334100, "start": 3354.0, "end": 3358.0, "text": " The different tools have different ways for specifying,", "tokens": [51014, 440, 819, 3873, 362, 819, 2098, 337, 1608, 5489, 11, 51214], "temperature": 0.0, "avg_logprob": -0.16987760351338518, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.010220193304121494}, {"id": 858, "seek": 334100, "start": 3358.0, "end": 3361.0, "text": " you know, this concept of custom agents.", "tokens": [51214, 291, 458, 11, 341, 3410, 295, 2375, 12554, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16987760351338518, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.010220193304121494}, {"id": 859, "seek": 334100, "start": 3361.0, "end": 3365.0, "text": " This workflow, the AI DLC is just a bunch of markdown files.", "tokens": [51364, 639, 20993, 11, 264, 7318, 30272, 307, 445, 257, 3840, 295, 1491, 5093, 7098, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16987760351338518, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.010220193304121494}, {"id": 860, "seek": 334100, "start": 3365.0, "end": 3369.0, "text": " So it would work fine in cloud code or cursor or what have you.", "tokens": [51564, 407, 309, 576, 589, 2489, 294, 4588, 3089, 420, 28169, 420, 437, 362, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16987760351338518, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.010220193304121494}, {"id": 861, "seek": 336900, "start": 3369.0, "end": 3373.0, "text": " So this isn't meant to be specifically tied to Kiro.", "tokens": [50364, 407, 341, 1943, 380, 4140, 281, 312, 4682, 9601, 281, 591, 5182, 13, 50564], "temperature": 0.0, "avg_logprob": -0.119558470589774, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.015991920605301857}, {"id": 862, "seek": 336900, "start": 3373.0, "end": 3376.0, "text": " We think that Kiro is a great platform for doing this,", "tokens": [50564, 492, 519, 300, 591, 5182, 307, 257, 869, 3663, 337, 884, 341, 11, 50714], "temperature": 0.0, "avg_logprob": -0.119558470589774, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.015991920605301857}, {"id": 863, "seek": 336900, "start": 3376.0, "end": 3381.0, "text": " but we're not trying to sort of lock it down to one particular tool.", "tokens": [50714, 457, 321, 434, 406, 1382, 281, 1333, 295, 4017, 309, 760, 281, 472, 1729, 2290, 13, 50964], "temperature": 0.0, "avg_logprob": -0.119558470589774, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.015991920605301857}, {"id": 864, "seek": 336900, "start": 3381.0, "end": 3390.0, "text": " But ideally, you're able to bundle together the AI DLC or the context files that you have that you want to steer the behavior of the model,", "tokens": [50964, 583, 22915, 11, 291, 434, 1075, 281, 24438, 1214, 264, 7318, 30272, 420, 264, 4319, 7098, 300, 291, 362, 300, 291, 528, 281, 30814, 264, 5223, 295, 264, 2316, 11, 51414], "temperature": 0.0, "avg_logprob": -0.119558470589774, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.015991920605301857}, {"id": 865, "seek": 336900, "start": 3390.0, "end": 3393.0, "text": " as well as your list of NCP servers together,", "tokens": [51414, 382, 731, 382, 428, 1329, 295, 426, 20049, 15909, 1214, 11, 51564], "temperature": 0.0, "avg_logprob": -0.119558470589774, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.015991920605301857}, {"id": 866, "seek": 336900, "start": 3393.0, "end": 3398.0, "text": " and have a way to publish that out to other folks in your organization.", "tokens": [51564, 293, 362, 257, 636, 281, 11374, 300, 484, 281, 661, 4024, 294, 428, 4475, 13, 51814], "temperature": 0.0, "avg_logprob": -0.119558470589774, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.015991920605301857}, {"id": 867, "seek": 339800, "start": 3398.0, "end": 3402.0, "text": " So with Kiro CLI, that's just a JSON file.", "tokens": [50364, 407, 365, 591, 5182, 12855, 40, 11, 300, 311, 445, 257, 31828, 3991, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14491960317781655, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.008038653992116451}, {"id": 868, "seek": 339800, "start": 3402.0, "end": 3406.0, "text": " And so that all that could be is an internal repository where you say,", "tokens": [50564, 400, 370, 300, 439, 300, 727, 312, 307, 364, 6920, 25841, 689, 291, 584, 11, 50764], "temperature": 0.0, "avg_logprob": -0.14491960317781655, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.008038653992116451}, {"id": 869, "seek": 339800, "start": 3406.0, "end": 3409.0, "text": " hey, if you're building web apps, use this custom agent.", "tokens": [50764, 4177, 11, 498, 291, 434, 2390, 3670, 7733, 11, 764, 341, 2375, 9461, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14491960317781655, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.008038653992116451}, {"id": 870, "seek": 339800, "start": 3409.0, "end": 3413.0, "text": " If you're extending this old legacy app, go use this agent.", "tokens": [50914, 759, 291, 434, 24360, 341, 1331, 11711, 724, 11, 352, 764, 341, 9461, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14491960317781655, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.008038653992116451}, {"id": 871, "seek": 339800, "start": 3413.0, "end": 3420.0, "text": " And that combined kind of collective wisdom of how to operate around those different domains is baked into those agents.", "tokens": [51114, 400, 300, 9354, 733, 295, 12590, 10712, 295, 577, 281, 9651, 926, 729, 819, 25514, 307, 19453, 666, 729, 12554, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14491960317781655, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.008038653992116451}, {"id": 872, "seek": 339800, "start": 3420.0, "end": 3421.0, "text": " Yeah.", "tokens": [51464, 865, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14491960317781655, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.008038653992116451}, {"id": 873, "seek": 339800, "start": 3421.0, "end": 3423.0, "text": " Guess so one more.", "tokens": [51514, 17795, 370, 472, 544, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14491960317781655, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.008038653992116451}, {"id": 874, "seek": 342300, "start": 3423.0, "end": 3428.0, "text": " Oh, come over to you next.", "tokens": [50364, 876, 11, 808, 670, 281, 291, 958, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17402111994077082, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.04116181284189224}, {"id": 875, "seek": 342300, "start": 3428.0, "end": 3432.0, "text": " First of all, thank you for that great presentation.", "tokens": [50614, 2386, 295, 439, 11, 1309, 291, 337, 300, 869, 5860, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17402111994077082, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.04116181284189224}, {"id": 876, "seek": 342300, "start": 3432.0, "end": 3436.0, "text": " When you have these different stages in the AI DLC,", "tokens": [50814, 1133, 291, 362, 613, 819, 10232, 294, 264, 7318, 30272, 11, 51014], "temperature": 0.0, "avg_logprob": -0.17402111994077082, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.04116181284189224}, {"id": 877, "seek": 342300, "start": 3436.0, "end": 3439.0, "text": " and in the agent definition,", "tokens": [51014, 293, 294, 264, 9461, 7123, 11, 51164], "temperature": 0.0, "avg_logprob": -0.17402111994077082, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.04116181284189224}, {"id": 878, "seek": 342300, "start": 3439.0, "end": 3442.0, "text": " there was this line model that was null.", "tokens": [51164, 456, 390, 341, 1622, 2316, 300, 390, 18184, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17402111994077082, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.04116181284189224}, {"id": 879, "seek": 342300, "start": 3442.0, "end": 3448.0, "text": " So you leave the selection of the model to use to the automatic,", "tokens": [51314, 407, 291, 1856, 264, 9450, 295, 264, 2316, 281, 764, 281, 264, 12509, 11, 51614], "temperature": 0.0, "avg_logprob": -0.17402111994077082, "compression_ratio": 1.4943820224719102, "no_speech_prob": 0.04116181284189224}, {"id": 880, "seek": 344800, "start": 3448.0, "end": 3455.0, "text": " or would you suggest using different models for different tasks and stages in the AI DLC?", "tokens": [50364, 420, 576, 291, 3402, 1228, 819, 5245, 337, 819, 9608, 293, 10232, 294, 264, 7318, 30272, 30, 50714], "temperature": 0.0, "avg_logprob": -0.1464230492001488, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.04622228071093559}, {"id": 881, "seek": 344800, "start": 3455.0, "end": 3456.0, "text": " Yep.", "tokens": [50714, 7010, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1464230492001488, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.04622228071093559}, {"id": 882, "seek": 344800, "start": 3456.0, "end": 3458.0, "text": " Thanks for the question.", "tokens": [50764, 2561, 337, 264, 1168, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1464230492001488, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.04622228071093559}, {"id": 883, "seek": 344800, "start": 3458.0, "end": 3462.0, "text": " So by default, we now have an auto mode in Kiro,", "tokens": [50864, 407, 538, 7576, 11, 321, 586, 362, 364, 8399, 4391, 294, 591, 5182, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1464230492001488, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.04622228071093559}, {"id": 884, "seek": 344800, "start": 3462.0, "end": 3469.0, "text": " and that's going to allow us to select the model and give you a discount in terms of credits.", "tokens": [51064, 293, 300, 311, 516, 281, 2089, 505, 281, 3048, 264, 2316, 293, 976, 291, 257, 11635, 294, 2115, 295, 16816, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1464230492001488, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.04622228071093559}, {"id": 885, "seek": 344800, "start": 3469.0, "end": 3471.0, "text": " And our goal with the engineer and team,", "tokens": [51414, 400, 527, 3387, 365, 264, 11403, 293, 1469, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1464230492001488, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.04622228071093559}, {"id": 886, "seek": 344800, "start": 3471.0, "end": 3473.0, "text": " and there's been a lot of energy on this,", "tokens": [51514, 293, 456, 311, 668, 257, 688, 295, 2281, 322, 341, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1464230492001488, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.04622228071093559}, {"id": 887, "seek": 344800, "start": 3473.0, "end": 3476.0, "text": " is making that auto mode work great,", "tokens": [51614, 307, 1455, 300, 8399, 4391, 589, 869, 11, 51764], "temperature": 0.0, "avg_logprob": -0.1464230492001488, "compression_ratio": 1.578512396694215, "no_speech_prob": 0.04622228071093559}, {"id": 888, "seek": 347600, "start": 3476.0, "end": 3479.0, "text": " and you as a user would never have a reason to change the model,", "tokens": [50364, 293, 291, 382, 257, 4195, 576, 1128, 362, 257, 1778, 281, 1319, 264, 2316, 11, 50514], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 889, "seek": 347600, "start": 3479.0, "end": 3481.0, "text": " because you're very happy with the outcome.", "tokens": [50514, 570, 291, 434, 588, 2055, 365, 264, 9700, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 890, "seek": 347600, "start": 3481.0, "end": 3483.0, "text": " So that would be our recommendation.", "tokens": [50614, 407, 300, 576, 312, 527, 11879, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 891, "seek": 347600, "start": 3483.0, "end": 3486.0, "text": " Start with the auto mode, hopefully you get great.", "tokens": [50714, 6481, 365, 264, 8399, 4391, 11, 4696, 291, 483, 869, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 892, "seek": 347600, "start": 3486.0, "end": 3489.0, "text": " If you're not sure you're getting great and you want to say,", "tokens": [50864, 759, 291, 434, 406, 988, 291, 434, 1242, 869, 293, 291, 528, 281, 584, 11, 51014], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 893, "seek": 347600, "start": 3489.0, "end": 3495.0, "text": " no, I definitely want Sonic 4-5 or whatever the latest model of the day is,", "tokens": [51014, 572, 11, 286, 2138, 528, 14290, 1017, 12, 20, 420, 2035, 264, 6792, 2316, 295, 264, 786, 307, 11, 51314], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 894, "seek": 347600, "start": 3495.0, "end": 3498.0, "text": " you could switch to that and see if you get different results.", "tokens": [51314, 291, 727, 3679, 281, 300, 293, 536, 498, 291, 483, 819, 3542, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 895, "seek": 347600, "start": 3498.0, "end": 3503.0, "text": " But our hope in our expectation is that you can start with just not defining the model,", "tokens": [51464, 583, 527, 1454, 294, 527, 14334, 307, 300, 291, 393, 722, 365, 445, 406, 17827, 264, 2316, 11, 51714], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 896, "seek": 347600, "start": 3503.0, "end": 3505.0, "text": " use the default auto mode.", "tokens": [51714, 764, 264, 7576, 8399, 4391, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14219787425564645, "compression_ratio": 1.7992957746478873, "no_speech_prob": 0.03131450340151787}, {"id": 897, "seek": 350500, "start": 3505.0, "end": 3506.0, "text": " Yep.", "tokens": [50364, 7010, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 898, "seek": 350500, "start": 3506.0, "end": 3510.0, "text": " I want to take one more here, and I know we're just wrapping up on time.", "tokens": [50414, 286, 528, 281, 747, 472, 544, 510, 11, 293, 286, 458, 321, 434, 445, 21993, 493, 322, 565, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 899, "seek": 350500, "start": 3510.0, "end": 3514.0, "text": " So first of all, thank you so much for the presentation at demo,", "tokens": [50614, 407, 700, 295, 439, 11, 1309, 291, 370, 709, 337, 264, 5860, 412, 10723, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 900, "seek": 350500, "start": 3514.0, "end": 3518.0, "text": " and I just tried to Kiro app and also the CLI as well.", "tokens": [50814, 293, 286, 445, 3031, 281, 591, 5182, 724, 293, 611, 264, 12855, 40, 382, 731, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 901, "seek": 350500, "start": 3518.0, "end": 3520.0, "text": " They both work really fast.", "tokens": [51014, 814, 1293, 589, 534, 2370, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 902, "seek": 350500, "start": 3520.0, "end": 3521.0, "text": " I really like it.", "tokens": [51114, 286, 534, 411, 309, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 903, "seek": 350500, "start": 3521.0, "end": 3522.0, "text": " Great.", "tokens": [51164, 3769, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 904, "seek": 350500, "start": 3522.0, "end": 3523.0, "text": " I like the broader.", "tokens": [51214, 286, 411, 264, 13227, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 905, "seek": 350500, "start": 3523.0, "end": 3525.0, "text": " But the things are, you know, I know a lot of independent developers.", "tokens": [51264, 583, 264, 721, 366, 11, 291, 458, 11, 286, 458, 257, 688, 295, 6695, 8849, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 906, "seek": 350500, "start": 3525.0, "end": 3529.0, "text": " They try to show me like AI coding agent, cursor, cloud code.", "tokens": [51364, 814, 853, 281, 855, 385, 411, 7318, 17720, 9461, 11, 28169, 11, 4588, 3089, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 907, "seek": 350500, "start": 3529.0, "end": 3531.0, "text": " So actually, I want to get more insight that,", "tokens": [51564, 407, 767, 11, 286, 528, 281, 483, 544, 11269, 300, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1808800837572883, "compression_ratio": 1.6290909090909091, "no_speech_prob": 0.01830601505935192}, {"id": 908, "seek": 353100, "start": 3531.0, "end": 3536.0, "text": " how to differentiate from Kiro and those not a coding agent.", "tokens": [50364, 577, 281, 23203, 490, 591, 5182, 293, 729, 406, 257, 17720, 9461, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 909, "seek": 353100, "start": 3536.0, "end": 3540.0, "text": " For example, are we going to have more integration on AWS or, you know,", "tokens": [50614, 1171, 1365, 11, 366, 321, 516, 281, 362, 544, 10980, 322, 17650, 420, 11, 291, 458, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 910, "seek": 353100, "start": 3540.0, "end": 3545.0, "text": " to some more integration work on enterprise integration?", "tokens": [50814, 281, 512, 544, 10980, 589, 322, 14132, 10980, 30, 51064], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 911, "seek": 353100, "start": 3545.0, "end": 3546.0, "text": " Yep.", "tokens": [51064, 7010, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 912, "seek": 353100, "start": 3546.0, "end": 3547.0, "text": " Yep.", "tokens": [51114, 7010, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 913, "seek": 353100, "start": 3547.0, "end": 3548.0, "text": " Thank you.", "tokens": [51164, 1044, 291, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 914, "seek": 353100, "start": 3548.0, "end": 3549.0, "text": " We're right at time.", "tokens": [51214, 492, 434, 558, 412, 565, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 915, "seek": 353100, "start": 3549.0, "end": 3553.0, "text": " But I'll say, in terms of differentiation,", "tokens": [51264, 583, 286, 603, 584, 11, 294, 2115, 295, 38902, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 916, "seek": 353100, "start": 3553.0, "end": 3556.0, "text": " we want care to be a great product regardless of where you're developing,", "tokens": [51464, 321, 528, 1127, 281, 312, 257, 869, 1674, 10060, 295, 689, 291, 434, 6416, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 917, "seek": 353100, "start": 3556.0, "end": 3558.0, "text": " even if it's not a native US.", "tokens": [51614, 754, 498, 309, 311, 406, 257, 8470, 2546, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1999878000330042, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.07404454052448273}, {"id": 918, "seek": 355800, "start": 3558.0, "end": 3562.0, "text": " And so our intention is to just focus on a great developer experience.", "tokens": [50364, 400, 370, 527, 7789, 307, 281, 445, 1879, 322, 257, 869, 10754, 1752, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 919, "seek": 355800, "start": 3562.0, "end": 3566.0, "text": " Of course, we want it to be really good at AWS as well.", "tokens": [50564, 2720, 1164, 11, 321, 528, 309, 281, 312, 534, 665, 412, 17650, 382, 731, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 920, "seek": 355800, "start": 3566.0, "end": 3568.0, "text": " So that's the thinking.", "tokens": [50764, 407, 300, 311, 264, 1953, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 921, "seek": 355800, "start": 3568.0, "end": 3569.0, "text": " You know, differentiators.", "tokens": [50864, 509, 458, 11, 27372, 3391, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 922, "seek": 355800, "start": 3569.0, "end": 3573.0, "text": " There's a lot of smart folks that are building other cool tools in this space.", "tokens": [50914, 821, 311, 257, 688, 295, 4069, 4024, 300, 366, 2390, 661, 1627, 3873, 294, 341, 1901, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 923, "seek": 355800, "start": 3573.0, "end": 3577.0, "text": " I think it's a good time for end users because that competition is,", "tokens": [51114, 286, 519, 309, 311, 257, 665, 565, 337, 917, 5022, 570, 300, 6211, 307, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 924, "seek": 355800, "start": 3577.0, "end": 3580.0, "text": " is having folks kind of leapfrog each other in capabilities.", "tokens": [51314, 307, 1419, 4024, 733, 295, 19438, 69, 6675, 1184, 661, 294, 10862, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 925, "seek": 355800, "start": 3580.0, "end": 3582.0, "text": " I expect that's going to continue to happen.", "tokens": [51464, 286, 2066, 300, 311, 516, 281, 2354, 281, 1051, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 926, "seek": 355800, "start": 3582.0, "end": 3585.0, "text": " I'd say one nice thing about Kiro is when you subscribe,", "tokens": [51564, 286, 1116, 584, 472, 1481, 551, 466, 591, 5182, 307, 562, 291, 3022, 11, 51714], "temperature": 0.0, "avg_logprob": -0.09177007675170898, "compression_ratio": 1.591503267973856, "no_speech_prob": 0.07558874785900116}, {"id": 927, "seek": 358500, "start": 3585.0, "end": 3589.0, "text": " you have access to both the CLI and the IDE for one subscription.", "tokens": [50364, 291, 362, 2105, 281, 1293, 264, 12855, 40, 293, 264, 40930, 337, 472, 17231, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 928, "seek": 358500, "start": 3589.0, "end": 3593.0, "text": " And we see a lot of folks that like to switch back and forth between those modes,", "tokens": [50564, 400, 321, 536, 257, 688, 295, 4024, 300, 411, 281, 3679, 646, 293, 5220, 1296, 729, 14068, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 929, "seek": 358500, "start": 3593.0, "end": 3595.0, "text": " or maybe even use both at once.", "tokens": [50764, 420, 1310, 754, 764, 1293, 412, 1564, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 930, "seek": 358500, "start": 3595.0, "end": 3597.0, "text": " So I think that's a nice thing.", "tokens": [50864, 407, 286, 519, 300, 311, 257, 1481, 551, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 931, "seek": 358500, "start": 3597.0, "end": 3600.0, "text": " I would say the spectrum and development in the IDE,", "tokens": [50964, 286, 576, 584, 264, 11143, 293, 3250, 294, 264, 40930, 11, 51114], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 932, "seek": 358500, "start": 3600.0, "end": 3603.0, "text": " which we didn't look at today, but we've heard a lot of great feedback", "tokens": [51114, 597, 321, 994, 380, 574, 412, 965, 11, 457, 321, 600, 2198, 257, 688, 295, 869, 5824, 51264], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 933, "seek": 358500, "start": 3603.0, "end": 3608.0, "text": " that that's a good way for developers to sort of structure their approach.", "tokens": [51264, 300, 300, 311, 257, 665, 636, 337, 8849, 281, 1333, 295, 3877, 641, 3109, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 934, "seek": 358500, "start": 3608.0, "end": 3610.0, "text": " So I would encourage you if you haven't to try both.", "tokens": [51514, 407, 286, 576, 5373, 291, 498, 291, 2378, 380, 281, 853, 1293, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 935, "seek": 358500, "start": 3610.0, "end": 3613.0, "text": " Try the spectrum and workflow in Kiro IDE.", "tokens": [51614, 6526, 264, 11143, 293, 20993, 294, 591, 5182, 40930, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08390071812797995, "compression_ratio": 1.6754966887417218, "no_speech_prob": 0.03900327533483505}, {"id": 936, "seek": 361300, "start": 3613.0, "end": 3617.0, "text": " Try the AI DLC, which is a bit more in depth,", "tokens": [50364, 6526, 264, 7318, 30272, 11, 597, 307, 257, 857, 544, 294, 7161, 11, 50564], "temperature": 0.0, "avg_logprob": -0.19402638348666104, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.022205950692296028}, {"id": 937, "seek": 361300, "start": 3617.0, "end": 3619.0, "text": " and see which one suits you the best.", "tokens": [50564, 293, 536, 597, 472, 15278, 291, 264, 1151, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19402638348666104, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.022205950692296028}, {"id": 938, "seek": 361300, "start": 3619.0, "end": 3621.0, "text": " So very brief answer to your question,", "tokens": [50664, 407, 588, 5353, 1867, 281, 428, 1168, 11, 50764], "temperature": 0.0, "avg_logprob": -0.19402638348666104, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.022205950692296028}, {"id": 939, "seek": 361300, "start": 3621.0, "end": 3623.0, "text": " but I know we're right at time.", "tokens": [50764, 457, 286, 458, 321, 434, 558, 412, 565, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19402638348666104, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.022205950692296028}, {"id": 940, "seek": 361300, "start": 3623.0, "end": 3624.0, "text": " All right?", "tokens": [50864, 1057, 558, 30, 50914], "temperature": 0.0, "avg_logprob": -0.19402638348666104, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.022205950692296028}, {"id": 941, "seek": 361300, "start": 3624.0, "end": 3626.0, "text": " Thank you all.", "tokens": [50914, 1044, 291, 439, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19402638348666104, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.022205950692296028}, {"id": 942, "seek": 361300, "start": 3626.0, "end": 3627.0, "text": " Thanks, Kiro.", "tokens": [51014, 2561, 11, 591, 5182, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19402638348666104, "compression_ratio": 1.3197278911564625, "no_speech_prob": 0.022205950692296028}], "language": "en"}