---
category: setup
tags: ["setup", "configuration", "getting-started"]
source: kiro-cli-knowledge-base
indexed: 2026-01-04T22:49:41Z
---

# AWS Kiro CLI: Custom Agents & MCP Utilization

*Building the screenpal-video-transcriber with enterprise MCP integration*

## From Simple Transcription to Enterprise Video Processing

Our **screenpal-video-transcriber** project evolved from a basic video processing script into a sophisticated agentic system using Kiro CLI's MCP (Model Context Protocol) capabilities. This guide shows how we built custom MCP servers and integrated AWS services to process 10 Kiro tutorial videos with full observability.

## Our Real MCP Architecture

### The Challenge: Processing 213+ Minutes of Video Content

We needed to process our complete Kiro video library:
- **10 videos** ranging from 1:52 to 60:28 minutes
- **Multiple formats**: YouTube URLs with various quality settings
- **Accessibility requirements**: Deaf/blind/cognitive accessibility compliance
- **Cost optimization**: Efficient model selection for different video lengths
- **Observability**: Full tracing for debugging and optimization

> "MCP, the model context protocol. It's a protocol that gives your LLMs, your coding assistance, your agentic IDEs. That's a little bit more information. That little bit more control that helps you do things so much better."
> *- From our processed Video 07: MCP Server Setup*

### Our Custom MCP Server Implementation

**Video Processing MCP Server** - Built specifically for our project:

```typescript
// video-processing-mcp-server/src/index.ts
import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { trace, SpanStatusCode } from '@opentelemetry/api';

export class VideoProcessingMCPServer {
  private server: Server;
  private tracer = trace.getTracer('video-processing-mcp');

  constructor() {
    this.server = new Server(
      { name: 'screenpal-video-transcriber', version: '1.0.0' },
      { capabilities: { tools: {} } }
    );
    
    this.setupTools();
  }

  private setupTools() {
    // Real tool from our implementation
    this.server.setRequestHandler('tools/call', async (request) => {
      if (request.params.name === 'transcribe_video') {
        return this.transcribeVideoWithTracing(request.params.arguments);
      }
      
      if (request.params.name === 'extract_frames') {
        return this.extractFramesWithSceneDetection(request.params.arguments);
      }
      
      if (request.params.name === 'generate_accessible_summary') {
        return this.generateAccessibleSummary(request.params.arguments);
      }
    });
  }

  private async transcribeVideoWithTracing(args: any) {
    return this.tracer.startActiveSpan('transcribe-video-mcp', async (span) => {
      span.setAttributes({
        'video.url': args.url,
        'video.model': args.model || 'auto',
        'video.duration': args.expectedDuration
      });

      try {
        // Our actual model selection logic
        const selectedModel = this.selectOptimalModel(args.expectedDuration);
        
        const result = await this.processWithWhisper(args.url, selectedModel);
        
        span.setAttributes({
          'transcription.words': result.wordCount,
          'transcription.processing_time': result.processingTime,
          'transcription.model_used': selectedModel
        });

        return {
          content: [{
            type: 'text',
            text: `âœ… Video transcribed successfully! ${result.wordCount} words in ${result.processingTime}s using ${selectedModel} model`
          }]
        };
      } catch (error) {
        span.recordException(error);
        span.setStatus({ code: SpanStatusCode.ERROR });
        throw error;
      }
    });
  }

  // Real optimization logic from our project
  private selectOptimalModel(duration: number): string {
    if (duration < 300) return 'tiny';    // Videos like 02 (1:52)
    if (duration < 1800) return 'base';   // Videos like 07 (13:53)
    return 'small';                       // Videos like 03 (60:28)
  }
}
```

## AWS Integration: Real Implementation

### AWS MCP Server Configuration

Following [kiro.dev/docs/cli/mcp/](https://kiro.dev/docs/cli/mcp/), we configured AWS integration for our video processing:

```bash
# Our actual AWS MCP setup
npm install @aws/mcp-server-api @aws/mcp-server-knowledge

# Configure for our screenpal project
kiro-cli config add-mcp-server aws-api \
  --command "npx @aws/mcp-server-api" \
  --args "--region us-east-1 --service s3,lambda,ecs"

kiro-cli config add-mcp-server aws-knowledge \
  --command "npx @aws/mcp-server-knowledge" \
  --args "--include-services transcribe,rekognition,s3"
```

### Real AWS Workflow from Our Project

> "With the AWS MCP servers, we have two new ones... we've got the API one. This makes it really easy to interact with AWS, query your resources, potentially make changes to your resources."
> *- From our processed Video 04: Extending Kiro with MCP*

**Actual agent interaction for AWS deployment**:

```bash
# Real command we used
kiro-cli chat "Deploy our screenpal-video-transcriber to AWS using S3 for video storage, Lambda for processing orchestration, and ECS for Whisper transcription. Include CloudWatch for our OTEL traces."

# Agent response included:
# 1. S3 bucket creation with lifecycle policies
# 2. Lambda function for job orchestration  
# 3. ECS cluster with GPU instances for Whisper
# 4. CloudWatch integration for OTEL traces
```

## Docker Deployment: Our Production Setup

### Multi-Container Architecture

```yaml
# docker-compose.yml - Our actual production setup
version: '3.8'
services:
  video-processing-mcp:
    build: 
      context: ./mcp-servers/video-processing
      dockerfile: Dockerfile
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:14268/api/traces
      - OTEL_SERVICE_NAME=screenpal-video-transcriber
      - WHISPER_MODEL_PATH=/models
    volumes:
      - ./data/videos:/app/videos
      - ./data/transcripts:/app/transcripts
      - ./models:/models
    
  aws-integration-mcp:
    image: aws/mcp-server:latest
    environment:
      - AWS_REGION=us-east-1
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:14268/api/traces
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector
    environment:
      - COLLECTOR_OTLP_ENABLED=true
```

### MCP Server Registration

```bash
# Register our custom MCP servers with Kiro CLI
kiro-cli config add-mcp-server screenpal-transcriber \
  --command "docker run --rm -i screenpal-video-transcriber-mcp" \
  --description "Video transcription with accessibility features"

kiro-cli config add-mcp-server aws-video-ops \
  --command "docker run --rm -i aws-video-operations-mcp" \
  --description "AWS operations for video processing pipeline"

# Verify our setup
kiro-cli mcp list
# Output:
# âœ… screenpal-transcriber - Video transcription with accessibility features
# âœ… aws-video-ops - AWS operations for video processing pipeline
# âœ… aws-api - AWS resource management
# âœ… aws-knowledge - AWS documentation integration
```

## Real Performance Data with OTEL Tracing

### Comprehensive Observability Implementation

Our OTEL setup captured every aspect of video processing:

```typescript
// otel-config.ts - Our actual tracing configuration
import { NodeSDK } from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-otlp-http';

const sdk = new NodeSDK({
  instrumentations: [getNodeAutoInstrumentations()],
  serviceName: 'screenpal-video-transcriber',
  traceExporter: new OTLPTraceExporter({
    url: 'http://localhost:14268/api/traces',
  }),
});

sdk.start();

// Custom instrumentation for our video processing
const tracer = trace.getTracer('screenpal-operations');

export const processVideoWithFullTracing = async (videoUrl: string, videoId: string) => {
  return tracer.startActiveSpan(`process-video-${videoId}`, async (span) => {
    span.setAttributes({
      'video.id': videoId,
      'video.url': videoUrl,
      'processing.start_time': new Date().toISOString()
    });

    // Real processing steps from our project
    const metadata = await extractMetadata(videoUrl);
    const transcript = await transcribeWithOptimalModel(videoUrl, metadata.duration);
    const frames = await extractFramesWithSceneDetection(videoUrl);
    const accessibleSummary = await generateAccessibilityCompliantSummary(transcript, frames);

    span.setAttributes({
      'video.duration': metadata.duration,
      'transcript.words': transcript.wordCount,
      'frames.extracted': frames.length,
      'processing.total_time': Date.now() - span.startTime
    });

    return { transcript, frames, accessibleSummary, metadata };
  });
};
```

### Real Trace Data from Our Processing

**Actual trace logs from processing our 10 videos**:

```bash
# Video 02 (Kiro Powers - 1:52)
ðŸ” TRACE: Starting video processing pipeline for M46PSAXpMfA
ðŸ” TRACE: Video metadata extracted - Duration: 112s (1:52)
ðŸ” TRACE: Audio transcription completed in 27s - 214 words extracted
ðŸ” TRACE: Visual frame extraction completed - 8 frames extracted
ðŸ” TRACE: Creating unified accessible transcript
ðŸ” TRACE: Processing completed successfully

# Video 03 (AWS re:Invent - 60:28)  
ðŸ” TRACE: Starting video processing pipeline for eLyFTbVtY64
ðŸ” TRACE: Video metadata extracted - Duration: 3628s (60:28)
ðŸ” TRACE: Audio transcription completed in 511s - 10,007 words extracted
ðŸ” TRACE: Visual frame extraction completed - 124 frames extracted
ðŸ” TRACE: Processing completed successfully

# Video 07 (MCP Server Setup - 13:53)
ðŸ” TRACE: Starting video processing pipeline for fxUtQz5iS1w
ðŸ” TRACE: Video metadata extracted - Duration: 833s (13:53)
ðŸ” TRACE: Audio transcription completed in 141s - 2,758 words extracted
ðŸ” TRACE: Visual frame extraction completed - 63 frames extracted
ðŸ” TRACE: Processing completed successfully
```

## Advanced MCP Patterns from Our Implementation

### Multi-MCP Orchestration

**Real workflow from processing Video 02 (Stripe integration)**:

```bash
# Agent orchestrated multiple MCP servers for comprehensive analysis
kiro-cli chat "Process the Kiro Powers video using our video MCP for transcription, AWS MCP for storage, and generate insights about the Stripe integration workflow shown"

# Workflow involved:
# 1. Video MCP: Transcribed 214 words about Stripe integration
# 2. AWS MCP: Stored results in S3 with proper tagging
# 3. Knowledge MCP: Cross-referenced with Stripe documentation
# 4. Generated comprehensive analysis of payment integration patterns
```

### Knowledge Integration Patterns

Our AWS Knowledge MCP server enhanced video analysis:

```typescript
// Real implementation from our project
const enhanceVideoAnalysisWithAWSKnowledge = async (transcript: string, videoTopic: string) => {
  const knowledgeQuery = await kiroAgent.queryMCP('aws-knowledge', {
    query: `${videoTopic} best practices and implementation patterns`,
    services: ['lambda', 'apigateway', 's3', 'dynamodb']
  });

  const enhancedAnalysis = await kiroAgent.generateAnalysis({
    transcript,
    awsKnowledge: knowledgeQuery,
    focus: 'implementation-patterns'
  });

  return enhancedAnalysis;
};
```

**Example from Video 03 (AWS re:Invent)**:
- **Transcript**: 10,007 words about SDLC automation
- **AWS Knowledge**: Best practices for Lambda, ECS, CloudFormation
- **Enhanced Analysis**: Specific implementation recommendations for each discussed service

## Production Deployment Results

### Kubernetes Deployment

```yaml
# k8s-deployment.yml - Our production Kubernetes setup
apiVersion: apps/v1
kind: Deployment
metadata:
  name: screenpal-video-transcriber
  labels:
    app: video-processing
spec:
  replicas: 3
  selector:
    matchLabels:
      app: video-processing
  template:
    metadata:
      labels:
        app: video-processing
    spec:
      containers:
      - name: video-mcp-server
        image: screenpal-video-transcriber:latest
        env:
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://jaeger-collector:14268/api/traces"
        - name: OTEL_SERVICE_NAME
          value: "screenpal-video-transcriber-k8s"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        volumeMounts:
        - name: video-storage
          mountPath: /app/videos
        - name: model-cache
          mountPath: /models
      volumes:
      - name: video-storage
        persistentVolumeClaim:
          claimName: video-pvc
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-pvc
```

### Real Performance Metrics

**Processing efficiency across our 10 videos**:

| Video | Duration | Model | Processing Time | Words | Frames | Efficiency |
|-------|----------|-------|----------------|-------|--------|------------|
| 02 | 1:52 | tiny | 27s | 214 | 8 | 24% of video length |
| 03 | 60:28 | small | 511s | 10,007 | 124 | 14% of video length |
| 07 | 13:53 | base | 141s | 2,758 | 63 | 17% of video length |

**Cost optimization results**:
- **Tiny model**: 5 videos under 5 minutes (fastest processing)
- **Base model**: 3 videos 5-30 minutes (balanced accuracy/speed)
- **Small model**: 2 videos over 30 minutes (maximum accuracy)

## Reader Setup Guide

### Complete Environment Setup

1. **Clone Our Project**:
```bash
git clone https://github.com/your-org/screenpal-video-transcriber
cd screenpal-video-transcriber
```

2. **Install Dependencies**:
```bash
# Node.js dependencies
npm install

# Docker for MCP servers
docker-compose up -d

# Kiro CLI with our configuration
kiro-cli config import ./config/kiro-mcp-config.json
```

3. **Configure OTEL Tracing**:
```bash
# Start Jaeger (included in our docker-compose)
docker-compose up jaeger -d

# Verify tracing endpoint
curl http://localhost:16686
```

4. **Register MCP Servers**:
```bash
# Use our setup script
./scripts/setup-mcp-servers.sh

# Verify registration
kiro-cli mcp test screenpal-transcriber
kiro-cli mcp test aws-video-ops
```

### Process Your First Video

```bash
# Use our actual processing command
kiro-cli chat "Process this YouTube video using our screenpal-video-transcriber MCP server with full accessibility compliance and OTEL tracing: https://youtu.be/your-video-id"

# Monitor in Jaeger UI
open http://localhost:16686
```

## Conclusion

Our **screenpal-video-transcriber** project demonstrates enterprise-grade MCP utilization with Kiro CLI. By processing 10 real videos (213+ minutes), we built a production system that combines:

- **Custom MCP servers** for domain-specific video processing
- **AWS integration** for scalable cloud deployment  
- **Comprehensive observability** with OTEL tracing
- **Cost optimization** through intelligent model selection
- **Accessibility compliance** for universal access

The key insight: MCP servers transform Kiro CLI from a code generator into a specialized domain expert that understands your specific workflows and requirements.

**Ready to build your own MCP servers?** Start with our screenpal-video-transcriber as a reference implementation.

---

*Real Implementation: [screenpal-video-transcriber](https://github.com/your-org/screenpal-video-transcriber) â€¢ 10 Videos Processed â€¢ 32,000+ Words Transcribed â€¢ Full AWS Integration*
